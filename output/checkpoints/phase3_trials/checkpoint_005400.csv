ref_pmid,ref_is_clinical_trial_pt_type,ref_publication_types,ref_primary_nct_number,ref_primary_nct_source,ref_all_registry_ids,ref_all_nct_numbers,ref_all_structured_nct_numbers,ref_all_nct_source_pairs,ref_all_structured_nct_source_pairs,ref_has_abstract,ref_abstract,ref_fetch_status
21952254,False,Journal Article,,,,,,,,True,"Noninvasive (NIBP) and intraarterial (ABP) blood pressure monitoring are used under different circumstances and may yield different values. The authors endeavored to characterize these differences and hypothesized that there could be differences in interventions associated with the use of ABP alone ([ABP]) versus ABP in combination with NIBP ([ABP+NIBP]). Simultaneous measurements of ABP and NIBP made during noncardiac cases were extracted from electronic anesthesia records; the differences were subjected to regression analysis. Records of blood products, vasopressors, and antihypertensives administered were also extracted, and associations between the use of these therapies and monitoring strategy ([ABP] vs. [ABP+NIBP]) were tested using univariate, multivariate, and propensity score matched analyses. Among 24,225 cases, 63% and 37% used [ABP+NIBP] and [ABP], respectively. Systolic NIBP was likely to be higher than ABP when ABP was less than 111 mmHg and lower than ABP otherwise. Among patients with hypotension, transfusion occurred in 27% versus 43% of patients in the [ABP+NIBP] versus [ABP] group, respectively (odds ratio = 0.4; 95% CI 0.35-0.46), and 7% versus 18% of patients in the [ABP+NIBP] versus [ABP] group received vasopressor infusions, respectively (P < 0.01). Among hypertensive patients, 12% versus 44% of those in the [ABP+NIBP] versus [ABP] group received antihypertensive agents, respectively (P < 0.01). NIBP was generally higher than ABP during periods of hypotension and lower than ABP during periods of hypertension. The use of NIBP measurements to supplement ABP measurements was associated with decreased use of blood transfusions, vasopressor infusions, and antihypertensive medications compared with the use of ABP alone.",success
27977471,False,Journal Article;Systematic Review,,,,,,,,True,"To interpret blood pressure (BP) data appropriately, healthcare providers need to be knowledgeable of the factors that can potentially impact the accuracy of BP measurement and contribute to variability between measurements. A systematic review of studies quantifying BP measurement inaccuracy. Medline and CINAHL databases were searched for empirical articles and systematic reviews published up to June 2015. Empirical articles were included if they reported a study that was relevant to the measurement of adult patients' resting BP at the upper arm in a clinical setting (e.g. ward or office); identified a specific source of inaccuracy; and quantified its effect. Reference lists and reviews were searched for additional articles. A total of 328 empirical studies were included. They investigated 29 potential sources of inaccuracy, categorized as relating to the patient, device, procedure or observer. Significant directional effects were found for 27; however, for some, the effects were inconsistent in direction. Compared with true resting BP, significant effects of individual sources ranged from -23.6 to +33 mmHg SBP and -14 to +23 mmHg DBP. A single BP value outside the expected range should be interpreted with caution and not taken as a definitive indicator of clinical deterioration. Where a measurement is abnormally high or low, further measurements should be taken and averaged. Wherever possible, BP values should be recorded graphically within ranges. This may reduce the impact of sources of inaccuracy and reduce the scope for misinterpretations based on small, likely erroneous or misleading, changes.",success
23269127,True,"Comparative Study;Journal Article;Multicenter Study;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Minimal clinical research has investigated the significance of different blood pressure monitoring techniques in the ICU and whether systolic vs. mean blood pressures should be targeted in therapeutic protocols and in defining clinical study cohorts. The objectives of this study are to compare real-world invasive arterial blood pressure with noninvasive blood pressure, and to determine if differences between the two techniques have clinical implications. We conducted a retrospective study comparing invasive arterial blood pressure and noninvasive blood pressure measurements using a large ICU database. We performed pairwise comparison between concurrent measures of invasive arterial blood pressure and noninvasive blood pressure. We studied the association of systolic and mean invasive arterial blood pressure and noninvasive blood pressure with acute kidney injury, and with ICU mortality. Adult intensive care units at a tertiary care hospital. Adult patients admitted to intensive care units between 2001 and 2007. None. Pairwise analysis of 27,022 simultaneously measured invasive arterial blood pressure/noninvasive blood pressure pairs indicated that noninvasive blood pressure overestimated systolic invasive arterial blood pressure during hypotension. Analysis of acute kidney injury and ICU mortality involved 1,633 and 4,957 patients, respectively. Our results indicated that hypotensive systolic noninvasive blood pressure readings were associated with a higher acute kidney injury prevalence (p = 0.008) and ICU mortality (p < 0.001) than systolic invasive arterial blood pressure in the same range (≤70 mm Hg). Noninvasive blood pressure and invasive arterial blood pressure mean arterial pressures showed better agreement; acute kidney injury prevalence (p = 0.28) and ICU mortality (p = 0.76) associated with hypotensive mean arterial pressure readings (≤60 mm Hg) were independent of measurement technique. Clinically significant discrepancies exist between invasive and noninvasive systolic blood pressure measurements during hypotension. Mean blood pressure from both techniques may be interpreted in a consistent manner in assessing patients' prognosis. Our results suggest that mean rather than systolic blood pressure is the preferred metric in the ICU to guide therapy.",success
33966560,True,"Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"<b>Background:</b> Acute increases of high blood pressure values, usually described as 'hypertensive crises', 'hypertensive urgencies' or 'hypertensive emergencies', are common causes of patients' presentation to emergency departments. Owing to the lack of ad hoc randomized clinical trials, current recommendations/suggestions for treatment of these patients are not evidenced-based and, therefore, the management of acute increases of blood pressure values represent a clinical challenge. However, an improved understanding of the underlying pathophysiology has changed radically the approach to management of the patients presenting with these conditions in recent years. Accordingly, it has been proposed to abandon the terms 'hypertensive crises' and 'hypertensive urgencies', and restrict the focus to 'hypertensive emergencies'. <b>Aims and Methods:</b> Starting from these premises, we aimed at systematically review all available studies (years 2010-2020) to garner information on the current management of hypertensive emergencies, in order to develop a novel symptoms- and evidence-based streamlined algorithm for the assessment and treatment of these patients.<b>Results and Conclusions:</b> In this educational review we proposed the BARKH-based algorithm for a quick identification of hypertensive emergencies and associated acute organ damage, to allow the patients with hypertensive emergencies to receive immediate treatment in a proper setting.",success
29265003,False,Journal Article,,,,,,,,True,"The aim of this study was to evaluate the use of as-needed (PRN) labetalol and hydralazine [intravenous (IV) or oral] in hospitalized medicine patients for the treatment of severe asymptomatic hypertension and to examine the potential negative outcomes associated with their use. The electronic health record of 250 medicine patients hospitalized at the University of Colorado Hospital between November 2014 and April 2016 who received at least one dose of PRN IV or oral hydralazine or labetalol were retrospectively reviewed. The primary outcome was to describe the use of PRN antihypertensive medications in this population. A total of 573 PRN doses of antihypertensive medication were administered. Oral hydralazine was the most common (521 doses, 90.9%). A total of 36% of PRN administrations were given for a systolic blood pressure (SBP) <180 mmHg and diastolic blood pressure (DBP) <110 mmHg (cut-point for acute severe hypertension). No serious adverse events were related to PRN antihypertensive administration. Despite receiving at least one PRN antihypertensive medication during hospitalization, 40.8% of patients were not continued on their home antihypertensive medication(s) while hospitalized, and 62.4% of patients did not have their home regimens intensified at discharge. As-needed oral hydralazine is frequently prescribed for acute blood pressure lowering with administration thresholds often less than what are used to define acute severe hypertension. Many patients are prescribed PRN antihypertensive medication instead of being continued on their home regimens, and most patients do not have the intensity of their home regimens increased. Providers need to be educated about the use of PRN antihypertensive medication for the management of severe asymptomatic hypertension in the hospital setting.",success
37252732,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Asymptomatic blood pressure (BP) elevations are common in hospitalized older adults, and widespread heterogeneity in the clinical management of elevated inpatient BPs exists. To examine the association of intensive treatment of elevated inpatient BPs with in-hospital clinical outcomes of older adults hospitalized for noncardiac conditions. This retrospective cohort study examined Veterans Health Administration data between October 1, 2015, and December 31, 2017, for patients aged 65 years or older hospitalized for noncardiovascular diagnoses and who experienced elevated BPs in the first 48 hours of hospitalization. Intensive BP treatment following the first 48 hours of hospitalization, defined as receipt of intravenous antihypertensives or oral classes not used prior to admission. The primary outcome was a composite of inpatient mortality, intensive care unit transfer, stroke, acute kidney injury, B-type natriuretic peptide elevation, and troponin elevation. Data were analyzed between October 1, 2021, and January 10, 2023, with propensity score overlap weighting used to adjust for confounding between those who did and did not receive early intensive treatment. Among 66 140 included patients (mean [SD] age, 74.4 [8.1] years; 97.5% male and 2.6% female; 17.4% Black, 1.7% Hispanic, and 75.9% White), 14 084 (21.3%) received intensive BP treatment in the first 48 hours of hospitalization. Patients who received early intensive treatment vs those who did not continued to receive a greater number of additional antihypertensives during the remainder of their hospitalization (mean additional doses, 6.1 [95% CI, 5.8-6.4] vs 1.6 [95% CI, 1.5-1.8], respectively). Intensive treatment was associated with a greater risk of the primary composite outcome (1220 [8.7%] vs 3570 [6.9%]; weighted odds ratio [OR], 1.28; 95% CI, 1.18-1.39), with the highest risk among patients receiving intravenous antihypertensives (weighted OR, 1.90; 95% CI, 1.65-2.19). Intensively treated patients were more likely to experience each component of the composite outcome except for stroke and mortality. Findings were consistent across subgroups stratified by age, frailty, preadmission BP, early hospitalization BP, and cardiovascular disease history. The study's findings indicate that among hospitalized older adults with elevated BPs, intensive pharmacologic antihypertensive treatment was associated with a greater risk of adverse events. These findings do not support the treatment of elevated inpatient BPs without evidence of end organ damage, and they highlight the need for randomized clinical trials of inpatient BP treatment targets.",success
34148363,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,[Figure: see text].,success
26560085,False,Journal Article,,,,,,,,True,"Hospitalized patients with elevated blood pressure (BP) in most cases should be treated with intensification of oral regimens, but are often given intravenous (IV) antihypertensives. To determine frequency of prescribing and administering episodic IV antihypertensives and outcomes. Retrospective review. Urban academic hospital. Non-critically ill, hospitalized patients with an IV antihypertensive order for enalaprilat, labetalol, hydralazine, or metoprolol. We analyzed BP thresholds for ordering and administering IV antihypertensives, the types and frequencies of IV antihypertensives administered, and the effect of IV antihypertensive use on short-term BP and adverse outcomes. The BP change during hospitalization was contrasted in those receiving IV antihypertensives between those who did and did not receive subsequent intensification of chronic oral antihypertensive regimens. Two hundred forty-six patients had an episodic IV antihypertensive order. One hundred seventy-two patients received 458 doses, with 48% receiving a single dose. Over 98% of episodic IV antihypertensive doses were administered for systolic blood pressure (SBP) <200 mm Hg and 84.5% for SBP <180 mm Hg. Within 6 hours of administration, there was a statistically significant decline in average SBP and diastolic BP in patients receiving IV hydralazine and labetolol. After administration of IV antihypertensives, the oral inpatient medication regimen was adjusted in 52% of patients; these patients had a greater reduction in SBP from admission to discharge than patients with no change to their oral regimens. A total of 32.6% of patients receiving treatment experienced a BP reduction of more than 25% within 6 hours. IV antihypertensive drugs are ordered and administered in patients with asymptomatic, uncontrolled BP for levels unassociated with substantive immediate cardiovascular risk, which may cause adverse effects.",success
21237363,False,Journal Article,,,,,,,,True,"We describe clinician-reported knowledge of the Joint National Committee (JNC7) on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure definitions of Stage I hypertension; perceived causes of elevated blood pressure; barriers to blood pressure re-assessment; risk of adverse events associated with the elevated blood pressure. Health care providers from five emergency departments completed a questionnaire assessing knowledge of blood pressure criteria for hypertension, perceived causes of elevated blood pressures, barriers to re-assessment, and perceived risk of an adverse event at one year in a patient within three defined systolic and diastolic blood pressure ranges. Descriptive statistics were used to analyze the data. Seventy-two percent (379/524) of providers (68 attending physicians, 87 residents, 209 nurses, and 15 nurse practitioners) completed questionnaires. One hundred and four providers (27%) correctly listed the systolic and diastolic criteria for Stage 1 hypertension. Nurses and physicians rated uncontrolled, known hypertension [mean (standard deviation)] [8.7 (2.1), 8.9 (1.9)] the highest and pain [8.3 (2.3), 8.3 (2.1)] as the second highest cause of elevated BP. Nurses and physicians rated the lack of time to perform a reassessment [5.2 (3.4), 4.7 (2.8)] and a lack of adequate staffing [4.7 (3.4), 4.6 (2.9)] the highest as barriers to re-assessment. Nurses' mean adverse risk assessment twice that of physicians. Twenty seven percent of providers were aware of the JNC7 criteria and often attributed elevated blood pressures to chronic, uncontrolled hypertension, pain or anxiety. No single barrier to repeating elevated blood pressures was identified.",success
27623082,False,Letter,,,,,,,,False,,success
23842053,False,Comparative Study;Journal Article;Review,,,,,,,,True,"This clinical policy from the American College of Emergency Physicians is the revision of a 2006 policy on the evaluation and management of adult patients with asymptomatic elevated blood pressure in the emergency department.1 A writing subcommittee reviewed the literature to derive evidence-based recommendations to help clinicians answer the following critical questions: (1) In emergency department patients with asymptomatic elevated blood pressure, does screening for target organ injury reduce rates of adverse outcomes? (2) In patients with asymptomatic markedly elevated blood pressure, does emergency department medical intervention reduce rates of adverse outcomes? A literature search was performed, the evidence was graded, and recommendations were given based on the strength of the available data in the medical literature.",success
18852389,False,"Comparative Study;Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"This study assesses trends in hypertension prevalence, blood pressure distributions and mean levels, and hypertension awareness, treatment, and control among US adults, age >or=18 years, between the third National Health and Nutrition Examination Survey (1988-1994) and the 1999-2004 National Health and Nutrition Examination Survey, a period of approximately 10 years. The age-standardized prevalence rate increased from 24.4% to 28.9% (P<0.001), with the largest increases among non-Hispanic women. Depending on gender and race/ethnicity, from one fifth to four fifths of the increase could be accounted for by increasing body mass index. Among hypertensive persons, there were modest increases in awareness (P=0.04), from 68.5% to 71.8%. The rate for men increased from 61.6% to 69.3% (P=0.001), whereas the rate for women did not change significantly. Rates remained higher for women than for men, although the difference narrowed considerably. Improvements in treatment and control rates were larger: 53.1% to 61.4% and 26.1% to 35.1%, respectively (both P<0.001). The greatest increases occurred among non-Hispanic white men and non-Hispanic black persons, especially men. Mexican American persons showed improvement in treatment and control rates, but these rates remained the lowest among race/ethnic subgroups (47.4% and 24.3%, respectively). Among all of the race/ethnic groups, women continued to have somewhat better awareness, treatment, and control, except for control rates among non-Hispanic white persons, which became higher in men. Differences between non-Hispanic black and white persons in awareness, treatment, and control were small. These divergent trends may translate into disparate trends in cardiovascular disease morbidity and mortality.",success
15869118,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Most research on access to health care focuses on individual-level determinants such as income and insurance coverage. The role of community-level factors in helping or hindering individuals in obtaining needed care, however, has not received much attention. We address this gap in the literature by examining how neighborhood socioeconomic disadvantage is associated with access to health care. We find that living in disadvantaged neighborhoods reduces the likelihood of having a usual source of care and of obtaining recommended preventive services, while it increases the likelihood of having unmet medical need. These associations are not explained by the supply of health care providers. Furthermore, though controlling for individual-level characteristics reduces the association between neighborhood disadvantage and access to health care, a significant association remains. This suggests that when individuals who are disadvantaged are concentrated into specific areas, disadvantage becomes an ""emergent characteristic "" of those areas that predicts the ability of residents to obtain health care.",success
20699458,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"The potential effects of increasing numbers of uninsured and underinsured persons on US emergency departments (EDs) is a concern for the health care safety net. To describe the changes in ED visits that occurred from 1997 through 2007 in the adult and pediatric US populations by sociodemographic group, designation of safety-net ED, and trends in ambulatory care-sensitive conditions. Publicly available ED visit data from the National Hospital Ambulatory Medical Care Survey (NHAMCS) from 1997 through 2007 were stratified by age, sex, race, ethnicity, insurance status, safety-net hospital classification, triage category, and disposition. Codes from the International Classification of Diseases, Ninth Revision (ICD-9), were used to extract visits related to ambulatory care-sensitive conditions. Visit rates were calculated using annual US Census estimates. Total annual visits to US EDs and ED visit rates for population subgroups. Between 1997 and 2007, ED visit rates increased from 352.8 to 390.5 per 1000 persons (rate difference, 37.7; 95% confidence interval [CI], -51.1 to 126.5; P = .001 for trend); the increase in total annual ED visits was almost double of what would be expected from population growth. Adults with Medicaid accounted for most of the increase in ED visits; the visit rate increased from 693.9 to 947.2 visits per 1000 enrollees between 1999 and 2007 (rate difference, 253.3; 95% CI, 41.1 to 465.5; P = .001 for trend). Although ED visit rates for adults with ambulatory care-sensitive conditions remained stable, ED visit rates among adults with Medicaid increased from 66.4 in 1999 to 83.9 in 2007 (rate difference, 17.5; 95% CI, -5.8 to 40.8; P = .007 for trend). The number of facilities qualifying as safety-net EDs increased from 1770 in 2000 to 2489 in 2007. These findings indicate that ED visit rates have increased from 1997 to 2007 and that EDs are increasingly serving as the safety net for medically underserved patients, particularly adults with Medicaid.",success
31218526,False,"Journal Article;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"The purpose of this review is to describe the role of the pharmacist in innovative pathways of care for hypertension (HTN) management for emergency department (ED) patients, particularly in under-resourced communities. Due to intersecting socioeconomic and personal health risk factors, these patients bear a disproportionate share of cardiovascular disease, yet often have limited access to high-quality primary care. Recent meta-analyses demonstrate a clear advantage associated with pharmacist-physician collaborative models over traditional physician-only care in achieving blood pressure control. However, no prior study has evaluated use of pharmacist-led follow-up for ED patients with uncontrolled blood pressure (BP). Thus, we developed a pharmacist-driven transitional care clinic (TCC) that utilizes a collaborative practice agreement with ED physicians to improve HTN management for ED patients. We have successfully implemented the TCC in a high-volume urban ED and in a pilot study have shown clinically relevant BP reductions with our collaborative model. The use of pharmacist-led follow-up for HTN management is highly effective. Novel programs such as our TCC, which extend the reach of such a model to ED patients, are promising, and future studies should focus on implementation through larger, multicenter, randomized trials. However, to be most effective, policy advocacy is needed to expand pharmacist prescriptive authority and develop innovative financial models to incentivize this practice.",success
33484897,True,"Journal Article;Research Support, N.I.H., Extramural;Clinical Trial Protocol",NCT03749499,databank,NCT03749499,NCT03749499,NCT03749499,NCT03749499|databank,NCT03749499|databank,True,"Uncontrolled or undiagnosed hypertension (HTN) is estimated to be as high as 46% in emergency departments (EDs). Uncontrolled HTN contributes significantly to cardiovascular morbidity and disproportionately affects communities of color. EDs serve high risk populations with uncontrolled conditions that are often missed by other clinical settings and effective interventions for uncontrolled HTN in the ED are critically needed. The ED is well situated to decrease the disparities in HTN control by providing a streamlined intervention to high risk populations that may use the ED as their primary care. Targeting of UnControlled Hypertension in the Emergency Department (TOUCHED), is a two-arm single site randomized controlled trial of 770 adults aged 18-75 presenting to the ED with uncontrolled HTN comparing (1) usual care, versus (2) an Educational and Empowerment (E2) intervention that integrates a Post-Acute Care Hypertension Consultation (PACHT-c) with a mobile health BP self-monitoring kit. The primary outcome is differences in mean systolic blood pressure (SBP) at 6-months post enrollment. Secondary outcomes include differences in mean SBP and mean diastolic BP (DBP) at 3-months and mean DBP at 6-months. Additionally, improvement in cardiovascular risk score, medication adherence, primary care engagement, and HTN knowledge will also be assessed as part of this study. The TOUCHED trial will be instrumental in determining the effectiveness of a brief ED-based intervention that is portable to other urban EDs with high-risk populations. clinicaltrials.gov Identifier: NCT03749499.",success
30659702,True,"Journal Article;Randomized Controlled Trial;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",NCT02301455,databank,NCT02301455,NCT02301455,NCT02301455,NCT02301455|databank,NCT02301455|databank,True,"We aimed to assess the feasibility of a text messaging intervention by determining the proportion of emergency department (ED) patients who responded to prompted home blood pressure (BP) self-monitoring and had persistent hypertension. We also explored the effect of the intervention on systolic blood pressure (sBP) over time. We conducted a randomized, controlled trial of ED patients with expected discharge to home with elevated BP. Participants were identified by automated alerts from the electronic health record. Those who consented received a BP cuff to take home and enrolled in the 3-week screening phase. Text responders with persistent hypertension were randomized to control or weekly prompted BP self-monitoring and healthy behavior text messages. Among the 104 patients enrolled in the ED, 73 reported at least one home BP over the 3-week run-in (screening) period. A total of 55 of 73 reported a home BP of ≥140/90 and were randomized to SMS intervention (n = 28) or control (n = 27). The intervention group had significant sBP reduction over time with a mean drop of 9.1 mm Hg (95% confidence interval = 1.1 to 17.6). The identification of ED patients with persistent hypertension using home BP self-monitoring and text messaging was feasible. The intervention was associated with a decrease in sBP likely to be clinically meaningful. Future studies are needed to further refine this approach and determine its efficacy.",success
31115736,False,Journal Article;Review,,,,,,,,True,"To review community health worker (CHW) interventions tailored for hypertension management and to determine if the emergency department (ED) population would benefit from such interventions. When working with patients who have one or more chronic diseases, CHW interventions have been very successful in improving health outcomes and are cost-effective. CHWs use a variety of techniques to address social determinants that patients may face that effect how they manage their chronic disease(s). Current CHW interventions in the ED have targeted the ""super-user"" population. CHW-based interventions help address social determinants of patients in a variety of settings, especially in the ED where the physicians have limited resources and time. There is limited information about how CHWs can improve community health outcomes outside of the ED ""super-user"" population. Future research needs to determine if creating a data-driven CHW intervention for the ED would be effective.",success
26691646,True,"Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",NCT02069015,databank,NCT02069015,NCT02069015,NCT02069015,NCT02069015|databank,NCT02069015|databank,True,"Persistently elevated blood pressure (BP) is a leading risk factor for cardiovascular disease development, making effective hypertension management an issue of considerable public health importance. Hypertension is particularly prominent among African Americans, who have higher disease prevalence and consistently lower BP control than Whites and Hispanics. Emergency departments (ED) have limited resources for chronic disease management, especially for under-served patients dependent upon the ED for primary care, and are not equipped to conduct follow-up. Kiosk-based patient education has been found to be effective in primary care settings, but little research has been done on the effectiveness of interactive patient education modules as ED enhanced discharge for an under-served urban minority population. Achieving Blood Pressure Control Through Enhanced Discharge (AchieveBP) is a behavioral RCT patient education intervention for patients with a history of hypertension who have uncontrolled BP at ED discharge. The project will recruit up to 200 eligible participants at the ED, primarily African-American, who will be asked to return to a nearby clinical research center for seven, thirty and ninety day visits, with a 180 day follow-up. Consenting participants will be randomized to either an attention-control or kiosk-based interactive patient education intervention. To control for potential medication effects, all participants will be prescribed similar, evidenced-based anti-hypertensive regimens and have their prescription filled onsite at the ED and during visits to the clinic. The primary target endpoint will be success in achieving BP control assessed at 180 days follow-up post-ED discharge. The secondary aim will be to assess the relationship between patient activation and self-care management. The AchieveBP trial will determine whether using interactive patient education delivered through health information technology as ED enhanced discharge with subsequent education sessions at a clinic is an effective strategy for achieving short-term patient management of BP. The project is innovative in that it uses the ED as an initial point of service for kiosk-based health education designed to increase BP self-management. It is anticipated findings from this translational research could also be used as a resource for patient education and follow-up with hypertensive patients in primary care settings. ClinicalTrials.gov. NCT02069015. Registered February 19, 2014.",success
33216142,False,Journal Article,,,,,,,,True,"Optimal triage of patients with hypertensive urgency (HU) in the emergency department (ED) is not well established. 2017 ACC/AHA hypertension (HTN) guidelines recommend treatment initiation and follow-up within 1 week. Objectives of our pilot study were to evaluate feasibility and impact of directly connecting ED patients with HU to outpatient HTN management on blood pressure (BP) control and ED utilization. ED patients with HU and no primary care physician were scheduled by a referral coordinator for an initial appointment in a HTN clinic embedded within a primary care practice. BP control and ED utilization over the subsequent 90 days were tracked and compared with BP at time of the referral ED visit, and ED utilization in the 90 days preceding referral. Data are reported for the first 40 referred patients. Average time to first visit was 7.8 days. Mean age was 51 years (range 28-76), 75% were African-American, and mean pooled 10-year atherosclerotic cardiovascular disease (ASCVD) risk was 20.8%. Mean BP declined from 198/116 mm Hg at ED visit to 167/98 mm Hg at HTN clinic visit 1 to 136/83 by 6 weeks and was sustained at 90 days. Total ED visits for the group decreased from 61 in the 90 days prior to referral, to 18 in the 90 days after the first HTN clinic visit. In this pilot study, coordinated referral between the ED and primary care provides safe, timely care for this high ASCVD risk population and leads to sustained reductions in BP and ED utilization.",success
29637311,False,Journal Article;Review,,,,,,,,True,"Hypertension (HTN) is the most prevalent cardiovascular disease and poses a major population level risk to long-term health outcomes. Despite this critical importance, and the widespread availability of effective and affordable medications, blood pressure (BP) remains uncontrolled in up to 50% of the diagnosed patients. This problem is exacerbated in communities with limited access to primary care, who often utilize hospital emergency departments (EDs) as their primary healthcare resource. Despite the ubiquity of patients presenting to EDs with severely elevated BP, a unified, evidence-based approach is not yet widely implemented, and both under- and overtreatment are common. The purpose of this review is to describe an approach towards institutional policy regarding asymptomatic HTN, in which we will translate the accepted principles of appropriate outpatient BP management to ED and inpatient settings. Results from the recent SPRINT trial, and the subsequent publication of the American Heart Association updated guidelines for the treatment of HTN, significantly lower both the diagnostic threshold and the treatment goals for hypertensive patients. This change will drastically increase the proportion of patients presenting to EDs with newly diagnosed and uncontrolled HTN. Several recent studies emphasize the safety in outpatient management of patients with severely elevated BP in the absence of acute end-organ damage and, conversely, the long- and intermediate-term risk associated with these patients. System-based approaches, particularly those led by non-physicians, have shown the greatest promise in reducing population level uncontrolled HTN. Evidence-based approaches, such as those described in emergency medicine and cardiology society guidelines, can guide appropriate management of ED and inpatient BP elevations. Translating these patient oriented guidelines into institutional policy, and maintaining provider adherence, is a challenge across healthcare institutions. We present here several examples of successful policies developed and implemented by the authors. While brief inpatient and ED encounters cannot replace long-term outpatient care, they have the potential to serve as a crucial inlet to health care and an opportunity to optimize care.",success
37061798,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The American Heart Association funded a Health Equity Research Network on the prevention of hypertension, the RESTORE Network, as part of its commitment to achieving health equity in all communities. This article provides an overview of the RESTORE Network. The RESTORE Network includes five independent, randomized trials testing approaches to implement non-pharmacological interventions that have been proven to lower blood pressure (BP). The trials are community-based, taking place in churches in rural Alabama, mobile health units in Michigan, barbershops in New York, community health centers in Maryland, and food deserts in Massachusetts. Each trial employs a hybrid effectiveness-implementation research design to test scalable and sustainable strategies that mitigate social determinants of health (SDOH) that contribute to hypertension in Black communities. The primary outcome in each trial is change in systolic BP. The RESTORE Network Coordinating Center has five cores: BP measurement, statistics, intervention, community engagement, and training that support the trials. Standardized protocols, data elements and analysis plans were adopted in each trial to facilitate cross-trial comparisons of the implementation strategies, and application of a standard costing instrument for health economic evaluations, scale up, and policy analysis. Herein, we discuss future RESTORE Network research plans and policy outreach activities designed to advance health equity by preventing hypertension. The RESTORE Network was designed to promote health equity in the US by testing effective and sustainable implementation strategies focused on addressing SDOH to prevent hypertension among Black adults.",success
35474502,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"In 2014, hypertension guidelines for older adults endorsed increased use of fixed-dose combinations, prioritized thiazide diuretics and calcium channel blockers (CCBs) for Black patients, and no longer recommend beta-blockers as first-line therapy. To evaluate older adults' antihypertensive use following guideline changes. Time series analysis. Twenty percent national sample of Medicare Part D beneficiaries aged 66 years and older with hypertension. Eighth Joint National Committee (JNC8) guidelines MAIN MEASURES: Quarterly trends in prevalent and initial antihypertensive use were examined before (2008 to 2013) and after (2014 to 2017) JNC8. Analyses were conducted among all beneficiaries with hypertension, beneficiaries without chronic conditions that might influence antihypertensive selection (hypertension-only cohort), and among Black patients, given race-based guideline recommendations. The number of beneficiaries with hypertension increased from 1,978,494 in 2008 to 2,809,680 in 2017, the proportions using antihypertensives increased from 80.3 to 81.2%, and the proportion using multiple classes and fixed-dose combinations declined (60.8 to 58.1% and 20.7 to 15.1%, respectively, all P<.01). Prior to JNC8, the use of angiotensin-converting enzyme inhibitors, angiotensin receptor blockers, and CCBs was increasing. Use of CCBs as initial therapy increased more rapidly following JNC8 (relative change in quarterly trend 0.15% [95% CI, 0.13-0.18%), especially among Black beneficiaries (relative change 0.44% [95% CI, 0.21-0.68%]). Contrary to guidelines, the use of thiazides and combinations as initial therapy consistently decreased in the hypertension-only cohort (13.8 to 8.3% and 25.1 to 15.7% respectively). By 2017, 65.9% of Black patients in the hypertension-only cohort were initiated on recommended first-line or combination therapy compared to 80.3% of non-Black patients. Many older adults, particularly Black patients, continue to be initiated on antihypertensive classes not recommended as first-line, indicating opportunities to improve the effectiveness and equity of hypertension care and potentially reduce antihypertensive regimen complexity.",success
35098730,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Little is known about the relationship between social determinants of health (SDH) and medication adherence among Medicaid beneficiaries with hypertension. We conducted a posthoc subgroup analysis of 3044 adult Medicaid beneficiaries who enrolled in a parent prospective cohort study and had a diagnosis of hypertension based on their Medicaid claims during a 24-month period before study enrollment. We calculated the proportion of days covered by at least one antihypertensive medication during the first 12 months after study enrollment using the prescription claims data. We measured numerous SDH at the time of study enrollment and we categorized our hypertension cohort into 4 social risk groups based on their response profiles to the SDH variables. We compared the mean proportion of days covered by the different levels of the SDH factors. We modeled the odds of being covered by an antihypertensive medication daily throughout the follow-up period by social risk group, adjusted for age, sex, and disease severity using a generalized linear model. The nonrandom sample was predominately Black (93%), female (62%) and had completed high school (77%). The mean proportion of days covered varied significantly by different SDH, such as food insecurity (49%-56%), length of time living at present place (47%-57%), smoking status (50%-56%), etc. Social risk group was a significant predictor of medication adherence. Participants in the 2 groups with the most social risks were 36% (adjusted odds ratio=0.64 [95% CI, 0.53-0.78]) and 20% (adjusted odds ratio=0.80 [95% CI, 0.70-0.93]) less adherent to their hypertension therapy compared with participants in the group with the fewest social risks. Social risks are associated with lower antihypertensive medication adherence in the Medicaid population.",success
34807229,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,This cross-sectional study characterizes the prevalent use of medications that may raise BP and examine their associations with BP control and antihypertensive use.,success
33469765,False,"Letter;Research Support, N.I.H., Extramural",,,,,,,,False,,success
19272490,False,Journal Article;Meta-Analysis,,,,,,,,True,"To quantify the incremental effect of combining blood pressure-lowering drugs from any 2 classes of thiazides, beta-blockers, angiotensin-converting enzyme inhibitors, and calcium channel blockers over 1 drug alone and to compare the effects of combining drugs with doubling dose. Meta-analysis of factorial trials in which participants were randomly allocated to 1 drug alone, another drug alone, both drugs together, or a placebo. We identified 42 trials (10,968 participants). With a thiazide used alone, the mean placebo-subtracted reduction in systolic blood pressure was 7.3 mm Hg and 14.6 mm Hg combined with a drug from another class. The corresponding reductions were 9.3 mm Hg and 18.9 mm Hg with a beta-blocker, 6.8 mm Hg and 13.9 mm Hg with an angiotensin-converting enzyme, and 8.4 mm Hg and 14.3 mm Hg with a calcium channel blocker. The expected blood pressure reduction from 2 drugs together, assuming an additive effect, closely predicted the observed blood pressure reductions. The ratios of the observed to expected incremental blood pressure reductions from combining each class of drug with any other over that from 1 drug were, respectively, for thiazides, beta-blockers, angiotensin-converting enzyme inhibitors, and calcium channel blockers: 1.04 (95% confidence interval [CI], 0.88-1.20), 1.00 (95% CI, 0.76-1.24), 1.16 (95% CI, 0.93-1.39), and 0.89 (95% CI, 0.69-1.09); the overall average was 1.01 (95% CI, 0.90-1.12). Comparison of our results with those of a published meta-analysis of different doses of the same drug showed that doubling the dose of 1 drug had approximately one fifth of the equivalent incremental effect (0.22 [95% CI, 0.19-0.25]). Blood pressure reduction from combining drugs from these 4 classes can be predicted on the basis of additive effects. The extra blood pressure reduction from combining drugs from 2 different classes is approximately 5 times greater than doubling the dose of 1 drug.",success
37158068,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Background Fixed-dose combination (FDC) antihypertensive products improve blood pressure control and adherence among patients with hypertension. It is unknown to what degree commercially available FDC products meet the current hypertension management prescription patterns in the United States. Methods and Results This cross-sectional analysis of the National Health and Nutrition Examination Surveys 2015 to March 2020 included participants with hypertension taking ≥2 antihypertensive medications (N=2451). After constructing each participant's regimen according to antihypertensive classes used, we estimated the extent to which the 7 class-level FDC regimens available in the United States as of January 2023 would match the regimens used. Among a weighted population of 34.1 million US adults (mean age, 66.0 years; 52.8% women; 69.1% non-Hispanic White race and ethnicity), the proportions using 2, 3, 4, and ≥5 antihypertensive classes were 60.6%, 28.2%, 9.1%, and 1.6%, respectively. The 7 FDC regimens were among 189 total regimens used (3.7%), and 39.2% of the population used one of the FDC regimens (95% CI, 35.5%-43.0%; 13.4 million US adults); 60.8% of the population (95% CI, 57.0%-64.5%; 20.7 million US adults) were using a regimen not available as a class-equivalent FDC product. Conclusions Three in 5 US adults with hypertension taking ≥2 antihypertensive classes are using a regimen that is not commercially available as a class-equivalent FDC product as of January 2023. To maximize the potential benefit of FDCs to improve medication adherence (and thus blood pressure control) among patients taking multiple antihypertensive medications, use of FDC-compatible regimens and improvements in the product landscape are needed.",success
16157827,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Despite the national attention being given to the problem of medication safety, little attention has been paid to the medication problems that are encountered by older patients who are receiving care across settings. The objective of this study was to determine the prevalence and contributing factors associated with posthospital medication discrepancies. The study population consisted of community-dwelling adults aged 65 years and older admitted to the hospital with 1 of 9 selected conditions (n = 375). A geriatric nurse practitioner performed a comprehensive medication assessment in the patient's home within 24 to 72 hours after institutional discharge. The assessment focused on what older patients reported taking in comparison with the prehospital medication regimen and the posthospital medication regimen. Prevalence and types of medication discrepancies were categorized using the Medication Discrepancy Tool. A total of 14.1% of patients experienced 1 or more medication discrepancies. Using the Medication Discrepancy Tool, 50.8% of identified contributing factors for discrepancies were categorized as patient-associated, and 49.2% were categorized as system-associated. Five medication classes accounted for half of all medication discrepancies. Medication discrepancies were associated with the total number of medications taken and the presence of congestive heart failure. A total of 14.3% of the patients who experienced medication discrepancies were rehospitalized at 30 days compared with 6.1% of the patients who did not experience a discrepancy (P = .04). A significant percentage of older patients experienced medication discrepancies after making the transition from hospital to home. Both patient-associated and system-associated solutions may be needed to ensure medication safety during this vulnerable period.",success
28060037,False,Journal Article,,,,,,,,True,"Approximately 50% to 75% of hospital patients have hypertension. At the time of discharge, patients experience a transition of care as they move from the hospital to home. This article describes the transition of care from the hospital to home for patients with hypertension and discusses practice implications for NPs.",success
32917145,False,"Journal Article;Research Support, Non-U.S. Gov't;Systematic Review",,,,,,,,True,"Demographic changes are taking place in most industrialized countries. Geriatric patients are defined by the European Union of Medical Specialists as aged over 65 years and suffering from frailty and multi-morbidity, whose complexity puts a major burden on these patients, their family caregivers and the public health care system. To counteract negative outcomes and to maintain consistency in care between hospital and community dwelling, the transitional of care has emerged over the last several decades. Our objectives were to identify and summarize the components of the Transitional Care Model implemented with geriatric patients (aged over 65 years, with multi-morbidity) for the reduction of all-cause readmission. Another objective was to recognize the Transitional Care Model components' role and impact on readmission rate reduction on the transition of care from hospital to community dwelling (not nursing homes). Randomized controlled trials (sample size ≥50 participants per group; intervention period ≥30 days), with geriatric patients were included. Electronic databases (MEDLINE, CINAHL, PsycINFO and The Cochrane Central Register of Controlled Trials) were searched from January 1994 to December 2019 published in English or German. A qualitative synthesis of the findings as well as a systematic assessment of the interventions intensities was performed. Three articles met the inclusion criteria. One of the included trials applied all of the nine Transitional Care Model components described by Hirschman and colleagues and obtained a high-intensity level of intervention in the intensities assessment. This and another trial reported reductions in the readmission rate (p < 0.05), but the third trial did not report significant differences between the groups in the longer follow-up period (up to 12 months). Our findings suggest that high intensity multicomponent and multidisciplinary interventions are likely to be effective reducing readmission rates in geriatric patients, without increasing cost. Components such as type of staffing, assessing and managing symptoms, educating and promoting self-management, maintaining relationships and fostering coordination seem to have an important role in reducing the readmission rate. Research is needed to perform further investigations addressing geriatric patients well above 65 years old, to further understand the importance of individual components of the TCM in this population.",success
19318384,True,"Comparative Study;Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",NCT00220987,databank,NCT00220987,NCT00220987,NCT00220987,NCT00220987|databank,NCT00220987|databank,True,"The optimal target range for blood glucose in critically ill patients remains unclear. Within 24 hours after admission to an intensive care unit (ICU), adults who were expected to require treatment in the ICU on 3 or more consecutive days were randomly assigned to undergo either intensive glucose control, with a target blood glucose range of 81 to 108 mg per deciliter (4.5 to 6.0 mmol per liter), or conventional glucose control, with a target of 180 mg or less per deciliter (10.0 mmol or less per liter). We defined the primary end point as death from any cause within 90 days after randomization. Of the 6104 patients who underwent randomization, 3054 were assigned to undergo intensive control and 3050 to undergo conventional control; data with regard to the primary outcome at day 90 were available for 3010 and 3012 patients, respectively. The two groups had similar characteristics at baseline. A total of 829 patients (27.5%) in the intensive-control group and 751 (24.9%) in the conventional-control group died (odds ratio for intensive control, 1.14; 95% confidence interval, 1.02 to 1.28; P=0.02). The treatment effect did not differ significantly between operative (surgical) patients and nonoperative (medical) patients (odds ratio for death in the intensive-control group, 1.31 and 1.07, respectively; P=0.10). Severe hypoglycemia (blood glucose level, < or = 40 mg per deciliter [2.2 mmol per liter]) was reported in 206 of 3016 patients (6.8%) in the intensive-control group and 15 of 3014 (0.5%) in the conventional-control group (P<0.001). There was no significant difference between the two treatment groups in the median number of days in the ICU (P=0.84) or hospital (P=0.86) or the median number of days of mechanical ventilation (P=0.56) or renal-replacement therapy (P=0.39). In this large, international, randomized trial, we found that intensive glucose control increased mortality among adults in the ICU: a blood glucose target of 180 mg or less per deciliter resulted in lower mortality than did a target of 81 to 108 mg per deciliter. (ClinicalTrials.gov number, NCT00220987.)",success
37285157,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,True,"Wearable devices may be able to improve cardiovascular health, but the current adoption of these devices could be skewed in ways that could exacerbate disparities. To assess sociodemographic patterns of use of wearable devices among adults with or at risk for cardiovascular disease (CVD) in the US population in 2019 to 2020. This population-based cross-sectional study included a nationally representative sample of the US adults from the Health Information National Trends Survey (HINTS). Data were analyzed from June 1 to November 15, 2022. Self-reported CVD (history of heart attack, angina, or congestive heart failure) and CVD risk factors (≥1 risk factor among hypertension, diabetes, obesity, or cigarette smoking). Self-reported access to wearable devices, frequency of use, and willingness to share health data with clinicians (referred to as health care providers in the survey). Of the overall 9303 HINTS participants representing 247.3 million US adults (mean [SD] age, 48.8 [17.9] years; 51% [95% CI, 49%-53%] women), 933 (10.0%) representing 20.3 million US adults had CVD (mean [SD] age, 62.2 [17.0] years; 43% [95% CI, 37%-49%] women), and 5185 (55.7%) representing 134.9 million US adults were at risk for CVD (mean [SD] age, 51.4 [16.9] years; 43% [95% CI, 37%-49%] women). In nationally weighted assessments, an estimated 3.6 million US adults with CVD (18% [95% CI, 14%-23%]) and 34.5 million at risk for CVD (26% [95% CI, 24%-28%]) used wearable devices compared with an estimated 29% (95% CI, 27%-30%) of the overall US adult population. After accounting for differences in demographic characteristics, cardiovascular risk factor profile, and socioeconomic features, older age (odds ratio [OR], 0.35 [95% CI, 0.26-0.48]), lower educational attainment (OR, 0.35 [95% CI, 0.24-0.52]), and lower household income (OR, 0.42 [95% CI, 0.29-0.60]) were independently associated with lower use of wearable devices in US adults at risk for CVD. Among wearable device users, a smaller proportion of adults with CVD reported using wearable devices every day (38% [95% CI, 26%-50%]) compared with overall (49% [95% CI, 45%-53%]) and at-risk (48% [95% CI, 43%-53%]) populations. Among wearable device users, an estimated 83% (95% CI, 70%-92%) of US adults with CVD and 81% (95% CI, 76%-85%) at risk for CVD favored sharing wearable device data with their clinicians to improve care. Among individuals with or at risk for CVD, fewer than 1 in 4 use wearable devices, with only half of those reporting consistent daily use. As wearable devices emerge as tools that can improve cardiovascular health, the current use patterns could exacerbate disparities unless there are strategies to ensure equitable adoption.",success
22052898,False,"Journal Article;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"There are several challenges in encoding guideline knowledge in a form that is portable to different clinical sites, including the heterogeneity of clinical decision support (CDS) tools, of patient data representations, and of workflows. We have developed a multi-layered knowledge representation framework for structuring guideline recommendations for implementation in a variety of CDS contexts. In this framework, guideline recommendations are increasingly structured through four layers, successively transforming a narrative text recommendation into input for a CDS system. We have used this framework to implement rules for a CDS service based on three guidelines. We also conducted a preliminary evaluation, where we asked CDS experts at four institutions to rate the implementability of six recommendations from the three guidelines. The experience in using the framework and the preliminary evaluation indicate that this approach has promise in creating structured knowledge, to implement in CDS systems, that is usable across organizations.",success
30302270,False,Journal Article;Review,,,,,,,,True,"Clinical practice guidelines (CPGs) document evidence-based information and recommendations on treatment and management of conditions. CPGs usually focus on management of a single condition; however, in many cases a patient will be at the centre of multiple health conditions (multimorbidity). Multiple CPGs need to be followed in parallel, each managing a separate condition, which often results in instructions that may interact with each other, such as conflicts in medication. Furthermore, the impetus to deliver customised care based on patient-specific information, results in the need to be able to offer guidelines in an integrated manner, identifying and managing their interactions. In recent years, CPGs have been formatted as computer-interpretable guidelines (CIGs). This enables developing CIG-driven clinical decision support systems (CDSSs), which allow the development of IT applications that contribute to the systematic and reliable management of multiple guidelines. This study focuses on understanding the use of CIG-based CDSSs, in order to manage care complexities of patients with multimorbidity. The literature between 2011 and 2017 is reviewed, which covers: (a) the challenges and barriers in the care of multimorbid patients, (b) the role of CIGs in CDSS augmented delivery of care, and (c) the approaches to alleviating care complexities of multimorbid patients. Generating integrated care plans, detecting and resolving adverse interactions between treatments and medications, dealing with temporal constraints in care steps, supporting patient-caregiver shared decision making and maintaining the continuity of care are some of the approaches that are enabled using a CIG-based CDSS.",success
32241375,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Ambulatory monitoring devices are enabling a new paradigm of health care by collecting and analyzing long-term data for reliable diagnostics. These devices are becoming increasingly popular for continuous monitoring of cardiac diseases. Recent advancements have enabled solutions that are both affordable and reliable, allowing monitoring of vulnerable populations from the comfort of their homes. They provide early detection of important physiological events, leading to timely alerts for seeking medical attention. In this review, the authors aim to summarize the recent developments in the area of ambulatory and remote monitoring solutions for cardiac diagnostics. The authors cover solutions based on wearable devices, smartphones, and other ambulatory sensors. The authors also present an overview of the limitations of current technologies, their effectiveness, and their adoption in the general population, and discuss some of the recently proposed methods to overcome these challenges. Lastly, we discuss the possibilities opened by this new paradigm, for the future of health care and personalized medicine.",success
28495301,False,Journal Article,,,,,,,,True,"Ambulatory ECG (AECG) is very commonly employed in a variety of clinical contexts to detect cardiac arrhythmias and/or arrhythmia patterns which are not readily obtained from the standard ECG. Accurate and timely characterization of arrhythmias is crucial to direct therapies that can have an important impact on diagnosis, prognosis or patient symptom status. The rhythm information derived from the large variety of AECG recording systems can often lead to appropriate and patient-specific medical and interventional management. The details in this document provide background and framework from which to apply AECG techniques in clinical practice, as well as clinical research.",success
34549333,False,Journal Article;Systematic Review,,,,,,,,True,"We aimed to systematically review the available literature on mobile Health (mHealth) solutions, including handheld and wearable devices, implantable loop recorders (ILRs), as well as mobile platforms and support systems in atrial fibrillation (AF) detection and management. This systematic review was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guidelines. The electronic databases PubMed (NCBI), Embase (Ovid), and Cochrane were searched for articles published until 10 February 2021, inclusive. Given that the included studies varied widely in their design, interventions, comparators, and outcomes, no synthesis was undertaken, and we undertook a narrative review. We found 208 studies, which were deemed potentially relevant. Of these studies included, 82, 46, and 49 studies aimed at validating handheld devices, wearables, and ILRs for AF detection and/or management, respectively, while 34 studies assessed mobile platforms/support systems. The diagnostic accuracy of mHealth solutions differs with respect to the type (handheld devices vs wearables vs ILRs) and technology used (electrocardiography vs photoplethysmography), as well as application setting (intermittent vs continuous, spot vs longitudinal assessment), and study population. While the use of mHealth solutions in the detection and management of AF is becoming increasingly popular, its clinical implications merit further investigation and several barriers to widespread mHealth adaption in healthcare systems need to be overcome. Mobile health solutions for atrial fibrillation detection and management: a systematic review.",success
28851729,True,Journal Article;Randomized Controlled Trial,,,,,,,,True,"Asymptomatic atrial fibrillation (AF) is increasingly common in the aging population and implicated in many ischemic strokes. Earlier identification of AF with appropriate anticoagulation may decrease stroke morbidity and mortality. We conducted a randomized controlled trial of AF screening using an AliveCor Kardia monitor attached to a WiFi-enabled iPod to obtain ECGs (iECGs) in ambulatory patients. Patients ≥65 years of age with a CHADS-VASc score ≥2 free from AF were randomized to the iECG arm or routine care (RC). iECG participants acquired iECGs twice weekly over 12 months (plus additional iECGs if symptomatic) onto a secure study server with overread by an automated AF detection algorithm and by a cardiac physiologist and/or consultant cardiologist. Time to diagnosis of AF was the primary outcome measure. The overall cost of the devices, ECG interpretation, and patient management were captured and used to generate the cost per AF diagnosis in iECG patients. Clinical events and patient attitudes/experience were also evaluated. We studied 1001 patients (500 iECG, 501 RC) who were 72.6±5.4 years of age; 534 were female. Mean CHADS-VASc score was 3.0 (heart failure, 1.4%; hypertension, 54%; diabetes mellitus, 30%; prior stroke/transient ischemic attack, 6.5%; arterial disease, 15.9%; all CHADS-VASc risk factors were evenly distributed between groups). Nineteen patients in the iECG group were diagnosed with AF over the 12-month study period versus 5 in the RC arm (hazard ratio, 3.9; 95% confidence interval=1.4-10.4; <i>P</i>=0.007) at a cost per AF diagnosis of $10 780 (£8255). There was a similar number of stroke/transient ischemic attack/systemic embolic events (6 versus 10, iECG versus RC; hazard ratio=0.61; 95% confidence interval=0.22-1.69; <i>P</i>=0.34). The majority of iECG patients were satisfied with the device, finding it easy to use without restricting activities or causing anxiety. Screening with twice-weekly single-lead iECG with remote interpretation in ambulatory patients ≥65 years of age at increased risk of stroke is significantly more likely to identify incident AF than RC over a 12-month period. This approach is also highly acceptable to this group of patients, supporting further evaluation in an appropriately powered, event-driven clinical trial. URL: https://www.isrctn.com. Unique identifier: ISRCTN10709813.",success
34469764,True,Journal Article;Multicenter Study;Randomized Controlled Trial,NCT01593553,databank,NCT01593553,NCT01593553,NCT01593553,NCT01593553|databank,NCT01593553|databank,True,"Atrial fibrillation is a leading cause of ischaemic stroke. Early detection of atrial fibrillation can enable anticoagulant therapy to reduce ischaemic stroke and mortality. In this randomised study in an older population, we aimed to assess whether systematic screening for atrial fibrillation could reduce mortality and morbidity compared with no screening. STROKESTOP was a multicentre, parallel group, unmasked, randomised controlled trial done in Halland and Stockholm in Sweden. All 75-76-year-olds residing in these two regions were randomly assigned (1:1) to be invited to screening for atrial fibrillation or to a control group. Participants attended local screening centres and those without a history of atrial fibrillation were asked to register intermittent electrocardiograms (ECGs) for 14 days. Treatment with oral anticoagulants was offered if atrial fibrillation was detected or untreated. All randomly assigned individuals were followed up in the intention-to-treat analysis for a minimum of 5 years for the primary combined endpoint of ischaemic or haemorrhagic stroke, systemic embolism, bleeding leading to hospitalisation, and all-cause death. This trial is registered with ClinicalTrials.gov, NCT01593553. From March 1, 2012, to May 28, 2014, 28 768 individuals were assessed for eligibility and randomly assigned to be invited to screening (n=14 387) or the control group (n=14 381). 408 individuals were excluded from the intervention group and 385 were excluded from the control group due to death or migration before invitation. There was no loss to follow-up. Of those invited to screening, 7165 (51·3%) of 13 979 participated. After a median follow-up of 6·9 years (IQR 6·5-7·2), significantly fewer primary endpoint events occurred in the intervention group (4456 [31·9%] of 13 979; 5·45 events per 100 years [95% CI 5·52-5·61]) than in the control group (4616 [33·0%] of 13 996; 5·68 events per 100 years [5·52-5·85]; hazard ratio 0·96 [95% CI 0·92-1·00]; p=0·045). Screening for atrial fibrillation showed a small net benefit compared with standard of care, indicating that screening is safe and beneficial in older populations. Stockholm County Council, the Swedish Heart & Lung Foundation, King Gustav V and Queen Victoria's Freemasons' Foundation, the Klebergska Foundation, the Tornspiran Foundation, the Scientific Council of Halland Region, the Southern Regional Healthcare Committee, the Swedish Stroke Fund, Carl Bennet AB, Boehringer Ingelheim, Bayer, and Bristol Myers Squibb-Pfizer.",success
36031651,True,Journal Article;Randomized Controlled Trial,NCT04250220,databank,NCT04250220,NCT04250220,NCT04250220,NCT04250220|databank,NCT04250220|databank,True,"Digital smart devices have the capability of detecting atrial fibrillation (AF), but the efficacy of this type of digital screening has not been directly compared to usual care for detection of treatment-relevant AF. In the eBRAVE-AF trial ( NCT04250220 ), we randomly assigned 5,551 policyholders of a German health insurance company who were free of AF at baseline (age 65 years (median; interquartile range (11) years, 31% females)) to digital screening (n = 2,860) or usual care (n = 2,691). In this siteless trial, for digital screening, participants used a certified app on their own smartphones to screen for irregularities in their pulse waves. Abnormal findings were evaluated by 14-day external electrocardiogram (ECG) loop recorders. The primary endpoint was newly diagnosed AF within 6 months treated with oral anti-coagulation by an independent physician not involved in the study. After 6 months, participants were invited to cross-over for a second study phase with reverse assignment for secondary analyses. The primary endpoint of the trial was met, as digital screening more than doubled the detection rate of treatment-relevant AF in both phases of the trial, with odds ratios of 2.12 (95% confidence interval (CI), 1.19-3.76; P = 0.010) and 2.75 (95% CI, 1.42-5.34; P = 0.003) in the first and second phases, respectively. This digital screening technology provides substantial benefits in detecting AF compared to usual care and has the potential for broad applicability due to its wide availability on ordinary smartphones. Future studies are needed to test whether digital screening for AF leads to better treatment outcomes.",success
33838317,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Validation Study",,,,,,,,True,"Consumer devices with broad reach may be useful in screening for atrial fibrillation (AF) in appropriate populations. However, currently no consumer devices are capable of continuous monitoring for AF. The purpose of this study was to estimate the sensitivity and specificity of a smartwatch algorithm for continuous detection of AF from sinus rhythm in a free-living setting. We studied a commercially available smartwatch with photoplethysmography (W-PPG) and electrocardiogram (W-ECG) capabilities. We validated a novel W-PPG algorithm combined with a W-ECG algorithm in a free-living setting, and compared the results to those of a 28-day continuous ECG patch (P-ECG). A total of 204 participants completed the free-living study, recording 81,944 hours with both P-ECG and smartwatch measurements. We found sensitivity of 87.8% (95% confidence interval [CI] 83.6%-91.0%) and specificity of 97.4% (95% CI 97.1%-97.7%) for the W-PPG algorithm (every 5-minute classification); sensitivity of 98.9% (95% CI 98.1%-99.4%) and specificity of 99.3% (95% CI 99.1%-99.5%) for the W-ECG algorithm; and sensitivity of 96.9% (95% CI 93.7%-98.5%) and specificity of 99.3% (95% CI 98.4%-99.7%) for W-PPG triggered W-ECG with a single W-ECG required for confirmation of AF. We found a very strong correlation of W-PPG in quantifying AF burden compared to P-ECG (r = 0.98). Our findings demonstrate that a novel algorithm using a commercially available smartwatch can continuously detect AF with excellent performance and that confirmation with W-ECG further enhances specificity. In addition, our W-PPG algorithm can estimate AF burden. Further research is needed to determine whether this algorithm is useful in screening for AF in select at-risk patients.",success
30062178,False,Journal Article,,,,,,,,True,"Ordinary cuff-based blood pressure-monitoring devices remain a technical limitation that disturbs activities of daily life. Here we report a novel system for the cuff-less blood pressure estimation (CLB) that requires only 1 sensor for photoplethysmography. The present study is the first report to validate and assess the clinical application of the CLB in accordance with the latest wearable device standard (issued by the Institute of Electrical and Electronics Engineers, standard 1708-2014). Our CLB is expected to offer a flexible and wearable device that permits blood pressure monitoring in more continuous and stress-free settings.",success
28267041,False,Evaluation Study;Journal Article,,,,,,,,True,"Current aortic SBP estimation methods require recording of a peripheral pressure waveform, a step with no consensus on method. This study investigates the possibility of aortic SBP estimation from radial SBP and DBP using artificial neural networks (ANN) with [ANNSBP.DBP.heart rate (HR)] and without HR (ANNSBP.DBP). Ten-fold cross validation was applied to invasive, simultaneously recorded aortic and radial pressure during rest and nitroglycerin infusion (n = 62 patients). The results of the ANN models were compared with an ANN model using additional waveform features (ANNwaveform), to an N-point moving average method (NPMA) and to existing, validated generalized transfer function (GTF). Estimated aortic SBP for all methods was on average less than 1 mmHg away from measured aortic SBP with the exception of NPMA (difference 2.0 ± 3.5 mmHg, P = 0.62). Variability of the difference was significantly greater in ANNSBP.DBP.HR and ANNSBP.DBP (both SD of ± 5.9 mmHg, P < 0.001 compared with GTF, ± 4.0 mmHg, P < 0.001). Inclusion of waveform features decreased the variability (ANNwaveform ± 3.9 mmHg, P = 0.264). Estimated aortic SBP in all models was correlated with measured SBP, with ANN models providing statistically similar results to the GTF method, only the NPMA being statistically different (P = 0.031). These findings indicate that use of radial SBP, DBP, and HR alone can provide aortic SBP estimation comparable with the GTF, albeit with slightly greater variance. Pending noninvasive validation, the technique provides plausible aortic SBP estimation without waveform analysis.",success
23647092,False,Journal Article;Review,,,,,,,,True,"1. Hypertension and atrial fibrillation (AF) often coexist and are strong risk factors for stroke. Current guidelines for blood pressure (BP) measurement in AF recommend repeated measurements using the auscultatory method, whereas the accuracy of the automated devices is regarded as questionable. This review presents the current evidence on the feasibility and accuracy of automated BP measurement in the presence of AF and the potential for automated detection of undiagnosed AF during such measurements. 2. Studies evaluating the use of automated BP monitors in AF are limited and have significant heterogeneity in methodology and protocols. Overall, the oscillometric method is feasible for static (office or home) and ambulatory use and appears to be more accurate for systolic than diastolic BP measurement. 3. Given that systolic hypertension is particularly common and important in the elderly, the automated BP measurement method may be acceptable for self-home and ambulatory monitoring, but not for professional office or clinic measurement. 4. An embedded algorithm for the detection of asymptomatic AF during routine automated BP measurement with high diagnostic accuracy has been developed and appears to be a useful screening tool for elderly hypertensives.",success
37338427,False,Journal Article,,,,,,,,True,"Life stressors have been linked to cardiovascular risk; however, studies typically focus on stressors that directly impact the individual, that is, personal stressors. Research suggests that women, particularly African-American women, may be more vulnerable to network stressors that involve family members and friends-potentially due to norms around needing to be a ""Superwoman."" Yet few studies have examined these phenomena. We examined associations between network, versus personal, stressors, and elevated blood pressure (BP) in N = 392 African-American women aged 30-46. Questionnaire-assessed negative life events were classified into upsetting network or personal stressors. BP was assessed in clinic and via 48-hr ambulatory monitoring. Linear and logistic regression models examined associations between type of stressors and 48-hr daytime and nighttime systolic BP (SBP) and diastolic BP (DBP), and sustained hypertension after adjusting for relevant covariates. Interactions with questionnaire-assessed superwoman schema (SWS) were tested in exploratory analyses. In age and sociodemographic-adjusted models, network stressors were significantly associated with daytime SBP, β (SE) = 2.01 (0.51), p ≤ .0001, and DBP, β (SE) = 1.59 (0.37), p ≤ .0001, but personal stressors were not (p values > .10). Associations persisted after adjustment for cardiovascular and psychosocial risk factors. Patterns were similar for nighttime BP and sustained hypertension. There were no interactions with SWS. Network, but not personal, stressors were associated with elevated rates of daytime SBP and DBP, as well as sustained hypertension in African-American women, irrespective of SWS endorsement. Future research is needed to determine whether stress-management interventions focused on network stressors might impact BP in this high-risk population. (PsycInfo Database Record (c) 2023 APA, all rights reserved).",success
33431425,False,Journal Article;Review,,,,,,,,True,"The incidence of heart failure (HF) remains high and patients with HF are at risk for frequent hospitalisations. Remote monitoring technologies may provide early indications of HF decompensation and potentially allow for optimisation of therapy to prevent HF hospitalisations. The need for reliable remote monitoring technology has never been greater as the COVID-19 pandemic has led to the rapid expansion of a new mode of healthcare delivery: the virtual visit. With the convergence of remote monitoring technologies and reliable method of remote healthcare delivery, an understanding of the role of both in the management of patients with HF is critical. In this review, we outline the evidence on current remote monitoring technologies in patients with HF and highlight how these advances may benefit patients in the context of the current pandemic.",success
31555544,False,Journal Article;Review,,,,,,,,True,"Implantable devices have been developed for continuous monitoring of heart failure. We investigated the effect of fluids and hemodynamic monitoring, using these devices, on heart failure clinical outcomes. Literature search was performed January 2000 through May 2017 of studies comparing device monitored patients with control group. Random-effects meta-analysis was used to pool outcomes across the studies. A total of 5,454 patients were included from 14 studies. There was no difference in heart failure (HF)-related admissions rate [odds ratio (OR) 1.25, 95% CI: 0.92-1.69, P=0.15], all-cause mortality (OR 1.21, 95% CI: 0.91-1.61, P=0.20) or combined admission rate and all-cause mortality (OR 1.21, 95% CI: 0.89-1.64, P=0.22) between the device monitored and the control group. In a subgroup analysis including only pressure sensors devices, there was no difference in all-cause mortality (OR 1.04, 95% CI: 0.62-1.74, P=0.89), however, there was a lower admissions rate (OR 1.63, 95% CI: 1.10-2.41, P=0.02). In a subgroup of only impedance monitoring devices, there was no difference in all-cause mortality or admissions rate. Pressure monitoring was associated with lower HF admissions rate. No improvement in these outcomes was noted with impedance monitoring.",success
31090869,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"In a randomized clinical trial, heart failure (HF) hospitalizations were lower in patients managed with guidance from an implantable pulmonary artery pressure sensor compared with usual care. It remains unclear if ambulatory monitoring could also improve long-term clinical outcomes in real-world practice. To determine the association between ambulatory hemodynamic monitoring and rates of HF hospitalization at 12 months in clinical practice. This matched cohort study of Medicare beneficiaries used claims data collected between June 1, 2014, and March 31, 2016. Medicare patients who received implants of a pulmonary artery pressure sensor were identified from the 100% Medicare claims database. Each patient who received an implant was matched to a control patient by demographic features, history of HF hospitalization, and number of all-cause hospitalizations. Propensity scoring based on comorbidities (arrhythmia, hypertension, diabetes, pulmonary disease, and renal disease) was used for additional matching. Data analysis was completed from July 2017 through January 2019. Implantable pulmonary artery pressure monitoring system. The rates of HF hospitalization were compared using the Andersen-Gill method. Days lost owing to events were compared using a nonparametric bootstrap method. The study cohort consisted of 1087 patients who received an implantable pulmonary artery pressure sensors and 1087 matched control patients. The treatment and control cohorts were well matched by age (mean [SD], 72.7 [10.2] years vs 72.9 [10.1] years) and sex (381 of 1087 female patients [35.1%] in each group), medical history, comorbidities, and timing of preimplant HF hospitalization. At 12 months postimplant, 616 HF hospitalizations occurred in the treatment cohort compared with 784 HF hospitalizations in the control cohort. The rate of HF hospitalization was lower in the treatment cohort at 12 months postimplant (hazard ratio [HR], 0.76 [95% CI, 0.65-0.89]; P < .001). The percentage of days lost to HF hospitalizations or death were lower in the treatment group (HR, 0.73 [95% CI, 0.64-0.84]; P < .001) and the percentage of days lost owing to all-cause hospitalization or death were also lower (HR, 0.77 [95% CI, 0.68-0.88]; P < .001). Patients with HF who were implanted with a pulmonary artery pressure sensor had lower rates of HF hospitalization than matched controls and spent more time alive out of hospital. Ambulatory hemodynamic monitoring may improve outcomes in patients with chronic HF.",success
31986289,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"The rate-limiting step in STEMI diagnosis often is the availability of a 12-lead electrocardiogram (ECG) and its interpretation. The potential may exist to speed the availability of 12-lead ECG information by using commonly available mobile technologies. We sought to test whether combining serial smartphone single-lead ECGs to create a virtual 12-lead ECG can accurately diagnose STEMI. Consenting patients presenting with symptoms consistent with a possible STEMI had contemporaneous standard 12-lead and smartphone '12-lead equivalent' ECG (produced by electronically combining serial single-lead ECGs) recordings obtained. Matched ECGs were evaluated qualitatively and quantitatively by a panel of blinded readers and classified as STEMI/STEMI equivalent (LBBB), Not-STEMI, or uninterpretable. Interpretable ECG pairs were graded as showing good, fair, or poor correlation. Two hundred four subjects (age = 60 years, males = 57%, STEMI activation = 45%) were enrolled from 5 international sites. Smartphone ECG quality was graded as good in 151 (74.0%), fair in 32 (15.7%), poor in 8 (3.9%), and uninterpretable in 13 (6.4%). A STEMI/STEMI equivalent diagnosis was identified by standard 12-lead ECG in 57/204 (27.9%) recordings. For all interpretable pairs of smartphone ECGs compared with standard ECGs (n = 190), the sensitivity, specificity, and positive and negative predictive values for STEMI/STEMI equivalent by smartphone were 0.89, 0.84, 0.70 and 0.95, respectively. A '12-lead equivalent' ECG obtained from multiple serial single-lead ECGs from a smartphone can identify STEMI with good correlation to a standard 12-lead ECG. This technology holds promise to improve outcomes in STEMI by enhancing the reach and speed of diagnosis and thereby early treatment.",success
35924875,False,Journal Article;Review,,,,,,,,True,"Although the outcome after myocardial infarction depends on the time to treatment, a delay between symptom onset and treatment is common. Apple Watch, a popular wearable device, provides the ability to perform an electrocardiogram. We review the progress made in using the Apple Watch to record multiple electrocardiogram leads for diagnosing myocardial infarction. Although the data are encouraging, many limitations remain, and more research is needed. Nevertheless, the Apple Watch could eventually serve as a self-check tool for patients who have chest pains or other symptoms of myocardial infarction, thus substantially decreasing the time to treatment and improving the outcome after myocardial infarction.",success
33672658,False,Journal Article,,,,,,,,True,"Electrochemically based technologies are rapidly moving from the laboratory to bedside applications and wearable devices, like in the field of cardiovascular disease. Major efforts have focused on the biosensor component in contrast with those employed in creating more suitable detection algorithms for long-term real-world monitoring solutions. The calibration curve procedure presents major limitations in this context. The objective is to propose a new algorithm, compliant with current clinical guidelines, which can overcome these limitations and contribute to the development of trustworthy wearable or telemonitoring solutions for home-based care. A total of 123 samples of phosphate buffer solution were spiked with different concentrations of troponin, the gold standard method for the diagnosis of the acute coronary syndrome. These were classified as normal or abnormal according to established clinical cut-off values. Off-the-shelf screen-printed electrochemical sensors and cyclic voltammetry measurements (sweep between -1 and 1 V in a 5 mV step) was performed to characterize the changes on the surface of the biosensor and to measure the concentration of troponin in each sample. A logistic regression model was developed to accurately classify these samples as normal or abnormal. The model presents high predictive performance according to specificity (94%), sensitivity (92%), precision (92%), recall (92%), negative predictive value (94%) and F-score (92%). The area under the curve of the precision-recall curve is 97% and the positive and negative likelihood ratios are 16.38 and 0.082, respectively. Moreover, high discriminative power is observed from the discriminate odd ratio (201) and the Youden index (0.866) values. The promising performance of the proposed algorithm suggests its capability to overcome the limitations of the calibration curve procedure and therefore its suitability for the development of trustworthy home-based care solutions.",success
35461692,False,"Journal Article;Systematic Review;Research Support, Non-U.S. Gov't",,,,,,,,True,"Containing the COVID-19 pandemic requires rapidly identifying infected individuals. Subtle changes in physiological parameters (such as heart rate, respiratory rate, and skin temperature), discernible by wearable devices, could act as early digital biomarkers of infections. Our primary objective was to assess the performance of statistical and algorithmic models using data from wearable devices to detect deviations compatible with a SARS-CoV-2 infection. We searched MEDLINE, Embase, Web of Science, the Cochrane Central Register of Controlled Trials (known as CENTRAL), International Clinical Trials Registry Platform, and ClinicalTrials.gov on July 27, 2021 for publications, preprints, and study protocols describing the use of wearable devices to identify a SARS-CoV-2 infection. Of 3196 records identified and screened, 12 articles and 12 study protocols were analysed. Most included articles had a moderate risk of bias, as per the National Institute of Health Quality Assessment Tool for Observational and Cross-Sectional Studies. The accuracy of algorithmic models to detect SARS-CoV-2 infection varied greatly (area under the curve 0·52-0·92). An algorithm's ability to detect presymptomatic infection varied greatly (from 20% to 88% of cases), from 14 days to 1 day before symptom onset. Increased heart rate was most frequently associated with SARS-CoV-2 infection, along with increased skin temperature and respiratory rate. All 12 protocols described prospective studies that had yet to be completed or to publish their results, including two randomised controlled trials. The evidence surrounding wearable devices in the early detection of SARS-CoV-2 infection is still in an early stage, with a limited overall number of studies identified. However, these studies show promise for the early detection of SARS-CoV-2 infection. Large prospective, and preferably controlled, studies recruiting and retaining larger and more diverse populations are needed to provide further evidence.",success
34913871,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Digital health technologies, such as smartphones and wearable devices, promise to revolutionize disease prevention, detection, and treatment. Recently, there has been a surge of digital health studies where data are collected through a bring-your-own-device (BYOD) approach, in which participants who already own a specific technology may voluntarily sign up for the study and provide their digital health data. BYOD study design accelerates the collection of data from a larger number of participants than cohort design; this is possible because researchers are not limited in the study population size based on the number of devices afforded by their budget or the number of people familiar with the technology. However, the BYOD study design may not support the collection of data from a representative random sample of the target population where digital health technologies are intended to be deployed. This may result in biased study results and biased downstream technology development, as has occurred in other fields. In this viewpoint paper, we describe demographic imbalances discovered in existing BYOD studies, including our own, and we propose the Demographic Improvement Guideline to address these imbalances.",success
36963907,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,False,,success
36292252,False,Journal Article;Review,,,,,,,,True,"With the significant numbers of sudden home deaths reported worldwide due to coronavirus disease 2019 (COVID-19), wearable technology has emerged as a method for surveilling this infection. This review explored the indicators of COVID-19 surveillance, such as vitals, respiratory condition, temperature, oxygen saturation (SpO<sub>2</sub>), and activity levels using wearable devices. Studies published between 31 December 2019, and 8 July 2022, were obtained from PubMed, and grey literature, reference lists, and key journals were also searched. All types of articles with the keywords ""COVID-19"", ""Diagnosis"", and ""Wearable Devices"" were screened. Four reviewers independently screened the articles against the eligibility criteria and extracted the data using a data charting form. A total of 56 articles were on monitoring, of which 28 included SpO<sub>2</sub> as a parameter. Although wearable devices are effective in the continuous monitoring of COVID-19 patients, further research on actual patients is necessary to determine the efficiency and effectiveness of wearable technology before policymakers can mandate its use.",success
34973948,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Clinical events adjudication is pivotal for generating consistent and comparable evidence in clinical trials. The methodology of event adjudication is evolving, but research is needed to develop best practices and spur innovation. A meeting of stakeholders from regulatory agencies, academic and contract research organizations, pharmaceutical and device companies, and clinical trialists convened in Chicago, IL, for Clinical Events Classification (CEC) Summit 2018 to discuss key topics and future directions. Formal studies are lacking on strategies to optimize CEC conduct, improve efficiency, minimize cost, and generally increase the speed and accuracy of the event adjudication process. Major challenges to CEC discussed included ensuring rigorous quality of the process, identifying safety events, standardizing event definitions, using uniform strategies for missing information, facilitating interactions between CEC members and other trial leadership, and determining the CEC's role in pragmatic trials or trials using real-world data. Consensus recommendations from the meeting include the following: (1) ensure an adequate adjudication infrastructure; (2) use negatively adjudicated events to identify important safety events reported only outside the scope of the primary endpoint; (3) conduct further research in the use of artificial intelligence and digital/mobile technologies to streamline adjudication processes; and (4) emphasize the importance of standardizing event definitions and quality metrics of CEC programs. As novel strategies for clinical trials emerge to generate evidence for regulatory approval and to guide clinical practice, a greater understanding of the role of the CEC process will be critical to optimize trial conduct and increase confidence in the data generated.",success
32387247,False,Journal Article,,,,,,,,True,"Many of the drugs being used in the treatment of the ongoing pandemic coronavirus disease 2019 (COVID-19) are associated with QT prolongation. Expert guidance supports electrocardiographic (ECG) monitoring to optimize patient safety. The purpose of this study was to establish an enhanced process for ECG monitoring of patients being treated for COVID-19. We created a Situation Background Assessment Recommendation tool identifying the indication for ECGs in patients with COVID-19 and tagged these ECGs to ensure prompt over reading and identification of those with QT prolongation (corrected QT interval > 470 ms for QRS duration ≤ 120 ms; corrected QT interval > 500 ms for QRS duration > 120 ms). This triggered a phone call from the electrophysiology service to the primary team to provide management guidance and a formal consultation if requested. During a 2-week period, we reviewed 2006 ECGs, corresponding to 524 unique patients, of whom 103 (19.7%) met the Situation Background Assessment Recommendation tool-defined criteria for QT prolongation. Compared with those without QT prolongation, these patients were more often in the intensive care unit (60 [58.3%] vs 149 [35.4%]) and more likely to be intubated (32 [31.1%] vs 76 [18.1%]). Fifty patients with QT prolongation (48.5%) had electrolyte abnormalities, 98 (95.1%) were on COVID-19-related QT-prolonging medications, and 62 (60.2%) were on 1-4 additional non-COVID-19-related QT-prolonging drugs. Electrophysiology recommendations were given to limit modifiable risk factors. No patient developed torsades de pointes. This process functioned efficiently, identified a high percentage of patients with QT prolongation, and led to relevant interventions. Arrhythmias were rare. No patient developed torsades de pointes.",success
33208926,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Consumer wearable devices that continuously measure vital signs have been used to monitor the onset of infectious disease. Here, we show that data from consumer smartwatches can be used for the pre-symptomatic detection of coronavirus disease 2019 (COVID-19). We analysed physiological and activity data from 32 individuals infected with COVID-19, identified from a cohort of nearly 5,300 participants, and found that 26 of them (81%) had alterations in their heart rate, number of daily steps or time asleep. Of the 25 cases of COVID-19 with detected physiological alterations for which we had symptom information, 22 were detected before (or at) symptom onset, with four cases detected at least nine days earlier. Using retrospective smartwatch data, we show that 63% of the COVID-19 cases could have been detected before symptom onset in real time via a two-tiered warning system based on the occurrence of extreme elevations in resting heart rate relative to the individual baseline. Our findings suggest that activity tracking and health monitoring via consumer wearable devices may be used for the large-scale, real-time detection of respiratory infections, often pre-symptomatically.",success
33524462,False,Journal Article;Observational Study,NCT04371744,databank,NCT04371744,NCT04371744,NCT04371744,NCT04371744|databank,NCT04371744|databank,True,"QTc interval monitoring, for the prevention of drug-induced arrhythmias is necessary, especially in the context of coronavirus disease 2019 (COVID-19). For the provision of widespread use, surrogates for 12‑lead ECG QTc assessment may be useful. This prospective observational study compared QTc duration assessed by artificial intelligence (AI-QTc) (Cardiologs®, Paris, France) on smartwatch single‑lead electrocardiograms (SW-ECGs) with those measured on 12‑lead ECGs, in patients with early stage COVID-19 treated with a hydroxychloroquine-azithromycin regimen. Consecutive patients with COVID-19 who needed hydroxychloroquine-azithromycin therapy, received a smartwatch (Withings Move ECG®, Withings, France). At baseline, day-6 and day-10, a 12‑lead ECG was recorded, and a SW-ECG was transmitted thereafter. Throughout the drug regimen, a SW-ECG was transmitted every morning at rest. Agreement between manual QTc measurement on a 12‑lead ECG and AI-QTc on the corresponding SW-ECG was assessed by the Bland-Altman method. 85 patients (30 men, mean age 38.3 ± 12.2 years) were included in the study. Fair agreement between manual and AI-QTc values was observed, particularly at day-10, where the delay between the 12‑lead ECG and the SW-ECG was the shortest (-2.6 ± 64.7 min): 407 ± 26 ms on the 12‑lead ECG vs 407 ± 22 ms on SW-ECG, bias -1 ms, limits of agreement -46 ms to +45 ms; the difference between the two measures was <50 ms in 98.2% of patients. In real-world epidemic conditions, AI-QTc duration measured by SW-ECG is in fair agreement with manual measurements on 12‑lead ECGs. Following further validation, AI-assisted SW-ECGs may be suitable for QTc interval monitoring. ClinicalTrial.govNCT04371744.",success
33037325,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"Ambulatory monitoring is increasingly important for cardiovascular care but is often limited by the unpredictability of cardiovascular events, the intermittent nature of ambulatory monitors and the variable clinical significance of recorded data in patients. Technological advances in computing have led to the introduction of novel physiological biosignals that can increase the frequency at which abnormalities in cardiovascular parameters can be detected, making expert-level, automated diagnosis a reality. However, use of these biosignals for diagnosis also raises numerous concerns related to accuracy and actionability within clinical guidelines, in addition to medico-legal and ethical issues. Analytical methods such as machine learning can potentially increase the accuracy and improve the actionability of device-based diagnoses. Coupled with interoperability of data to widen access to all stakeholders, seamless connectivity (an internet of things) and maintenance of anonymity, this approach could ultimately facilitate near-real-time diagnosis and therapy. These tools are increasingly recognized by regulatory agencies and professional medical societies, but several technical and ethical issues remain. In this Review, we describe the current state of cardiovascular monitoring along the continuum from biosignal acquisition to the identification of novel biosensors and the development of analytical techniques and ultimately to regulatory and ethical issues. Furthermore, we outline new paradigms for cardiovascular monitoring.",success
30815669,False,"Journal Article;Research Support, N.I.H., Extramural;Review;Webcast",,,,,,,,True,"Deep learning (DL) is a branch of machine learning (ML) showing increasing promise in medicine, to assist in data classification, novel disease phenotyping and complex decision making. Deep learning is a form of ML typically implemented via multi-layered neural networks. Deep learning has accelerated by recent advances in computer hardware and algorithms and is increasingly applied in e-commerce, finance, and voice and image recognition to learn and classify complex datasets. The current medical literature shows both strengths and limitations of DL. Strengths of DL include its ability to automate medical image interpretation, enhance clinical decision-making, identify novel phenotypes, and select better treatment pathways in complex diseases. Deep learning may be well-suited to cardiovascular medicine in which haemodynamic and electrophysiological indices are increasingly captured on a continuous basis by wearable devices as well as image segmentation in cardiac imaging. However, DL also has significant weaknesses including difficulties in interpreting its models (the 'black-box' criticism), its need for extensive adjudicated ('labelled') data in training, lack of standardization in design, lack of data-efficiency in training, limited applicability to clinical trials, and other factors. Thus, the optimal clinical application of DL requires careful formulation of solvable problems, selection of most appropriate DL algorithms and data, and balanced interpretation of results. This review synthesizes the current state of DL for cardiovascular clinicians and investigators, and provides technical context to appreciate the promise, pitfalls, near-term challenges, and opportunities for this exciting new area.",success
34345627,False,Journal Article;Review,,,,,,,,True,"The early postoperative period is a crucial stage in a patient's recovery as they are susceptible to a range of complications, with detection and management the key to avoiding long term consequences. Wearable devices are an innovative way of monitoring patient's post-intervention and may translate into improved patient outcomes, and reduced strain on healthcare resources, as they may facilitate safer and earlier discharge from the hospital setting. Several recent studies have investigated the use of wearable devices in postoperative monitoring. This review outlines the current literature including the range of wearable devices used for postoperative monitoring, the variety of surgeries investigated, and the outcomes assessed. A search of five electronic databases was performed. Data on the range of wearable devices, outcomes and surgeries investigated were extracted and synoptically analysed. Twenty-four articles were retrieved. Data on several different types of surgery were available and discussed. Most studies used wrist-mounted wearable devices and accelerometers or pedometers to assess physical activity metrics, including step counts and physical activity intensity (PAI), as markers of recovery. Wearable devices can provide objective data capture in the early postoperative phase to remotely monitor patients using various metrics including temperature, cardiac monitoring and physical activity. The majority of current research is focussed on wrist-mounted accelerometers and pedometers used to assess physical activity as a marker of postoperative function. Further research is required to demonstrate improved safety and cost-effectiveness of this technology.",success
32807931,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Validation Study",,,,,,,,True,"The global burden of diabetes is rapidly increasing, from 451 million people in 2019 to 693 million by 2045<sup>1</sup>. The insidious onset of type 2 diabetes delays diagnosis and increases morbidity<sup>2</sup>. Given the multifactorial vascular effects of diabetes, we hypothesized that smartphone-based photoplethysmography could provide a widely accessible digital biomarker for diabetes. Here we developed a deep neural network (DNN) to detect prevalent diabetes using smartphone-based photoplethysmography from an initial cohort of 53,870 individuals (the 'primary cohort'), which we then validated in a separate cohort of 7,806 individuals (the 'contemporary cohort') and a cohort of 181 prospectively enrolled individuals from three clinics (the 'clinic cohort'). The DNN achieved an area under the curve for prevalent diabetes of 0.766 in the primary cohort (95% confidence interval: 0.750-0.782; sensitivity 75%, specificity 65%) and 0.740 in the contemporary cohort (95% confidence interval: 0.723-0.758; sensitivity 81%, specificity 54%). When the output of the DNN, called the DNN score, was included in a regression analysis alongside age, gender, race/ethnicity and body mass index, the area under the curve was 0.830 and the DNN score remained independently predictive of diabetes. The performance of the DNN in the clinic cohort was similar to that in other validation datasets. There was a significant and positive association between the continuous DNN score and hemoglobin A1c (P ≤ 0.001) among those with hemoglobin A1c data. These findings demonstrate that smartphone-based photoplethysmography provides a readily attainable, non-invasive digital biomarker of prevalent diabetes.",success
34031607,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Vital signs, including heart rate and body temperature, are useful in detecting or monitoring medical conditions, but are typically measured in the clinic and require follow-up laboratory testing for more definitive diagnoses. Here we examined whether vital signs as measured by consumer wearable devices (that is, continuously monitored heart rate, body temperature, electrodermal activity and movement) can predict clinical laboratory test results using machine learning models, including random forest and Lasso models. Our results demonstrate that vital sign data collected from wearables give a more consistent and precise depiction of resting heart rate than do measurements taken in the clinic. Vital sign data collected from wearables can also predict several clinical laboratory measurements with lower prediction error than predictions made using clinically obtained vital sign measurements. The length of time over which vital signs are monitored and the proximity of the monitoring period to the date of prediction play a critical role in the performance of the machine learning models. These results demonstrate the value of commercial wearable devices for continuous and longitudinal assessment of physiological measurements that today can be measured only with clinical laboratory tests.",success
34846321,False,Journal Article,,,,,,,,True,"Chatbots are automated conversation pathways that users can access through text message or email on smartphones or other connected devices. In care management, they can be used to monitor patients' health conditions or recovery from procedures. This article describes nurse care managers' experiences using chatbots in patient care, illustrated through two patient case reviews. Considerations for planning and implementing chatbot technology in care management settings are discussed. This care management service is part of an accountable care organization that serves 582,000 patients in University Hospitals of Cleveland, Ohio. Care management focuses on patients with chronic conditions, recent hospital discharges, and other needs. Care managers comprise a centralized team as well as embedded staff in select primary care practices. The two patient cases are exemplars from the care management program serving patients recently discharged from the hospital with ongoing chronic conditions that increase risk for readmission. Use of chatbots helped overcome obstacles to conventional care management outreach and resulted in improved outcomes and strong trusting relationships with the care managers. Patients who typically do not respond to other types of care manager outreach may respond to text message-based, asynchronous chatbot communication. Interpersonal relationships between care managers and patients can be strengthened by chatbot support. Chatbot technology tracks patients' progress and offers insights to patients and clinicians to facilitate earlier interventions when problems occur. Chatbots make frequent patient contact to collect and provide routine information, allowing care managers to spend more time on high-value interactions that require clinical judgment. Potential concerns about chatbots include effect on labor force, information security, health equity, and oversight of content.",success
33664502,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"Technological innovations reach deeply into our daily lives and an emerging trend supports the use of commercial smart wearable devices to manage health. In the era of remote, decentralized and increasingly personalized patient care, catalysed by the COVID-19 pandemic, the cardiovascular community must familiarize itself with the wearable technologies on the market and their wide range of clinical applications. In this Review, we highlight the basic engineering principles of common wearable sensors and where they can be error-prone. We also examine the role of these devices in the remote screening and diagnosis of common cardiovascular diseases, such as arrhythmias, and in the management of patients with established cardiovascular conditions, for example, heart failure. To date, challenges such as device accuracy, clinical validity, a lack of standardized regulatory policies and concerns for patient privacy are still hindering the widespread adoption of smart wearable technologies in clinical practice. We present several recommendations to navigate these challenges and propose a simple and practical 'ABCD' guide for clinicians, personalized to their specific practice needs, to accelerate the integration of these devices into the clinical workflow for optimal patient care.",success
35121356,False,Journal Article;Systematic Review,,,,,,,,True,"Wearable activity trackers are gaining traction in medical research, providing both real-time and remote monitoring of physical fitness. Activity trackers offer an excellent source of personalized physical activity data from patients, as well as healthy individuals, that would provide insights into healthcare analytics and user-feedback on health status. In addition, these activity trackers would also allow researchers to monitor symptom severity and assist clinicians in providing their patients a more holistic care. Despite the promise of wearable device technology, there is still a lack of standardization in the medical literature regarding the analysis and reporting of adherence, validity and physical activity data generated by these activity trackers. We performed a systematic review to identify the activity tracker-derived measures and evaluate the relations of reported adherence, validity, and physical activity types across currently available literature. The searches were performed using Pubmed and Embase databases. Studies enrolling at least 1,000 human subjects regardless of health or disease status, using activity trackers of any brand used to track step count, distance, heart rate, energy expenditure or activity intensity, were included. Studies have been published between 2009 to March 2021, with editorials, systematic reviews, meta-analysis, grey literature, validation studies, study protocols and studies using smartphone trackers being excluded. This study was conducted in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. A total of 27 studies met the eligibility criteria and were included in the review, with a total of 514,418 and 1,186,530 subjects recruited in observational and interventional studies, respectively. Apart from ActiGraph (n = 11, 41%), Fitbit (n = 4, 15%) and Axivity (n = 3, 11%) were found to be the most commonly used activity trackers in both types of studies. The wear duration of activity trackers ranged from 1 day to 59 months, with 1 week being the most common length (n = 16, 59%). The most frequently collected physical activity measure was activity intensity (n = 21, 78%), followed by step count (n = 9, 33%) and energy expenditure (n = 2, 7%). Most studies defined a valid day as wear-time of at least 10 h within 1 day (n = 10, 37%), and a valid interval as a week with at least 3 valid days (n = 8, 30%). This systematic review reveals the diverse analysis and reporting of activity tracker data in the medical literature. Future studies will need to evaluate the feasibility on adopting minimum reporting thresholds of data generated by wearable activity trackers.",success
33501238,False,Journal Article;Review,,,,,,,,True,"Falling is among the most damaging event elderly people may experience. With the ever-growing aging population, there is an urgent need for the development of fall detection systems. Thanks to the rapid development of sensor networks and the Internet of Things (IoT), human-computer interaction using sensor fusion has been regarded as an effective method to address the problem of fall detection. In this paper, we provide a literature survey of work conducted on elderly fall detection using sensor networks and IoT. Although there are various existing studies which focus on the fall detection with individual sensors, such as wearable ones and depth cameras, the performance of these systems are still not satisfying as they suffer mostly from high false alarms. Literature shows that fusing the signals of different sensors could result in higher accuracy and lower false alarms, while improving the robustness of such systems. We approach this survey from different perspectives, including data collection, data transmission, sensor fusion, data analysis, security, and privacy. We also review the benchmark data sets available that have been used to quantify the performance of the proposed methods. The survey is meant to provide researchers in the field of elderly fall detection using sensor networks with a summary of progress achieved up to date and to identify areas where further effort would be beneficial.",success
31203472,False,Journal Article;Systematic Review,,,,,,,,True,"This review aims to present current advancements in wearable technologies and IoT-based applications to support independent living. The secondary aim was to investigate the barriers and challenges of wearable sensors and Internet-of-Things (IoT) monitoring solutions for older adults. For this work, we considered falls and activity of daily life (ADLs) for the ageing population (older adults). A total of 327 articles were screened, and 14 articles were selected for this review. This review considered recent studies published between 2015 and 2019. The research articles were selected based on the inclusion and exclusion criteria, and studies that support or present a vision to provide advancement to the current space of ADLs, independent living and supporting the ageing population. Most studies focused on the system aspects of wearable sensors and IoT monitoring solutions including advanced sensors, wireless data collection, communication platform and usability. Moderate to low usability/ user-friendly approach is reported in most of the studies. Other issues found were inaccurate sensors, battery/ power issues, restricting the users within the monitoring area/ space and lack of interoperability. The advancement of wearable technology and the possibilities of using advanced IoT technology to assist older adults with their ADLs and independent living is the subject of many recent research and investigation.",success
29149483,False,Journal Article,,,,,,,,True,"To investigate reasons for inadequate documentation of vital signs in an electronic health record. Monitoring vital signs is crucial to detecting and responding to patient deterioration. The ways in which vital signs are documented in electronic health records have received limited attention in the research literature. A previous study revealed that vital signs in an electronic health record were incomplete and inconsistent. Qualitative study. Qualitative study. Data were collected by observing (68 hr) and interviewing nurses (n = 11) and doctors (n = 3), and analysed by thematic analysis to examine processes for measuring, documenting and retrieving vital signs in four clinical settings in a 353-bed hospital. We identified two central reasons for inadequate vital sign documentation. First, there was an absence of firm guidelines for observing patients' vital signs, resulting in inconsistencies in the ways vital signs were recorded. Second, there was a lack of adequate facilities in the electronic health record for recording vital signs. This led to poor presentation of vital signs in the electronic health record and to staff creating paper ""workarounds."" This study demonstrated inadequate routines and poor facilities for vital sign documentation in an electronic health record, and makes an important contribution to knowledge by identifying problems and barriers that may occur. Further, it has demonstrated the need for improved facilities for electronic documentation of vital signs. Patient safety may have been compromised because of poor presentation of vital signs. Thus, our results emphasised the need for standardised routines for monitoring patients. In addition, designers should consult the clinical end-users to optimise facilities for electronic documentation of vital signs. This could have a positive impact on clinical practice and thus improve patient safety.",success
35385625,True,"Letter;Randomized Controlled Trial;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,False,,success
36287564,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Posthospital contact with a primary care team is an established pillar of safe transitions. The prevailing model of telephone outreach is usually limited in scope and operationally burdensome. To determine whether a 30-day automated texting program to support primary care patients after hospital discharge is associated with reductions in the use of acute care resources. This cohort study used a difference-in-differences approach at 2 academic primary care practices in Philadelphia from January 27 through August 27, 2021. Established patients of the study practices who were 18 years or older, were discharged from an acute care hospitalization, and received the usual transitional care management telephone call were eligible for the study. At the intervention practice, 604 discharges were eligible and 430 (374 patients, of whom 46 had >1 discharge) were enrolled in the intervention. At the control practice, 953 patients met eligibility criteria. The study period, including before and after the intervention, ran from August 27, 2020, through August 27, 2021. Patients received automated check-in text messages from their primary care practice on a tapering schedule during the 30 days after discharge. Any needs identified by the automated messaging platform were escalated to practice staff for follow-up via an electronic medical record inbox. The primary study outcome was any emergency department (ED) visit or readmission within 30 days of discharge. Secondary outcomes included any ED visit or any readmission within 30 days, analyzed separately, and 30- and 60-day mortality. Analyses were based on intention to treat. A total of 1885 patients (mean [SD] age, 63.2 [17.3] years; 1101 women [58.4%]) representing 2617 discharges (447 before and 604 after the intervention at the intervention practice; 613 before and 953 after the intervention at the control practice) were included in the analysis. The adjusted odds ratio (aOR) for any use of acute care resources after implementation of the intervention was 0.59 (95% CI, 0.38-0.92). The aOR for an ED visit was 0.77 (95% CI, 0.45-1.30) and for a readmission was 0.45 (95% CI, 0.23-0.86). The aORs for death within 30 and 60 days of discharge at the intervention practice were 0.92 (95% CI, 0.23-3.61) and 0.63 (95% CI, 0.21-1.85), respectively. The findings of this cohort study suggest that an automated texting program to support primary care patients after hospital discharge was associated with significant reductions in use of acute care resources. This patient-centered approach may serve as a model for improving postdischarge care.",success
34981267,False,Journal Article;Review,,,,,,,,False,,success
30855232,False,Journal Article,,,,,,,,True,"Wrist-worn smart watches and fitness monitors (ie, wearables) have become widely adopted by consumers and are gaining increased attention from researchers for their potential contribution to naturalistic digital measurement of health in a scalable, mobile, and unobtrusive way. Various studies have examined the accuracy of these devices in controlled laboratory settings (eg, treadmill and stationary bike); however, no studies have investigated the heart rate accuracy of wearables during a continuous and ecologically valid 24-hour period of actual consumer device use conditions. The aim of this study was to determine the heart rate accuracy of 2 popular wearable devices, the Apple Watch 3 and Fitbit Charge 2, as compared with the gold standard reference method, an ambulatory electrocardiogram (ECG), during consumer device use conditions in an individual. Data were collected across 5 daily conditions, including sitting, walking, running, activities of daily living (ADL; eg, chores, brushing teeth), and sleeping. One participant, (first author; 29-year-old Caucasian male) completed a 24-hour ecologically valid protocol by wearing 2 popular wrist wearable devices (Apple Watch 3 and Fitbit Charge 2). In addition, an ambulatory ECG (Vrije Universiteit Ambulatory Monitoring System) was used as the gold standard reference method, which resulted in the collection of 102,740 individual heartbeats. A single-subject design was used to keep all variables constant except for wearable devices while providing a rapid response design to provide initial assessment of wearable accuracy for allowing the research cycle to keep pace with technological advancements. Accuracy of these devices compared with the gold standard ECG was assessed using mean error, mean absolute error, and mean absolute percent error. These data were supplemented with Bland-Altman analyses and concordance class correlation to assess agreement between devices. The Apple Watch 3 and Fitbit Charge 2 were generally highly accurate across the 24-hour condition. Specifically, the Apple Watch 3 had a mean difference of -1.80 beats per minute (bpm), a mean absolute error percent of 5.86%, and a mean agreement of 95% when compared with the ECG across 24 hours. The Fitbit Charge 2 had a mean difference of -3.47 bpm, a mean absolute error of 5.96%, and a mean agreement of 91% when compared with the ECG across 24 hours. These findings varied by condition. The Apple Watch 3 and the Fitbit Charge 2 provided acceptable heart rate accuracy (<±10%) across the 24 hour and during each activity, except for the Apple Watch 3 during the daily activities condition. Overall, these findings provide preliminary support that these devices appear to be useful for implementing ambulatory measurement of cardiac activity in research studies, especially those where the specific advantages of these methods (eg, scalability, low participant burden) are particularly suited to the population or research question.",success
36241896,False,Systematic Review;Meta-Analysis;Journal Article,,,,,,,,True,"Congestion is a key driver of morbidity and mortality in heart failure. Implanted haemodynamic monitoring devices might allow early identification and management of congestion. Here, we provide a state-of-the-art review of implanted haemodynamic monitoring devices for patients with heart failure, including a meta-analysis of randomised trials. We did a systematic search for pre-print and published trials in Medline, Embase, and the Cochrane Central Register of Controlled Trials (CENTRAL) on the 22nd of September 2021. We included randomised trials that compared management with or without information from implanted haemodynamic monitoring devices for patients with heart failure. Outcomes selected were hospitalisation for heart failure and all-cause mortality. Changes in treatment associated with haemodynamic monitoring resulted in only a small reduction in mean pulmonary artery pressure (typically < 1 mmHg as a daily average), which generally remained much greater than 20 mmHg. Haemodynamic monitoring reduced hospitalisations for heart failure (HR 0.75; 95% CI 0.58-0.96; p = 0.03) but not mortality (RR 0.92; 95% CI 0.68-1.26; p = 0.48). Haemodynamic monitoring for patients with heart failure may reduce the risk of hospitalization for heart failure but this has not yet translated into a reduction in mortality, perhaps because the duration of trials was too short or the reduction in pulmonary artery pressure was not sufficiently large. The efficacy and safety of aiming for larger reductions in pulmonary artery pressure should be explored. After selecting key words, a systematic review for implanted haemodynamic telemonitoring devices was performed in different dataset and 4 randomised clinical trials were identified and included in this meta-analysis. Three different devices (Chronicle, Chronicle/ICD and CardioMEMS) were tested. All-cause mortality and total heart failure hospitalisations were selected as outcomes. No reduction in all-cause mortality rate was reported but a potential benefit on total heart failure hospitalisation was identified.",success
18342224,True,Journal Article;Multicenter Study;Randomized Controlled Trial,,,,,,,,True,"The purpose of this study was to determine whether a heart failure (HF) management strategy using continuous intracardiac pressure monitoring could decrease HF morbidity. Patients with HF may experience frequent decompensations that require hospitalization despite intensive treatment and follow-up. The COMPASS-HF (Chronicle Offers Management to Patients with Advanced Signs and Symptoms of Heart Failure) study was a prospective, multicenter, randomized, single-blind, parallel-controlled trial of 274 New York Heart Association functional class III or IV HF patients who received an implantable continuous hemodynamic monitor. Patients were randomized to a Chronicle (Medtronic Inc., Minneapolis, Minnesota) (n = 134) or control (n = 140) group. All patients received optimal medical therapy, but the hemodynamic information from the monitor was used to guide patient management only in the Chronicle group. Primary end points included freedom from system-related complications, freedom from pressure-sensor failure, and reduction in the rate of HF-related events (hospitalizations and emergency or urgent care visits requiring intravenous therapy). The 2 safety end points were met with no pressure-sensor failures and system-related complications in only 8% of the 277 patients who underwent implantation (all but 4 complications were successfully resolved). The primary efficacy end point was not met because the Chronicle group had a nonsignificant 21% lower rate of all HF-related events compared with the control group (p = 0.33). A retrospective analysis of the time to first HF hospitalization showed a 36% reduction (p = 0.03) in the relative risk of a HF-related hospitalization in the Chronicle group. The implantable continuous hemodynamic monitor-guided care did not significantly reduce total HF-related events compared with optimal medical management. Additional trials will be necessary to establish the clinical benefit of implantable continuous hemodynamic monitor-guided care in patients with advanced HF.",success
28330751,False,Comparative Study;Journal Article,,,,,,,,True,"In the CHAMPION (CardioMEMS Heart Sensor Allows Monitoring of Pressure to Improve Outcomes in New York Heart Association [NYHA] Functional Class III Heart Failure Patients) trial, heart failure hospitalization (HFH) rates were lower in patients managed with guidance from an implantable pulmonary artery pressure sensor compared with usual care. This study examined the effectiveness of ambulatory hemodynamic monitoring in reducing HFH outside of the clinical trial setting. We conducted a retrospective cohort study using U.S. Medicare claims data from patients undergoing pulmonary artery pressure sensor implantation between June 1, 2014, and December 31, 2015. Rates of HFH during pre-defined periods before and after implantation were compared using the Andersen-Gill extension to the Cox proportional hazards model while accounting for the competing risk of death, ventricular assist device implantation, or cardiac transplantation. Comprehensive heart failure (HF)-related costs were compared over the same periods. Among 1,114 patients receiving implants, there were 1,020 HFHs in the 6 months before, compared with 381 HFHs, 139 deaths, and 17 ventricular assist device implantations and/or transplants in the 6 months after implantation (hazard ratio [HR]: 0.55; 95% confidence interval [CI]: 0.49 to 0.61; p < 0.001). This lower rate of HFH was associated with a 6-month comprehensive HF cost reduction of $7,433 per patient (IQR: $7,000 to $7,884), and was robust in analyses restricted to 6-month survivors. Similar reductions in HFH and costs were noted in the subset of 480 patients with complete data available for 12 months before and after implantation (HR: 0.66; 95% CI: 0.57 to 0.76; p < 0.001). As in clinical trials, use of ambulatory hemodynamic monitoring in clinical practice is associated with lower HFH and comprehensive HF costs. These benefits are sustained to 1 year and support the ""real-world"" effectiveness of this approach to HF management.",success
21703528,False,Comparative Study;Journal Article,,,,,,,,True,"In patients with advanced heart failure (HF), elevated jugular venous pressure (JVP) is the most reliable sign of elevated left-sided filling pressures. However, discordance between right- and left-sided filling pressures (R-L mismatch) could lead to inadequate or excessive therapy guided by JVP. We determined the prevalence of R-L mismatch in the current era and investigated whether mismatch might be identified from clinical information. Right-sided heart catheterization was performed in 537 consecutive patients hospitalized with advanced HF during complete transplantation evaluation. Patients with high filling pressures were categorized as matched (right atrial pressure (RAP) ≥10 mm Hg and pulmonary wedge pressure (PCWP) ≥22 mm Hg), high-R mismatch (RAP ≥10 but PCWP <22 mm Hg) or high-L mismatch (PCWP ≥22 but RAP <10 mm Hg). Among all of the patients, 195 (36%) were matched low and 194 (36%) were matched high, and 148 (28%) had R-L mismatch. Among patients with high filling pressures, 194 (57%) were matched high and 82 (24%) had high-L and 66 (19%) high-R mismatch. Mismatches were not associated with differences in demographic or clinical data, including pulmonary and hepatic function, or severity of valvular regurgitation and right ventricular function by echo. However, among all patients with RAP ≥10 mm Hg, pulmonary artery systolic pressure (PASP) was higher in those patients with matched high left- and right-sided pressures (59 ± 12 mm Hg) versus high-R mismatch (41 ± 13 mm Hg; P < .0001). Similarly among all patients with low RAP, PASP was lower in patients with matched low right- and left-side pressures (33 ± 11 mm Hg) versus high-L mismatch (53 ± 13 mm Hg; P < .0001). R-L mismatch was present in >1 in 4 total patients, and >1 in 3 with elevated filling pressures. Regardless of clinical history, when empiric therapy to optimize volume status to JVP is not effective, additional measurement should be considered to establish the R-L relationship.",success
23392790,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Although right atrial pressure (RAP) and pulmonary capillary wedge pressure (PCWP) are correlated in heart failure, in a sizeable minority of patients, the RAP and PCWP are not tightly coupled. The basis of this variability in the RAP/PCWP ratio, and whether it conveys prognostic value, is not known. We analyzed the Evaluation Study of Congestive Heart Failure and Pulmonary Artery Catheterization Effectiveness (ESCAPE) trial database. Baseline characteristics, including echocardiographic assessment of right ventricular (RV) structure and function, and invasively measured hemodynamic parameters, were compared among tertiles of the RAP/PCWP ratio. Multivariable Cox proportional hazard models assessed the association of RAP/PCWP ratio with the primary ESCAPE outcome (6-month death or hospitalization [days]) adjusting for systolic blood pressure, blood urea nitrogen, 6-minute walk distance, and PCWP. The RAP/PCWP tertiles were 0.27 to 0.4 (tertile 1); 0.41 to 0.615 (tertile 2), and 0.62 to 1.21 (tertile 3). Increasing RAP/PCWP was associated with increasing median right atrial area (23, 26, 29 cm2, respectively; P<0.005), RV area in diastole (21, 27, 27 cm2, respectively; P<0.005), and pulmonary vascular resistance (2.4, 2.9, 3.6 woods units, respectively; P=0.003), and lower RV stroke work index (8.6, 8.4, 5.5 g·m/m2 per beat, respectively; P<0.001). RAP/PCWP ratio was associated with death or hospitalization within 6 months (hazard ratio, 1.16 [1, 1.4]; P<0.05). Increased RAP/PCWP ratio was associated with higher pulmonary vascular resistance, reduced RV function (manifest as a larger right atrium and ventricle and lower RV stroke work index), and an increased risk of adverse outcomes in patients with advanced heart failure.",success
25921522,True,"Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"Daily measurements of left atrial pressure (LAP) may be useful for guiding adjustments in medical therapy that prevent clinical decompensation in patients with severe heart failure (HF). LAPTOP-HF is a prospective, multicenter, randomized, controlled clinical trial in ambulatory patients with advanced heart failure in which the safety and clinical effectiveness of a physician-directed patient self-management therapeutic strategy based on LAP measured twice daily by means of an implantable sensor will be compared with a control group receiving optimal medical therapy. The trial will enroll up to 730 patients with New York Heart Association functional class III symptoms and either a hospitalization for HF during the previous 12 months or an elevated B-type natriuretic peptide level, regardless of ejection fraction, at up to 75 investigational centers. Randomization to the treatment group or control group will be at a 1:1 ratio in 3 strata based on the ejection fraction (EF > or ≤35%) and the presence of a de novo CRT device indication. LAPTOP-HF will provide essential information about the role of implantable LAP monitoring in conjunction with a new HF treatment paradigm across the spectrum of HF patients.",success
35275808,False,Journal Article,,,,,,,,True,"This paper aims to introduce a wearable solution and a low-complexity algorithm for real-time continuous ambulatory respiratory monitoring. A wearable chest patch is designed using a bioimpedance (BioZ) sensor to measure the changes in chest impedance caused by breathing. Besides, a medical-grade infrared temperature sensor is utilized to monitor body temperature. The computing algorithm implemented on the patch enables computation of breath-by-breath respiratory rate and chest temperature in real-time. Two wireless communication protocols are included in the system, namely Bluetooth and Long Range (LoRa), which enable both short-range and long-range data transmission. The breathing rate measured in static (i.e., standing, sitting, supine, and lateral lying) and dynamic (i.e., walking, running, and cycling) positions by our device yielded an accuracy of more than 97.8% and 98.5% relative to the ground truth, respectively. Additionally, the device's performance is evaluated in real-world scenarios both indoors and outdoors. The proposed system is capable of measuring breathing rate throughout a variety of daily activities. To the best of our knowledge, this is the first BioZ-based wearable patch capable of detecting breath-by-breath respiratory rate in real-time remotely under unrestricted ambulatory conditions. This study establishes a strategy for continuous respiratory monitoring that could aid in the early detection of cardiopulmonary disorders in everyday life.",success
27362754,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"We present a noncontact method to measure ballistocardiogram (BCG) and photoplethysmogram (PPG) simultaneously using a single camera. The method tracks the motion of facial features to determine displacement BCG, and extracts the corresponding velocity and acceleration BCGs by taking first and second temporal derivatives from the displacement BCG, respectively. The measured BCG waveforms are consistent with those reported in the literature and also with those recorded with an accelerometer-based reference method. The method also tracks PPG based on the reflected light from the same facial region, which makes it possible to track both BCG and PPG with the same optics. We verify the robustness and reproducibility of the noncontact method with a small pilot study with 23 subjects. The presented method is the first demonstration of simultaneous BCG and PPG monitoring without wearing any extra equipment or marker by the subject.",success
36502267,False,Journal Article;Review,,,,,,,,True,"Ballistocardiography (BCG) and seismocardiography (SCG) are non-invasive techniques used to record the micromovements induced by cardiovascular activity at the body's center of mass and on the chest, respectively. Since their inception, their potential for evaluating cardiovascular health has been studied. However, both BCG and SCG are impacted by respiration, leading to a periodic modulation of these signals. As a result, data processing algorithms have been developed to exclude the respiratory signals, or recording protocols have been designed to limit the respiratory bias. Reviewing the present status of the literature reveals an increasing interest in applying these techniques to extract respiratory information, as well as cardiac information. The possibility of simultaneous monitoring of respiratory and cardiovascular signals via BCG or SCG enables the monitoring of vital signs during activities that require considerable mental concentration, in extreme environments, or during sleep, where data acquisition must occur without introducing recording bias due to irritating monitoring equipment. This work aims to provide a theoretical and practical overview of cardiopulmonary interaction based on BCG and SCG signals. It covers the recent improvements in extracting respiratory signals, computing markers of the cardiorespiratory interaction with practical applications, and investigating sleep breathing disorders, as well as a comparison of different sensors used for these applications. According to the results of this review, recent studies have mainly concentrated on a few domains, especially sleep studies and heart rate variability computation. Even in those instances, the study population is not always large or diversified. Furthermore, BCG and SCG are prone to movement artifacts and are relatively subject dependent. However, the growing tendency toward artificial intelligence may help achieve a more accurate and efficient diagnosis. These encouraging results bring hope that, in the near future, such compact, lightweight BCG and SCG devices will offer a good proxy for the gold standard methods for assessing cardiorespiratory function, with the added benefit of being able to perform measurements in real-world situations, outside of the clinic, and thus decrease costs and time.",success
34013173,False,Journal Article;Review,,,,,,,,True,"Despite the increasing awareness of the importance of sleep, the number of people suffering from insufficient sleep has increased every year. The gold-standard sleep assessment uses polysomnography (PSG) with various sensors to identify sleep patterns and disorders. However, due to the high cost of PSG and limited availability, many people with sleep disorders are left undiagnosed. Recent wearable sensors and electronics enable portable, continuous monitoring of sleep at home, overcoming the limitations of PSG. This report reviews the advances in wearable sensors, miniaturized electronics, and system packaging for home sleep monitoring. New devices available in the market and systems are collectively summarized based on their overall structure, form factor, materials, and sleep assessment method. It is expected that this review provides a comprehensive view of newly developed technologies and broad insights on wearable sensors and portable electronics toward advanced sleep monitoring as well as at-home sleep assessment.",success
36215979,False,Journal Article;Review,,,,,,,,True,"Sleep apnea (SA) is characterized by intermittent episodes of apnea or hypopnea paused or reduced breathing, respectively each lasting at least ten seconds that occur during sleep. SA has an estimated global prevalence of 200 million and is associated with medical comorbidity, and sufferers are also more likely to sustain traffic- and work-related injury due to daytime somnolence. SA is amenable to treatment if detected early. Polysomnography (PSG) involving multi-channel signal acquisition is the reference standard for diagnosing SA but is onerous and costly. For home-based detection of SA, single-channel<i>SpO</i><sub>2</sub>signal acquisition using portable pulse oximeters is feasible. Machine (ML) and deep learning (DL) models have been developed for automated classification of SA versus no SA using<i>SpO</i><sub>2</sub>signals alone. In this work, we review studies published between 2012 and 2022 on the use of ML and DL for<i>SpO</i><sub>2</sub>signal-based diagnosis of SA. A literature search based on PRISMA recommendations yielded 297 publications, of which 31 were selected after considering the inclusion and exclusion criteria. There were 20 ML and 11 DL models; their methods, differences, results, merits, and limitations were discussed. Many studies reported encouraging performance, which indicates the utility of<i>SpO</i><sub>2</sub>signals in wearable devices for home-based SA detection.",success
28174819,False,Journal Article,,,,,,,,True,"The Centers for Medicare & Medicaid Services' Stage 2 final rule requires that eligible hospitals provide a visit summary electronically at transitions of care in order to qualify for ""meaningful use"" incentive payments. However, Massachusetts state law and Federal law prohibit the transmission of documents containing ""sensitive"" data unless there is a new patient consent for each transmission. To describe the implementation and evaluation of a rule-based decision support system used to screen transition of care documents for sensitive data. We implemented a rule-based document screening system to identify transition of care documents that might contain sensitive data. The transmission of detected documents is withheld until a new patient consent is obtained. The documents that were flagged as containing sensitive data were reviewed in two different time periods to verify that the decision support system was not missing documents or withholding more documents than necessary. The rule-based screening system has been in regular production use for the past 18 months. During the first evaluation period, 3% of 5,841 documents were identified as containing sensitive data (true-positive rate of 44%). After additional enhancements to the rules, the system was evaluated a second time and 4.5% of 6,935 documents were identified as containing sensitive data (true-positive rate of 98.4%). The analysis of the system demonstrates that production rules can be used to automatically screen the content of transition of care documents for sensitive data. The utilization of the rule-based decision support system enabled our hospitals to achieve meaningful use and, at the same time, remain compliant with state and federal laws.",success
32337371,False,Journal Article;Review,,,,,,,,True,"Digital medicine is an interdisciplinary field, drawing together stakeholders with expertize in engineering, manufacturing, clinical science, data science, biostatistics, regulatory science, ethics, patient advocacy, and healthcare policy, to name a few. Although this diversity is undoubtedly valuable, it can lead to confusion regarding terminology and best practices. There are many instances, as we detail in this paper, where a single term is used by different groups to mean different things, as well as cases where multiple terms are used to describe essentially the same concept. Our intent is to clarify core terminology and best practices for the evaluation of Biometric Monitoring Technologies (BioMeTs), without unnecessarily introducing new terms. We focus on the evaluation of BioMeTs as fit-for-purpose for use in clinical trials. However, our intent is for this framework to be instructional to all users of digital measurement tools, regardless of setting or intended use. We propose and describe a three-component framework intended to provide a foundational evaluation framework for BioMeTs. This framework includes (1) verification, (2) analytical validation, and (3) clinical validation. We aim for this common vocabulary to enable more effective communication and collaboration, generate a common and meaningful evidence base for BioMeTs, and improve the accessibility of the digital medicine field.",success
28877013,False,Journal Article,,,,,,,,False,,success
16357345,False,Journal Article,,,,,,,,True,"Recently there has been a remarkable upsurge in activity surrounding the adoption of personal health record (PHR) systems for patients and consumers. The biomedical literature does not yet adequately describe the potential capabilities and utility of PHR systems. In addition, the lack of a proven business case for widespread deployment hinders PHR adoption. In a 2005 working symposium, the American Medical Informatics Association's College of Medical Informatics discussed the issues surrounding personal health record systems and developed recommendations for PHR-promoting activities. Personal health record systems are more than just static repositories for patient data; they combine data, knowledge, and software tools, which help patients to become active participants in their own care. When PHRs are integrated with electronic health record systems, they provide greater benefits than would stand-alone systems for consumers. This paper summarizes the College Symposium discussions on PHR systems and provides definitions, system characteristics, technical architectures, benefits, barriers to adoption, and strategies for increasing adoption.",success
26104044,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"Patient portals (ie, electronic personal health records tethered to institutional electronic health records) are recognized as a promising mechanism to support greater patient engagement, yet questions remain about how health care leaders, policy makers, and designers can encourage adoption of patient portals and what factors might contribute to sustained utilization. The purposes of this state of the science review are to (1) present the definition, background, and how current literature addresses the encouragement and support of patient engagement through the patient portal, and (2) provide a summary of future directions for patient portal research and development to meaningfully impact patient engagement. We reviewed literature from 2006 through 2014 in PubMed, Ovid Medline, and PsycInfo using the search terms ""patient portal"" OR ""personal health record"" OR ""electronic personal health record"". Final inclusion criterion dictated that studies report on the patient experience and/or ways that patients may be supported to make competent health care decisions and act on those decisions using patient portal functionality. We found 120 studies that met the inclusion criteria. Based on the research questions, explicit and implicit aims of the studies, and related measures addressed, the studies were grouped into five major topics (patient adoption, provider endorsement, health literacy, usability, and utility). We discuss the findings and conclusions of studies that address the five topical areas. Current research has demonstrated that patients' interest and ability to use patient portals is strongly influenced by personal factors such age, ethnicity, education level, health literacy, health status, and role as a caregiver. Health care delivery factors, mainly provider endorsement and patient portal usability also contribute to patient's ability to engage through and with the patient portal. Future directions of research should focus on identifying specific populations and contextual considerations that would benefit most from a greater degree of patient engagement through a patient portal. Ultimately, adoption by patients and endorsement by providers will come when existing patient portal features align with patients' and providers' information needs and functionality.",success
30184156,False,Journal Article,,,,,,,,True,"While federal regulation provides patients the right to access their electronic health records and promotes increased use of health information technology, patient access to electronic health records remains limited. The 21st Century Cures Act, signed into law over a year ago, has important provisions that could significantly improve access and availability of health data. Specifically, the provisions call for partnerships among health information exchange networks, educational and research initiatives, and health information technology certification requirements that encourage interoperability. The article reviews the potential benefits and concerns regarding implementation of these provisions, particularly the difficulty of aligning incentives and requirements for data sharing and the question of whether currently proposed rules and guidance will support the goal of improved patient access and health information exchange. Researchers, clinicians, and patients have the power to advocate for improved patient access and interoperability as policy development and implementation of the 21st Century Cures Act continues.",success
26911829,False,"Historical Article;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"In early 2010, Harvard Medical School and Boston Children's Hospital began an interoperability project with the distinctive goal of developing a platform to enable medical applications to be written once and run unmodified across different healthcare IT systems. The project was called Substitutable Medical Applications and Reusable Technologies (SMART). We adopted contemporary web standards for application programming interface transport, authorization, and user interface, and standard medical terminologies for coded data. In our initial design, we created our own openly licensed clinical data models to enforce consistency and simplicity. During the second half of 2013, we updated SMART to take advantage of the clinical data models and the application-programming interface described in a new, openly licensed Health Level Seven draft standard called Fast Health Interoperability Resources (FHIR). Signaling our adoption of the emerging FHIR standard, we called the new platform SMART on FHIR. We introduced the SMART on FHIR platform with a demonstration that included several commercial healthcare IT vendors and app developers showcasing prototypes at the Health Information Management Systems Society conference in February 2014. This established the feasibility of SMART on FHIR, while highlighting the need for commonly accepted pragmatic constraints on the base FHIR specification. In this paper, we describe the creation of SMART on FHIR, relate the experience of the vendors and developers who built SMART on FHIR prototypes, and discuss some challenges in going from early industry prototyping to industry-wide production use.",success
33758798,False,Journal Article;Scoping Review,,,,,,,,True,"Patient-generated health data (PGHD) are clinically relevant data captured by patients outside of the traditional care setting. Clinical use of PGHD has emerged as an essential issue. This study explored the evidence to determine the extent of and describe the characteristics of PGHD integration into electronic health records (EHRs). In August 2019, we conducted a systematic scoping review. We included studies with complete, partial, or in-progress PGHD and EHR integration within a clinical setting. The retrieved articles were screened for eligibility by 2 researchers, and data from eligible articles were abstracted, coded, and analyzed. A total of 19 studies met inclusion criteria after screening 9463 abstracts. Most of the study designs were pilots and all were published between 2013 and 2019. Types of PGHD were biometric and patient activity (57.9%), questionnaires and surveys (36.8%), and health history (5.3%). Diabetes was the most common patient condition (42.1%) for PGHD collection. Active integration (57.9%) was slightly more common than passive integration (31.6%). We categorized emergent themes into the 3 steps of PGHD flow. Themes emerged concerning resource requirements, data delivery to the EHR, and preferences for review. PGHD integration into EHRs appears to be at an early stage. PGHD have the potential to close health care gaps and support personalized medicine. Efforts are needed to understand how to optimize PGHD integration into EHRs considering resources, standards for EHR delivery, and clinical workflows.",success
35106506,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Artificial intelligence (AI) algorithms are being applied across a large spectrum of everyday life activities. The implementation of AI algorithms in clinical practice has been met with some skepticism and concern, mainly because of the uneasiness that stems, in part, from a lack of understanding of how AI operates, together with the role of physicians and patients in the decision-making process; uncertainties regarding the reliability of the data and the outcomes; as well as concerns regarding the transparency, accountability, liability, handling of personal data, and monitoring and system upgrades. In this viewpoint, we take these issues into consideration and offer an integrated regulatory framework to AI developers, clinicians, researchers, and regulators, aiming to facilitate the adoption of AI that rests within the FDA's pathway, in research, development, and clinical medicine.",success
33692479,False,Journal Article;Review,,,,,,,,True,"Wearable technologies promise to redefine assessment of health behaviors, yet their clinical implementation remains a challenge. To address this gap, two of the NIH's Big Data to Knowledge Centers of Excellence organized a workshop on potential clinical applications of wearables. A workgroup comprised of 14 stakeholders from diverse backgrounds (hospital administration, clinical medicine, academia, insurance, and the commercial device industry) discussed two successful digital health interventions that involve wearables to identify common features responsible for their success. Seven features were identified including: a clearly defined problem, integration into a system of healthcare delivery, technology support, personalized experience, focus on end-user experience, alignment with reimbursement models, and inclusion of clinician champions. Health providers and systems keen to establish new models of care inclusive of wearables may consider these features during program design. A better understanding of these features is necessary to guide future clinical applications of wearable technology.",success
36376461,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Although artificial intelligence (AI) algorithms have been shown to be capable of identifying cardiac dysfunction, defined as ejection fraction (EF) ≤ 40%, from 12-lead electrocardiograms (ECGs), identification of cardiac dysfunction using the single-lead ECG of a smartwatch has yet to be tested. In the present study, a prospective study in which patients of Mayo Clinic were invited by email to download a Mayo Clinic iPhone application that sends watch ECGs to a secure data platform, we examined patient engagement with the study app and the diagnostic utility of the ECGs. We digitally enrolled 2,454 unique patients (mean age 53 ± 15 years, 56% female) from 46 US states and 11 countries, who sent 125,610 ECGs to the data platform between August 2021 and February 2022; 421 participants had at least one watch-classified sinus rhythm ECG within 30 d of an echocardiogram, of whom 16 (3.8%) had an EF ≤ 40%. The AI algorithm detected patients with low EF with an area under the curve of 0.885 (95% confidence interval 0.823-0.946) and 0.881 (0.815-0.947), using the mean prediction within a 30-d window or the closest ECG relative to the echocardiogram that determined the EF, respectively. These findings indicate that consumer watch ECGs, acquired in nonclinical environments, can be used to identify patients with cardiac dysfunction, a potentially life-threatening and often asymptomatic condition.",success
38410812,False,Journal Article,,,,,,,,False,,success
36463327,False,Journal Article;Review,,,,,,,,True,"The ""Taxonomy of Artificial Intelligence for Medical Services and Procedures"" became part of the Current Procedural Terminology (CPT®) code set effective January 1, 2022. It provides a framework for discrete and differentiable CPT codes which; are consistent with the features of the devices' output, characterize interaction between the device and the physician or other qualified health care professional, and foster appropriate payment. Descriptors include ""Assistive"", ""Augmentative"", and ""Autonomous"". As software increasingly augments the provision of medical services the taxonomy will foster consistent language in coding enabling patient, provider, and payer access to the benefits of innovation.",success
35242586,False,Journal Article,,,,,,,,True,"•Civic literacy refers to the ability to engage meaningfully with one's community.•Digital, health, and civic literacy are key predictors for digital health literacy.•The extent to which these three affect digital health literacy remains unclear.•Building digital health literacy is vital to limit inequalities from expanding.",success
33398052,False,Journal Article;Review,,,,,,,,True,"The National Academy of Medicine has long advocated for a ""learning healthcare system"" that produces constantly updated reference data during the care process. Moving toward a rapid learning system to solve intractable problems in health demands a balance between protecting patients and making data available to improve health and health care. Public concerns in the U.S. about privacy and the potential for unethical or harmful uses of this data, if not proactively addressed, could upset this balance. New federal laws prioritize sharing health data, including with patient digital tools. U.S. health privacy laws do not cover data collected by many consumer digital technologies and have not been updated to address concerns about the entry of large technology companies into health care. Further, there is increasing recognition that many classes of data not traditionally considered to be healthcare-related, for example consumer credit histories, are indeed predictive of health status and outcomes. We propose a multi-pronged approach to protecting health-relevant data while promoting and supporting beneficial uses and disclosures to improve health and health care for individuals and populations. Such protections should apply to entities collecting health-relevant data regardless of whether they are covered by federal health privacy laws. We focus largely on privacy but also address protections against harms as a critical component of a comprehensive approach to governing health-relevant data. U.S. policymakers and regulators should consider these recommendations in crafting privacy bills and rules. However, our recommendations also can inform best practices even in the absence of new federal requirements.",success
36797124,False,"Systematic Review;Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Wearable devices have made it easier to generate and share data collected on individuals. This systematic review seeks to investigate whether deidentifying data from wearable devices is sufficient to protect the privacy of individuals in datasets. We searched Web of Science, IEEE Xplore Digital Library, PubMed, Scopus, and the ACM Digital Library on Dec 6, 2021 (PROSPERO registration number CRD42022312922). We also performed manual searches in journals of interest until April 12, 2022. Although our search strategy had no language restrictions, all retrieved studies were in English. We included studies showing reidentification, identification, or authentication with data from wearable devices. Our search retrieved 17 625 studies, and 72 studies met our inclusion criteria. We designed a custom assessment tool for study quality and risk of bias assessments. 64 studies were classified as high quality and eight as moderate quality, and we did not detect any bias in any of the included studies. Correct identification rates were typically 86-100%, indicating a high risk of reidentification. Additionally, as little as 1-300 s of recording were required to enable reidentification from sensors that are generally not thought to generate identifiable information, such as electrocardiograms. These findings call for concerted efforts to rethink methods for data sharing to promote advances in research innovation while preventing the loss of individual privacy.",success
34856332,False,Journal Article;Review,,,,,,,,True,"Clinical databases, particularly those composed of big data, face growing security challenges. Blockchain, the open, decentralized, distributed public ledger technology powering cryptocurrency, records transactions securely without the need for third-party verification. In the health care setting, decentralized blockchain networks offer a secure interoperable gateway for clinical research and practice data. Here, we discuss recent advances and potential future directions for the application of blockchain and its integration with artificial intelligence (AI) in cardiovascular medicine. We first review the basic underlying concepts of this technology and contextualise it within the spectrum of current, well known applications. We then consider specific applications for cardiovascular medicine and research in areas such as high-throughput gene sequencing, wearable technologies, and clinical trials. We then evaluate current challenges to effective implementation and future directions. We also summarise the health care applications that can be realised by combining decentralized blockchain computing platforms (for data security) and AI computing (for data analytics). By leveraging high-performance computing and AI capable of securely managing large and rapidly expanding medical databases, blockchain incorporation can provide clinically meaningful predictions, help advance research methodology (eg, via robust AI-blockchain decentralized clinical trials), and provide virtual tools in clinical practice (eg, telehealth, sensory-based technologies, wearable medical devices). Integrating AI and blockchain approaches synergistically amplifies the strengths of both technologies to create novel solutions to serve the objective of providing precision cardiovascular medicine.",success
37646159,False,Journal Article;Review,,,,,,,,True,"The evolution of the electronic health record, combined with advances in data curation and analytic technologies, increasingly enables data sharing and harmonization. Advances in the analysis of health-related and health-proxy information have already accelerated research discoveries and improved patient care. This American Heart Association policy statement discusses how broad data sharing can be an enabling driver of progress by providing data to develop, test, and benchmark innovative methods, scalable insights, and potential new paradigms for data storage and workflow. Along with these advances come concerns about the sensitive nature of some health data, equity considerations about the involvement of historically excluded communities, and the complex intersection of laws attempting to govern behavior. Data-sharing principles are therefore necessary across a wide swath of entities, including parties who collect health information, funders, researchers, patients, legislatures, commercial companies, and regulatory departments and agencies. This policy statement outlines some of the key equity and legal background relevant to health data sharing and responsible management. It then articulates principles that will guide the American Heart Association's engagement in public policy related to data collection, sharing, and use to continue to inform its work across the research enterprise, as well as specific examples of how these principles might be applied in the policy landscape. The goal of these principles is to improve policy to support the use or reuse of health information in ways that are respectful of patients and research participants, equitable in impact in terms of both risks and potential benefits, and beneficial across broad and demographically diverse communities in the United States.",success
29205294,False,Journal Article;Review,,,,,,,,True,"The development of innovative wearable technologies has raised great interest in new means of data collection in healthcare and biopharmaceutical research and development. Multiple applications for wearables have been identified in a number of therapeutic areas; however, researchers face many challenges in the clinic, including scientific methodology as well as regulatory, legal, and operational hurdles. To facilitate further evaluation and adoption of these technologies, we highlight methodological and logistical considerations for implementation in clinical trials, including key elements of analytical and clinical validation in the specific context of use (COU). Additionally, we provide an assessment of the maturity of the field and successful examples of recent clinical experiments.",success
38415358,False,Journal Article;Review,,,,,,,,True,"A major focus of academia, industry, and global governmental agencies is to develop and apply artificial intelligence and other advanced analytical tools to transform health care delivery. The American Heart Association supports the creation of tools and services that would further the science and practice of precision medicine by enabling more precise approaches to cardiovascular and stroke research, prevention, and care of individuals and populations. Nevertheless, several challenges exist, and few artificial intelligence tools have been shown to improve cardiovascular and stroke care sufficiently to be widely adopted. This scientific statement outlines the current state of the art on the use of artificial intelligence algorithms and data science in the diagnosis, classification, and treatment of cardiovascular disease. It also sets out to advance this mission, focusing on how digital tools and, in particular, artificial intelligence may provide clinical and mechanistic insights, address bias in clinical studies, and facilitate education and implementation science to improve cardiovascular and stroke outcomes. Last, a key objective of this scientific statement is to further the field by identifying best practices, gaps, and challenges for interested stakeholders.",success
38375059,False,Journal Article,,,,,,,,True,"Precision prevention embraces personalized prevention but includes broader factors such as social determinants of health to improve cardiovascular health. The quality, quantity, precision, and diversity of data relatable to individuals and communities continue to expand. New analytical methods can be applied to these data to create tools to attribute risk, which may allow a better understanding of cardiovascular health disparities. Interventions using these analytic tools should be evaluated to establish feasibility and efficacy for addressing cardiovascular disease disparities in diverse individuals and communities. Training in these approaches is important to create the next generation of scientists and practitioners in precision prevention. This state-of-the-art review is based on a workshop convened to identify current gaps in knowledge and methods used in precision prevention intervention research, discuss opportunities to expand trials of implementation science to close the health equity gaps, and expand the education and training of a diverse precision prevention workforce.",success
31329886,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Although patient generated health data (PGHD) has stimulated excitement about its potential to increase patient engagement and to offer clinicians new insights into patient health status, we know little about these efforts at scale and whether they align with patient preferences. This study sought to characterize provider-led PGHD approaches, assess whether they aligned with patient preferences, and identify challenges to scale and impact. We interviewed leaders from a geographically diverse set of health systems (n = 6), leaders from large electronic health record vendors (n = 3), and leaders from vendors providing PGHD solutions to health systems (n = 3). Next, we interviewed patients with 1 or more chronic conditions (n = 10), half of whom had PGHD experience. We conducted content analysis to characterize health system PGHD approaches, assess alignment with patient preferences, and identify challenges. In this study, 3 primary approaches were identified, and each was designed to support collection of a different type of PGHD: 1) health history, 2) validated questionnaires and surveys, and 3) biometric and health activity. Whereas patient preferences aligned with health system approaches, patients raised concerns about data security and the value of reporting. Health systems cited challenges related to lack of reimbursement, data quality, and clinical usefulness of PGHD. Despite a federal policy focus on PGHD, it is not yet being pursued at scale. Whereas many barriers contribute to this narrow pursuit, uncertainty around the value of PGHD, from both patients and providers, is a primary inhibitor. Our results reveal a fairly narrow set of approaches to PGHD currently pursued by health systems at scale.",success
36812619,False,Journal Article;Review,,,,,,,,True,"Wearable devices are increasingly present in the health context, as tools for biomedical research and clinical care. In this context, wearables are considered key tools for a more digital, personalised, preventive medicine. At the same time, wearables have also been associated with issues and risks, such as those connected to privacy and data sharing. Yet, discussions in the literature have mostly focused on either technical or ethical considerations, framing these as largely separate areas of discussion, and the contribution of wearables to the collection, development, application of biomedical knowledge has only partially been discussed. To fill in these gaps, in this article we provide an epistemic (knowledge-related) overview of the main functions of wearable technology for health: monitoring, screening, detection, and prediction. On this basis, we identify 4 areas of concern in the application of wearables for these functions: data quality, balanced estimations, health equity, and fairness. To move the field forward in an effective and beneficial direction, we present recommendations for the 4 areas: local standards of quality, interoperability, access, and representativity.",success
34611493,False,Journal Article,,,,,,,,True,"This article answers two questions from the perspective of United Kingdom law and policy: (i) is health information property? and (ii) should it be? We argue that special features of health information make it unsuitable for conferral of property rights without an extensive system of data-specific rules, like those that govern intellectual property. Additionally, we argue that even if an extensive set of rules were developed, the advantages of a property framework to govern health information would be slight: propertization is unlikely to enhance patient self-determination, increase market efficiency, provide patients a foothold in the data economy, clarify legal uses of information, or encourage data-driven innovation. The better approach is to rely less, not more, on property. We recommend a regulatory model with four signature features: (i) substantial protection for personal health data similar to the GDPR with transparent limits on how, when, and by whom patient data can be accessed, used, and transmitted; (ii) input from relevant stakeholders; (iii) interoperability; and (iv) greater research into a health-data service, rather than goods, model.",success
35509277,False,Journal Article,,,,,,,,True,"We sought to study the prevalence of hypertension and the levels of awareness, treatment and control of hypertension in the young adults in Kerala, India compared to older adults. We identified 1,221 young adults (men 36.7%) in the age group 20-39 years from the 5,150 participants of the Cardiological Society of India Kerala Coronary artery disease (CAD) and its Risk factors Prevalence (CSI Kerala CRP) Study. We determined prevalence and levels of awareness, treatment and control of hypertension among them compared to older adults. We found that among the young adults, 11.2% had hypertension and 33.3% had prehypertension. Hypertension was nearly three times more prevalent among men than women (20.5 vs. 7.5% <i>p</i> < 0.001) while in older adults there was no difference between men and women in its prevalence. Male sex (OR 3.36, 95% CI 2.15-5.25 <i>p</i>-value <0.001), urban residence (OR 2.21, 95% CI 1.52-3.22 <i>p</i>-value <0.001), abdominal obesity (OR 1.74, 95% CI 1.06-2.87 <i>p</i>-value 0.028) and hypercholesterolemia (OR 1.64 95% CI 1.12-2.40 <i>p-</i>value 0.011) were significant factors favoring hypertension in the young adults. Awareness and treatment of hypertension were significantly poor among younger adults compared to older adults. In young adults, awareness, treatment and control of hypertension were significantly lower among men compared to women (23.9 vs. 51.7% <i>p</i>-value 0.001, 12.0 vs. 25.9% <i>p</i>-value 0.045, and 18.5 vs. 37.9% <i>p</i>-value 0.012, respectively). Participants who had checked blood pressure at least once during the previous year had significantly better awareness and treatment (58.7 vs. 24.0% and 41.3 vs. 19.2%, respectively). We found that one eighth of young subjects had hypertension with three times higher prevalence of hypertension among men compared to women. Awareness, treatment and control of hypertension were less among young adults and worse in young men compared to young women. Identifying hypertension and measures to control it are important and should be specifically targeted to young men.",success
34690044,False,"Journal Article;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"This review aims to summarize and discuss some of the most relevant clinical trials in epidemiology, diagnostics, and treatment of hypertension published in 2020 and 2021. The trials included in this review are related to hypertension onset age and risk for future cardiovascular disease, reliability of different blood pressure monitoring methods, role of exercise-induced hypertension, treatment of hypertension in patients with SARS-CoV-2 infection, management of hypertension high-risk patient groups, e.g., in the elderly (≥80 years) and patients with atrial fibrillation, and the interplay between nutrition and hypertension, as well as recent insights into renal denervation for treatment of hypertension. Hypertension onset age, nighttime blood pressure levels and a riser pattern are relevant for the prognosis of future cardiovascular diseases. The risk of coronary heart disease appears to increase linearly with increasing exercise systolic blood pressure. Renin-angiotensin system blockers are not associated with an increased risk for a severe course of COVID-19. In elderly patients, a risk-benefit assessment of intensified blood pressure control should be individually evaluated. A J-shaped association between cardiovascular disease and achieved blood pressure could also be demonstrated in patients with atrial fibrillation on anticoagulation. Salt restriction and lifestyle modification remain effective options in treating hypertensive patients at low cardiovascular risk. Sodium glucose co-transporter 2 inhibitors and Glucagon-like peptide-1 receptor agonists show BP-lowering effects. Renal denervation should be considered as an additional or alternative treatment option in selected patients with uncontrolled hypertension.",success
32024986,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"Hypertension is the leading cause of cardiovascular disease and premature death worldwide. Owing to the widespread use of antihypertensive medications, global mean blood pressure (BP) has remained constant or has decreased slightly over the past four decades. By contrast, the prevalence of hypertension has increased, especially in low- and middle-income countries (LMICs). Estimates suggest that 31.1% of adults (1.39 billion) worldwide had hypertension in 2010. The prevalence of hypertension among adults was higher in LMICs (31.5%, 1.04 billion people) than in high-income countries (28.5%, 349 million people). Variations in the levels of risk factors for hypertension, such as high sodium intake, low potassium intake, obesity, alcohol consumption, physical inactivity and unhealthy diet, may explain some of the regional heterogeneity in hypertension prevalence. Despite the increasing prevalence, the proportions of hypertension awareness, treatment and BP control are low, particularly in LMICs, and few comprehensive assessments of the economic impact of hypertension exist. Future studies are warranted to test implementation strategies for hypertension prevention and control, especially in low-income populations, and to accurately assess the prevalence and financial burden of hypertension worldwide.",success
26724178,False,"Journal Article;Meta-Analysis;Research Support, Non-U.S. Gov't;Systematic Review",,,,,,,,True,"The benefits of blood pressure lowering treatment for prevention of cardiovascular disease are well established. However, the extent to which these effects differ by baseline blood pressure, presence of comorbidities, or drug class is less clear. We therefore performed a systematic review and meta-analysis to clarify these differences. For this systematic review and meta-analysis, we searched MEDLINE for large-scale blood pressure lowering trials, published between Jan 1, 1966, and July 7, 2015, and we searched the medical literature to identify trials up to Nov 9, 2015. All randomised controlled trials of blood pressure lowering treatment were eligible for inclusion if they included a minimum of 1000 patient-years of follow-up in each study arm. No trials were excluded because of presence of baseline comorbidities, and trials of antihypertensive drugs for indications other than hypertension were eligible. We extracted summary-level data about study characteristics and the outcomes of major cardiovascular disease events, coronary heart disease, stroke, heart failure, renal failure, and all-cause mortality. We used inverse variance weighted fixed-effects meta-analyses to pool the estimates. We identified 123 studies with 613,815 participants for the tabular meta-analysis. Meta-regression analyses showed relative risk reductions proportional to the magnitude of the blood pressure reductions achieved. Every 10 mm Hg reduction in systolic blood pressure significantly reduced the risk of major cardiovascular disease events (relative risk [RR] 0·80, 95% CI 0·77-0·83), coronary heart disease (0·83, 0·78-0·88), stroke (0·73, 0·68-0·77), and heart failure (0·72, 0·67-0·78), which, in the populations studied, led to a significant 13% reduction in all-cause mortality (0·87, 0·84-0·91). However, the effect on renal failure was not significant (0·95, 0·84-1·07). Similar proportional risk reductions (per 10 mm Hg lower systolic blood pressure) were noted in trials with higher mean baseline systolic blood pressure and trials with lower mean baseline systolic blood pressure (all ptrend>0·05). There was no clear evidence that proportional risk reductions in major cardiovascular disease differed by baseline disease history, except for diabetes and chronic kidney disease, for which smaller, but significant, risk reductions were detected. β blockers were inferior to other drugs for the prevention of major cardiovascular disease events, stroke, and renal failure. Calcium channel blockers were superior to other drugs for the prevention of stroke. For the prevention of heart failure, calcium channel blockers were inferior and diuretics were superior to other drug classes. Risk of bias was judged to be low for 113 trials and unclear for 10 trials. Heterogeneity for outcomes was low to moderate; the I(2) statistic for heterogeneity for major cardiovascular disease events was 41%, for coronary heart disease 25%, for stroke 26%, for heart failure 37%, for renal failure 28%, and for all-cause mortality 35%. Blood pressure lowering significantly reduces vascular risk across various baseline blood pressure levels and comorbidities. Our results provide strong support for lowering blood pressure to systolic blood pressures less than 130 mm Hg and providing blood pressure lowering treatment to individuals with a history of cardiovascular disease, coronary heart disease, stroke, diabetes, heart failure, and chronic kidney disease. National Institute for Health Research and Oxford Martin School.",success
29414255,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"The Systolic Blood Pressure Intervention Trial is the first large prospective randomized controlled trial to demonstrate the benefit of an intensive systolic blood pressure (SBP) treatment target (<120 mm Hg) compared to a standard target (<140 mm Hg) in reducing cardiovascular morbidity and mortality and all-cause mortality in high-risk hypertensive patients. The impact of SPRINT on hypertension treatment has been large, but major questions remain about the feasibility of achieving the SPRINT intensive SBP target in routine practice, the generalizability of the SPRINT findings to hypertensive populations that were excluded from the trial, and the cost effectiveness of adopting the SPRINT intensive treatment goal. In this review, we discuss the generalizability of SPRINT data to the general population of adults with hypertension and with various comorbidities, the cost effectiveness of intensive SBP-lowering therapy, and the implications of SPRINT for future hypertension guideline development and clinical practice.",success
31355878,False,Journal Article,NCT01198496,databank,NCT01198496,NCT01198496,NCT01198496,NCT01198496|databank,NCT01198496|databank,True,"The Systolic Blood Pressure Intervention Trial (SPRINT) demonstrated that a systolic blood pressure (BP) target less than 120 mm Hg was superior to less than 140 mm Hg for preventing vascular events. This trial excluded patients with prior stroke; therefore, the ideal BP target for secondary stroke prevention remains unknown. To assess whether intensive BP control would achieve fewer recurrent strokes vs standard BP control. Randomized clinical trial (RCT) of standard vs intensive BP control in an intent-to-treat population of patients who had a history of stroke. Patients were enrolled between October 20, 2010, and December 7, 2016. For an updated meta-analysis, PubMed and the Cochrane Central Library database were searched through September 30, 2018, using the Medical Subject Headings and relevant search terms for cerebrovascular disease and for intensive BP lowering. This was a multicenter trial that included 140 hospitals in Japan; 1514 patients who had a history of stroke within the previous 3 years were approached, but 234 refused to give informed consent. In total, 1280 patients were randomized 1:1 to BP control to less than 140/90 mm Hg (standard treatment) (n = 640) or to less than 120/80 mm Hg (intensive treatment) (n = 640). However, 17 patients never received intervention; therefore, 1263 patients assigned to standard treatment (n = 630) or intensive treatment (n = 633) were analyzed. The primary outcome was stroke recurrence. The trial was stopped early. Among 1263 analyzed patients (mean [SD] age, 67.2 [8.8] years; 69.4% male), 1257 of 1263 (99.5%) completed a mean (SD) of 3.9 (1.5) years of follow-up. The mean BP at baseline was 145.4/83.6 mm Hg. Throughout the overall follow-up period, the mean BP was 133.2/77.7 (95% CI, 132.5-133.8/77.1-78.4) mm Hg in the standard group and 126.7/77.4 (95% CI, 125.9-127.2/73.8-75.0) mm Hg in the intensive group. Ninety-one first recurrent strokes occurred. Nonsignificant rate reductions were seen for recurrent stroke in the intensive group compared with the standard group (hazard ratio [HR], 0.73; 95% CI, 0.49-1.11; P = .15). When this finding was pooled in 3 previous relevant RCTs in a meta-analysis, the risk ratio favored intensive BP control (relative risk, 0.78; 95% CI, 0.64-0.96; P = .02; absolute risk difference, -1.5%; 95% CI, -2.6% to -0.4%; number needed to treat, 67; 95% CI, 39-250). Intensive BP lowering tended to reduce stroke recurrence. The updated meta-analysis supports a target BP less than 130/80 mm Hg in secondary stroke prevention. ClinicalTrials.gov identifier: NCT01198496.",success
35586003,False,Editorial,,,,,,,,False,,success
29379964,False,Journal Article,,,,,,,,True,"Understanding how diabetes and hypertension prevalence varies within a country as large as India is essential for targeting of prevention, screening, and treatment services. However, to our knowledge there has been no prior nationally representative study of these conditions to guide the design of effective policies. To determine the prevalence of diabetes and hypertension in India, and its variation by state, rural vs urban location, and individual-level sociodemographic characteristics. This was a cross-sectional, nationally representative, population-based study carried out between 2012 and 2014. A total of 1 320 555 adults 18 years or older with plasma glucose (PG) and blood pressure (BP) measurements were included in the analysis. State, rural vs urban location, age, sex, household wealth quintile, education, and marital status. Diabetes (PG level ≥126 mg/dL if the participant had fasted or ≥200 mg/dL if the participant had not fasted) and hypertension (systolic BP≥140 mm Hg or diastolic BP≥90 mm Hg). Of the 1 320 555 adults, 701 408 (53.1%) were women. The crude prevalence of diabetes and hypertension was 7.5% (95% CI, 7.3%-7.7%) and 25.3% (95% CI, 25.0%-25.6%), respectively. Notably, hypertension was common even among younger age groups (eg, 18-25 years: 12.1%; 95% CI, 11.8%-12.5%). Being in the richest household wealth quintile compared with being in the poorest quintile was associated with only a modestly higher probability of diabetes (rural: 2.81 percentage points; 95% CI, 2.53-3.08 and urban: 3.47 percentage points; 95% CI, 3.03-3.91) and hypertension (rural: 4.15 percentage points; 95% CI, 3.68-4.61 and urban: 3.01 percentage points; 95% CI, 2.38-3.65). The differences in the probability of both conditions by educational category were generally small (≤2 percentage points). Among states, the crude prevalence of diabetes and hypertension varied from 3.2% (95% CI, 2.7%-3.7%) to 19.9% (95% CI, 17.6%-22.3%), and 18.0% (95% CI, 16.6%-19.5%) to 41.6% (95% CI, 37.8%-45.5%), respectively. Diabetes and hypertension prevalence is high in middle and old age across all geographical areas and sociodemographic groups in India, and hypertension prevalence among young adults is higher than previously thought. Evidence on the variations in prevalence by state, age group, and rural vs urban location is critical to effectively target diabetes and hypertension prevention, screening, and treatment programs to those most in need.",success
24407539,False,Journal Article;Review,,,,,,,,True,"Several classes of antihypertensive agents have been in clinical use, including diuretics, α-blockers, β-blockers, angiotensin converting enzyme (ACE) inhibitors, angiotensin II type 1 receptor blockers (ARB), and organic calcium channel blockers (CCBs). All these drugs are being currently used in the treatment of Hypertension & various disease conditions of the heart either alone or in combination. Cilnidipine is a new antihypertensive drug distinguished from other L-type Ca(2+) channel blockers or even other antihypertensives, which will be useful for selection of antihypertensive drugs according to the pathophysiological condition of a patient.",success
25761102,False,Journal Article;Review,,,,,,,,True,"The aging population is rapidly increasing, and is mainly due to medical advances and the control of chronic diseases, with a real worldwide increase in the elderly population. Special emphasis has been placed on the management of hypertension in the geriatric patient, since its long-term benefits have been shown to prevent both cerebral and cardiac infarctions. Calcium channel blockers have been shown to be effective in this condition in the elderly. Their success depends on their mechanism of action, as well as on the physiological changes observed, and on the aging process itself, which include cardiac hypertrophy, calcification of cardiac valves, and a decrease in the excitation-conduction system. There is thickening of the tunica intima of the arteries, and the production of nitric oxide at cellular level decreases with age, along with an increase in endothelin 1, which leads to vascular endothelium dysfunction. In the kidneys, there is a decrease in prostacyclin, endothelial hyperpolarization factor, as well as the Klotho anti-aging protein, which leads to an increase in blood pressure. Calcium channel blocker drugs have been shown to be effective in any age group for the management of hypertension, and are safe in the elderly patients. These drugs block L-type calcium channels, with the long-acting or latest generation dihydropyridines being the most effective of this group. Several studies, including SYST-EUR2, NORDIL, and STOP-2, have demonstrated the effectiveness of these drugs in the geriatric patient. The prescribing of long-acting calcium channel blocker drugs in a single dose is the most recommended. The safety in the use of this drug group has been demonstrated in the treatment of hypertension in the elderly patient, with a level of effectiveness similar to other widely used drugs.",success
11136301,True,"Clinical Trial;Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"The aim of the present study was to evaluate the effects of cilnidipine, a novel dihydropyridine calcium antagonist, on autonomic function, ambulatory blood pressure and heart rate in patients with essential hypertension. Ten inpatients with mild to moderate essential hypertension (four men and six women; age: 44-64 years) underwent a drug-free period for 7 days and a treatment period with cilnidipine 10 mg orally for another 7 days, in a randomized crossover study. On the sixth day of each period, they underwent autonomic function tests including a mental arithmetic test, a cold pressor test and a Valsalva manoeuvre. After these tests, 24 h ambulatory blood pressure, heart rate, and the electrocardiogram R-R intervals were monitored every 30 min. A power spectral analysis of R-R intervals was performed to obtain the low-and high-frequency components. Cilnidipine significantly decreased the 24 h blood pressure by 6.5 +/- 1.7 mm Hg systolic (mean +/- s.e.mean; P < 0.01) and 5.0 +/- 1.1 mmHg diastolic (P < 0.01), whereas cilnidipine did not change heart rate or any indices of power spectral components. During the cold pressor test, the maximum change in systolic blood pressure and percentage changes in both systolic and diastolic blood pressures were significantly lower during the treatment period with cilnidipine than during the drug-free period. The baroreflex sensitivity measured from the overshoot phase of the Valsalva manoeuvre did not differ significantly between the two periods. Cilnidipine is effective as a once-daily antihypertensive agent and causes little influence on heart rate and the autonomic nervous system in patients with mild to moderate essential hypertension. Moreover, it is suggested that cilnidipine has an additional clinical benefit in the inhibition of the pressor response induced by acute cold stress.",success
23339732,True,"Clinical Trial;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"The Ambulatory Blood Pressure Control and Home Blood Pressure (Morning and Evening) Lowering By N-Channel Blocker Cilnidipine (ACHIEVE-ONE) trial is a large-scale clinical study on blood pressure (BP) and pulse rate (PR) in the real world with use of cilnidipine, a unique L/N-type Ca channel blocker, possessing a suppressive action on increased sympathetic activity in patients with essential hypertension. The effects of cilnidipine on morning hypertension were examined. The authors examined 2319 patients treated with cilnidipine for 12 weeks. Clinic systolic BP (SBP) decreased by 19.6 mm Hg from 155.0 mm Hg, whereas morning SBP decreased by 17.0 mm Hg from 152.9 mm Hg after 12-week cilnidipine treatment. Cilnidipine reduced both morning SBP and PR more markedly in patients with higher baseline morning SBP (-3.2 mm Hg and -1.3 beats per minute in the first quartile of morning SBP, -30.9 mm Hg and -3.2 beats per minute in the fourth quartile), and also reduced both morning PR and SBP more markedly in patients with higher baseline morning PR (0.6 beats per minute and -15.6 mm Hg in <70 beats per minute, and -9.7 beats per minute and -20.2 mm Hg in ≥85 beats per minute). Cilnidipine significantly reduced BP and PR in hypertensive patients at the clinic and at home, especially with higher BP and PR in the morning.",success
19426250,False,Historical Article;Journal Article;Review,,,,,,,,True,"Cilnidipine is a unique Ca(2+) channel blocker with an inhibitory action on the sympathetic N-type Ca(2+) channels, which is used for patients with hypertension in Japan. Cilnidipine has been clarified to exert antisympathetic actions in various examinations from cell to human levels, in contrast to classical Ca(2+) channel blockers. Furthermore, renoprotective and neuroprotective effects as well as cardioprotective action of cilnidipine have been demonstrated in clinical practice or animal examinations. After the introduction of nifedipine as an antihypertensive drug, many Ca(2+) channel blockers with long-lasting action for blood pressure have been developed to minimize sympathetic reflex during antihypertensive therapy, which have been divided into three groups; namely, first, second, and third generation based on their pharmacokinetic profiles. Since cilnidipine directly inhibits the sympathetic neurotransmitter release by N-type Ca(2+) channel-blocking property, the drug can be expected as fourth generation, providing an effective strategy for the treatment of cardiovascular diseases.",success
22763473,True,"Journal Article;Multicenter Study;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cilnidipine, an L/N-type calcium channel blocker (CCB), has been reported to have more beneficial effects on proteinuria progression in hypertensive patients than amlodipine, an L-type CCB. The N-type calcium channel blockade that inhibits renal sympathetic nerve activity might reduce glomerular hypertension by facilitating vasodilation of the efferent arterioles. However, the precise mechanism of the renoprotective effect of cilnidipine remains unknown. Because cilnidipine exerted significantly higher antioxidant activity than amlodipine in cultured human mesangial cells, we hypothesized that cilnidipine might exert a renoprotective effect by suppressing oxidative stress. A total of 35 hypertensive patients receiving a renin-angiotensin system inhibitor were randomly assigned to a cilnidipine (n=18; 10 mg per day cilnidipine titrated to 20 mg per day) or amlodipine (n=17; 5 mg per day amlodipine titrated to 10 mg per day) group; the target blood pressure (BP) was set at 130/85 mmHg. After 6 months of treatment, systolic and diastolic BPs were significantly reduced in both of the groups, without any significant difference between the groups. The urinary albumin, 8-hydroxy-2'-deoxyguanosine (OHdG) and liver-type fatty-acid-binding protein (L-FABP) to creatinine ratios significantly decreased in the cilnidipine group (P<0.05) compared with those in the amlodipine group. The reductions in urinary albumin, 8-OHdG and L-FABP were not correlated with the change in systolic BP. In conclusion, cilnidipine, but not amlodipine, ameliorated urinary albumin excretion and decreased urinary 8-OHdG and L-FABP in the hypertensive patients. Cilnidipine probably exerts a greater renoprotective effect through its antioxidative properties.",success
35140441,False,Journal Article;Observational Study,,,,,,,,True,"Hypertension is the most common cardiovascular disease. In Nepal, 27.3% populations are suffering from hypertension. Amlodipine is the most frequently prescribed anti-hypertensive drug. Up to 15% of patients develop pedal edema with amlodipine that may lead to discontinuation. Cilnidipine, a new calcium channel blocker, found equally effective and edema is uncommon in different studies from India. We aimed to study anti-hypertensive effect and to assess resolution of amlodipine induced Pedal edema with clinidipine. This was a prospective, single centre observational study. Study was conducted in the department of cardiology, Manipal teaching hospital from 7th May to 6th November 2020. Hypertensive Patients who were on amlodipine for at least 6 months and presented with pedal edema were enrolled for the study. Total of 107 patients were enrolled for the study. The mean age of patients was 56.35 ± 12.84 years, ranged from 29 to 85 years and more than half(52.3 %) were male. Of the 107 patients, 90 (84.1%) patients received 10mg of clinidipine. On follow up, all patients except three (2.8%) had resolution of pedal edema. The blood pressure reduction with clinidipine was comparable with amlodipine (p: >0.05). Three patients who had persistent edema on follow up were on higher dose of clinidipine. The newer L and N type CCB, Clinidipine has comparable efficacy with amlodipine and well tolerated in our population. Though the incidence of pedal edema is low but can occur with clinidipine especially with higher doses.",success
25878978,False,Journal Article,,,,,,,,True,"To compare amlodipine with cilnidipine on antihypertensive efficacy and incidence of pedal edema in hypertensive individuals. This was a three months prospective, observational study done at the tertiary care center of Karnataka, India. A total number of 60 (n = 60) newly diagnosed hypertensives (≥140/90) of either gender, attending outpatient department of medicine, were included in the study. Out of 60 patients, 30 patients who have been prescribed tablet amlodipine 5-10 mg/day and the other 30 who have been prescribed tablet cilnidipine 10-20 mg/day orally by the consulting physician, depending upon the severity of hypertension were followed every fortnight, screened for the presence of pedal edema and blood pressure control over a period of 3 months. Antihypertensive efficacy between two groups was compared by unpaired t-test and incidence of pedal edema was compared by Fisher's exact test. Of 30 patients in the amlodipine group, 19 patients presented with pedal edema (63.3%) and 2 patients (6.66%) in cilnidipine group presented with pedal edema during the study period. There was a significant difference in the incidence of pedal edema between amlodipine and cilnidipine group (P < 0.05), but no significant difference was found in the antihypertensive efficacy of amlodipine and cilnidipine (P > 0.05). Both amlodipine and cilnidipine have shown equal efficacy in reducing blood pressure in hypertensive individuals. But cilnidipine being N-type and L-type calcium channel blocker, associated with lower incidence of pedal edema compared to only L-type channel blocked by amlodipine.",success
23815532,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,False,,success
19782264,True,Journal Article;Randomized Controlled Trial,,,,,,,,True,"Left ventricular (LV) diastolic dysfunction is related to increased cardiac sympathetic activity. We investigated the effect of cilnidipine, an L/N-type calcium channel blocker, on LV diastolic function and cardiac sympathetic activity in patients with hypertensive heart disease (HHD) using radionuclide myocardial imaging. Thirty-two frame electrocardiography (ECG) -gated (99m)Tc-sestamibi (MIBI) myocardial single photon emission computed tomography (SPECT), and (123)I-metaiodobenzylguanidine (MIBG) imaging were performed before and 6 months after drug administration in 32 outpatients with HHD. Sixteen of the patients were treated with cilnidipine and the other 16 were treated with nifedipine retard. The parameters for assessing LV diastolic function evaluated using ECG-gated (99m)Tc-MIBI SPECT were peak filling rate (PFR), first-third filling rate (1/3FR), and time to peak filling (TPF). Cardiac sympathetic activity was assessed as early and delayed heart to mediastinum (H/M) ratios and a washout rate (WR), using (123)I-MIBG imaging. The PFR and 1/3FR significantly increased after 6 months of treatment with cilnidipine (p<0.05 for both), but did not with nifedipine retard. The H/M ratios significantly increased (p<0.05 for both) in conjunction with a decreased WR (p<0.05) in the cilnidipine group. Moreover, a significant positive correlation was seen between the rate of change in PFR and the rate of change in early and delayed H/M ratios in the cilnidipine group (p<0.05 for both). The same results were obtained for the relationship between the rate of change in 1/3FR and the rate of change in H/M ratios (p<0.05 for both). However, no such relationship was seen in the nifedipine group. These data indicate that cilnidipine seems to suppress cardiac sympathetic overactivity via blockade of N-type calcium channels and improves LV diastolic function in patients with HHD.",success
6496291,False,"Journal Article;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Despite considerable clinical experience with proteinuria, its prognostic meaning in the ambulatory general population is poorly documented. From a 16-year study of 5209 men and women in the Framingham cohort it is evident that proteinuria, even in casual urine specimens, carries substantial risk with the mortality rate increased threefold. Proteinuria was three times as common in hypertensive persons and also occurred to excess in diabetic patients and in persons with cardiac enlargement. In the absence of these factors, proteinuria was so uncommon that its risk could not be accurately assessed. Among persons with these associated risk factors, those with proteinuria have higher death rates than those without proteinuria. In men, overall mortality and cardiovascular mortality rates remained significantly increased even when other contributors to risk were taken into account. Proteinuria in the ambulatory general population is not a benign condition and carries a serious prognosis. It appears to reflect widespread vascular damage.",success
17943080,True,Journal Article;Multicenter Study;Randomized Controlled Trial,,,,,,,,True,"Cilnidipine, a dual L-/N-type calcium channel blocker, dilates both efferent and afferent arterioles and is renoprotective. Our multi-center, open-labeled, and randomized trial compared the antiproteinuric effect of cilnidipine with that of amlodipine in hypertensive patients with kidney disease. A group of 339 patients, already receiving renin-angiotensin system inhibitor treatment, were randomly assigned to cilnidipine or amlodipine. The primary endpoint was a decrease in the urinary protein to creatinine ratio. After 1-year of treatment, systolic and diastolic blood pressures were significantly reduced in both groups which did not differ between them. The urinary protein to creatinine ratio significantly decreased in the cilnidipine compared to the amlodipine group. Cilnidipine exerted a greater antiproteinuric effect than amlodipine even in the subgroup whose blood pressure fell below the target level. This study suggests that cilnidipine is superior to amlodipine in preventing the progression of proteinuria in hypertensive patients when coupled with a renin-angiotensin system inhibitor.",success
29584458,True,Journal Article;Multicenter Study;Randomized Controlled Trial,,,,,,,,True,"Intradialytic hypertension (HTN), which is one of the poor prognostic markers in patients undergoing hemodialysis, may be associated with sympathetic overactivity. The L/N-type calcium channel blocker, cilnidipine, has been reported to suppress sympathetic nerves activity in vivo. Therefore, we hypothesized that cilnidipine could attenuate intradialytic systolic blood pressure (SBP) elevation. Fifty-one patients on chronic hemodialysis who had intradialytic-HTN (SBP elevation ≥10 mmHg during hemodialysis) and no fluid overload were prospectively randomized into two groups: control and cilnidipine groups. Cilnidipine group patients took cilnidipine (10 mg/day) for 12 weeks. The primary endpoint was the change in the intradialytic SBP elevation before and after the 12-week intervention. Before the intervention, no differences were observed in age, sex or pre-dialytic SBP (148.5 ± 12.9 vs. 148.3 ± 19.3 mmHg) between the two groups. Intradialytic SBP elevation was unchanged in the control group. Cilnidipine significantly lowered the post-dialytic SBP with an attenuation of the intradialytic SBP elevation from 12.0 ± 15.4 mmHg to 4.8 ± 10.1 mmHg. However, the observed difference in the intradialytic SBP elevation by cilnidipine did not reach statistical significance (group×time interaction effect p = 0.25). Cathecolamine levels were unaffected by the intervention in both groups. Cilnidipine lowers both the pre- and post-dialytic SBP and might attenuate intradialytic SBP elevation. Therefore, cilnidipine may be effective in lowering SBP during HD in patients with intradialytic-HTN.",success
28448181,False,Journal Article,,,,,,,,True,"Blood pressure control is important in post-stroke hypertensive patients and antihypertensive treatment is recommended for such patients. Ca-channel blockers are recommended as the medications of choice for the treatment of post-stroke patients. Here, we report the results of a large-scale prospective post-marketing surveillance study of post-stroke hypertensive patients (n = 2667, male 60.4%, 69.0 ± 10.9 years) treated with cilnidipine, with regard to blood pressure control and adverse reactions. Cilnidipine treatment caused a decrease in both clinic and home blood pressures 2 months after the beginning of treatment, and the decreased blood pressure was maintained until the end of 12 months' observation. The proportion of patients in whom clinic blood pressure was well controlled (<140/90 mmHg) increased from 21.5% to 65.3% in cilnidipine treatment, with no differences in effectiveness among the various clinical subtypes of stroke. In total, 346 adverse events occurred, with an overall incidence of 8.9% (238 of 2667 patients). In the elderly group, specifically, a fall and a hip fracture each occurred in 1 (0.1%) patient. These results indicate that cilnidipine was effective in treating uncontrolled blood pressure and was well tolerated in Japanese post-stroke hypertensive patients in a real-world clinical setting.",success
23815534,True,"Clinical Trial;Journal Article;Observational Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"The authors examined the effect of cilnidipine, a unique L/N-type calcium channel blocker, on abnormal nocturnal blood pressure (BP) dipping in Japanese hypertensive patients in the real world. The Ambulatory Blood Pressure Control and Home Blood Pressure (Morning and Evening) Lowering by N-Channel Blocker Cilnidipine (ACHIEVE-ONE), a large-scale clinical study, was designed to evaluate the effects of cilnidipine in daily medical practice. Among the study, 24-hour ambulatory BP data were obtained from 615 patients and classified according to their nocturnal dipping status as extreme dippers, dippers, nondippers, or risers. A 12-week treatment with cilnidipine significantly reduced 24-hour BP in all groups (P<.001). Changes in nocturnal systolic BP (SBP) from baseline were -17.9 mm Hg from 154.6 mm Hg in risers and -11.9 mm Hg from 142.1 mm Hg, -6.6 mm Hg from 128.5 mm Hg, and 0.1 mm Hg from 115.8 mm Hg in nondippers, dippers, and extreme dippers, respectively. Changes from baseline in nocturnal SBP reduction rate were 8.2% in risers (P<.001) but -7.0% in extreme dippers (P<.001), while no change was observed in the nighttime SBP reduction rate for the total patients (-0.2%±9.6%, P=.617). Cilnidipine partially, but significantly, restored abnormal nocturnal dipping status toward a normal dipping pattern in hypertensive patients.",success
25980338,False,Journal Article,,,,,,,,True,"It has been suggested that antihypertensive drug therapy is attributable to the lower blood pressure variability, we investigated the effects of 4 classes of antihypertensives on the blood pressure variability; in addition, we also compared the effects among 4 calcium channel blockers. We measured the 24-hour blood pressure variability in 309 patients with a history of cerebrovascular disease treated with angiotensin-converting enzyme inhibitor, angiotensin receptor blocker, β blocker, or calcium channel blocker. The daytime blood pressure variability treated with β blockers (14.3 ± 3.1) was higher than that treated with an angiotensin receptor blockers (11.5 ± 3.1) or calcium channel blockers (12.6 ± 3.4) in patients with cerebrovascular disease (P < .05). In the analysis of the patient distribution of blood pressure variability, patients receiving β blockers occurred more frequently in the higher blood pressure variability (P = .0023). Treatment with angiotensin receptor blockers and cilnidipine, which blocks N-type calcium channels, was shown to be more frequently associated with the lower blood pressure variability (P = .0202 and .0467). The mean blood pressure of patients grouped by distribution of blood pressure variability was found to be independent to blood pressure variability, for any of the antihypertensive drugs or calcium channel blockers examined. From the results, it is suggested that angiotensin receptor blocker and calcium channel blockers rather than β blockers may be more favorable for blood pressure management in patients with cerebrovascular disease. Among the calcium channel blockers, cilnidipine may be more favorable than other calcium channel blockers.",success
29225324,False,Journal Article,,,,,,,,True,"Although several antihypertensive agents reduced the carotid intima-media thickness (IMT), it remains unclear whether those agents affect the interadventitial diameter (IAD). We aimed to examine whether cilnidipine, an L/N-type calcium channel blocker, reduced the common carotid IMT or IAD in post-stroke hypertensive patients. The common carotid IMT and IAD were measured at the start of cilnidipine treatment and 12 months from that. The changes in the mean max-IMT or IAD between baseline and the 12-month follow-up were evaluated and compared between the thick group (max-IMT ≥1.1 mm) and the normal group (max-IMT ＜1.1 mm). A total of 603 post-stroke hypertensive subjects (mean age=69.3 yr, 378 males) were included in the analysis. At baseline, IAD was increased stepwise according to the value of max-IMT (p for trend ＜0.001). Among them, 326 subjects were followed up for 12 months. The mean max-IMT from baseline to 12 months did not change in the normal group (－0.01 mm, 95% confidence interval [CI] －0.03 to 0.01, n=170), whereas a significant reduction was observed in the thick group (－0.09 mm, 95% CI －0.13 to －0.05, n=156). The mean IAD was significantly reduced during the study period in the normal group (－0.14 mm, 95% CI －0.22 to －0.05) as well as in the thick group (－0.12 mm, 95% CI －0.21 to －0.03). Cilnidipine promoted the regression of common carotid IMT in post-stroke hypertensive patients, especially in the thick group. Cilnidipine also reduced the IAD in both normal and thick groups.",success
37499598,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cardiac sarcoidosis (CS) is a potentially serious form of infiltrative cardiomyopathy. Despite scarce evidence, immunosuppressive treatment is generally recommended, but local routines may vary significantly. We sought to survey the clinical practices in the treatment of CS, with the aim that the results may suggest future research priorities. We conducted a web-based survey focused on treatment-naive patients with CS. We subclassified CS according to the presence/absence of overt cardiac presentation (clinically manifest/silent) and to the presence/absence of active inflammation (metabolically active/inactive by fluorodeoxyglucose positron emission tomography). The survey was developed jointly by the authors and administered to expert clinicians (n = 79) involved in CS treatment. An agreement threshold was set at 70%. A total of 62 of 79 respondents (78.5%) from 12 countries completed the survey. The agreement threshold was reached for: (1) always treating clinically manifest, metabolically active CS, 57 of 62 (91.9%), (2) never treating clinically silent, metabolically inactive CS, 44 of 62 (71.0%), (3) not requiring histopathologic confirmation of sarcoidosis before treatment initiation, (49 of 62, 79.0%), (4) using fluorodeoxyglucose positron emission tomography for assessing treatment indication (44 of 62, 71.0%) and treatment response (44 of 62, 71.0%), and (5) using prednisone as a first-line agent (100%), although respondents were divided on monotherapy (69.4%) or combination with methotrexate 25.8%. The approach to particular scenarios, tapering, and duration of treatment showed the greatest variation in response. In conclusion, in this survey of clinical practice, important aspects of CS treatment reached the agreement threshold, whereas others showed a great degree of clinical equipoise.",success
34496176,False,Journal Article;Review,,,,,,,,False,,success
31273209,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Sarcoidosis is an inflammatory disorder of unknown cause that is characterized by granuloma formation in affected organs, most often in the lungs. Patients frequently suffer from cough, shortness of breath, chest pain and pronounced fatigue and are at risk of developing lung fibrosis or irreversible damage to other organs. The disease develops in genetically predisposed individuals with exposure to an as-yet unknown antigen. Genetic factors affect not only the risk of developing sarcoidosis but also the disease course, which is highly variable and difficult to predict. The typical T cell accumulation, local T cell immune response and granuloma formation in the lungs indicate that the inflammatory response in sarcoidosis is induced by specific antigens, possibly including self-antigens, which is consistent with an autoimmune involvement. Diagnosis can be challenging for clinicians because of the potential for almost any organ to be affected. As the aetiology of sarcoidosis is unknown, no specific treatment and no pathognomic markers exist. Thus, improved biomarkers to determine disease activity and to identify patients at risk of developing fibrosis are needed. Corticosteroids still constitute the first-line treatment, but new treatment strategies, including those targeting quality-of-life issues, are being evaluated and should yield appropriate, personalized and more effective treatments.",success
18539233,False,Journal Article;Review,,,,,,,,True,"Sarcoidosis continues to be a disease of research interest because of its complicated immune mechanisms and elusive etiology. So far, it has been established that granulomatous inflammation in sarcoidosis is predominantly a T-helper 1 immune response mediated by a complex network of lymphocytes, macrophages, and cytokines. The cause of progression to a chronic and potentially fibrotic form is unclear but may involve loss of apoptotic mechanisms, loss of regulatory response, or a persistent antigen that cannot be cleared. Recent genomic and proteomic technology has emphasized the importance of host susceptibility and gene-environment interaction in the expression of the disease.",success
25707386,False,Journal Article;Review,,,,,,,,True,"Cardiac sarcoidosis is a potentially life-threatening condition characterized by formation of granulomas in the heart, resulting in conduction disturbances, atrial and ventricular arrhythmias, and ventricular dysfunction. The presentation of cardiac sarcoidosis ranges from asymptomatic with an abnormal imaging scan, to palpitations, syncope, symptoms of congestive heart failure, and sudden cardiac death. Screening for cardiac sarcoidosis has not been standardized, but the presence of cardiac symptoms on medical history and physical examination, and an abnormal electrocardiogram (ECG), Holter monitoring, or echocardiogram has been shown to be highly sensitive for detecting cardiac sarcoidosis. A signal-averaged ECG might also have a role in screening for cardiac sarcoidosis in asymptomatic patients. Although endomyocardial biopsies are highly specific for the diagnosis of cardiac sarcoidosis, procedural yield is very low and appropriate findings on cardiac MRI or PET are, therefore, often used as diagnostic surrogates. Treatment for cardiac sarcoidosis usually involves immunosuppressive therapy, particularly corticosteroids. Additional therapy might be required, depending on the clinical presentation, including implantation of an internal defibrillator, antiarrhythmic agents, and catheter ablation.",success
26585430,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"In pulmonary sarcoidosis, CD4(+) T-cells expressing T-cell receptor Vα2.3 accumulate in the lungs of HLA-DRB1*03(+) patients. To investigate T-cell receptor-HLA-DRB1*03 interactions underlying recognition of hitherto unknown antigens, we performed detailed analyses of T-cell receptor expression on bronchoalveolar lavage fluid CD4(+) T-cells from sarcoidosis patients.Pulmonary sarcoidosis patients (n=43) underwent bronchoscopy with bronchoalveolar lavage. T-cell receptor α and β chains of CD4(+) T-cells were analysed by flow cytometry, DNA-sequenced, and three-dimensional molecular models of T-cell receptor-HLA-DRB1*03 complexes generated.Simultaneous expression of Vα2.3 with the Vβ22 chain was identified in the lungs of all HLA-DRB1*03(+) patients. Accumulated Vα2.3/Vβ22-expressing T-cells were highly clonal, with identical or near-identical Vα2.3 chain sequences and inter-patient similarities in Vβ22 chain amino acid distribution. Molecular modelling revealed specific T-cell receptor-HLA-DRB1*03-peptide interactions, with a previously identified, sarcoidosis-associated vimentin peptide, (Vim)429-443 DSLPLVDTHSKRTLL, matching both the HLA peptide-binding cleft and distinct T-cell receptor features perfectly.We demonstrate, for the first time, the accumulation of large clonal populations of specific Vα2.3/Vβ22 T-cell receptor-expressing CD4(+) T-cells in the lungs of HLA-DRB1*03(+) sarcoidosis patients. Several distinct contact points between Vα2.3/Vβ22 receptors and HLA-DRB1*03 molecules suggest presentation of prototypic vimentin-derived peptides.",success
29073364,False,Journal Article;Review,,,,,,,,True,"Sarcoidosis is a multisystem disease with tremendous heterogeneity in disease manifestations, severity, and clinical course that varies among different ethnic and racial groups. To better understand this disease and to improve the outcomes of patients, a National Heart, Lung, and Blood Institute workshop was convened to assess the current state of knowledge, gaps, and research needs across the clinical, genetic, environmental, and immunologic arenas. We also explored to what extent the interplay of the genetic, environmental, and immunologic factors could explain the different phenotypes and outcomes of patients with sarcoidosis, including the chronic phenotypes that have the greatest healthcare burden. The potential use of current genetic, epigenetic, and immunologic tools along with study approaches that integrate environmental exposures and precise clinical phenotyping were also explored. Finally, we made expert panel-based consensus recommendations for research approaches and priorities to improve our understanding of the effect of these factors on the health outcomes in sarcoidosis.",success
12615619,False,"Journal Article;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Sarcoidosis, in the United States, more commonly and severely affects African Americans. HLA associations with sarcoidosis have been reported, but most studies used case-control designs, which may produce biased results because of population stratification. We examined transmission of HLA-DQB1 alleles in 225 African American families with at least one offspring with sarcoidosis. Of five low-resolution HLA-DQB1 alleles, *02 and *06 showed significant deviation in transmission patterns to affected offspring. High-resolution typing of these allelic subsets revealed that HLA-DQB1*0201 was transmitted to affected offspring half as often as expected (p = 0.001), whereas DQB1*0602 was transmitted to affected offspring about 20% more often than expected (p = 0.029). Examining interactions between *0201 and *0602 alleles and environmental exposures showed that *0602 varied little with respect to exposure, but sarcoidosis risk associated with *0201 often depended on exposure status. Alternatively, the *0602 allele in affected probands was associated with radiographic disease progression, but the *0201 allele showed no significant correlation with phenotype. Major differences in the amino acid sequences encoded by *0201 and *0602 alleles exist, which may explain the differential effects these alleles have on sarcoidosis susceptibility and progression in African Americans.",success
36089937,False,Journal Article,,,,,,,,False,,success
27509154,False,Journal Article,,,,,,,,True,"There have been no recent comprehensive studies of the epidemiology of sarcoidosis in the United States. Changes in health care use have made available access to data on large numbers of patients with sarcoidosis. To use a U.S. national health care database to gather data on patients with sarcoidosis identified over a 3-year period who were 18 years of age and older, and to determine health care costs for these patients. The Optum health care database was queried for a 3-year period (2010-2013). This database includes approximately 15% of U.S. residents. The incidence rate of sarcoidosis was calculated for new cases identified in each year. Calculation of prevalence was based on any patient with sarcoidosis seen during the year. Incidence and prevalence rates are reported per 100,000 patients. A total of 29,372 adult patients with sarcoidosis were identified. Of these, 14,700 (55%) were over 55 years of age at the time of diagnosis. The incidence and prevalence rates were higher for African Americans (17.8 and 141.4 per 100,000, respectively) than for white individuals (8.1 and 49.8), Hispanics (4.3 and 21.7), or Asians (3.2 and 18.9). Women were two times more likely to have sarcoidosis, with the highest prevalence for sarcoidosis noted in African American women (178.5). Overall, the yearly health care cost reported for patients with sarcoidosis was low, with a median of $18,663 per year. However, the yearly cost for the top 5% was $93,201. For patients 18 years of age and older enrolled in a U.S. national administrative database, sarcoidosis was more common among African Americans, but it was reported for all four of the major ethnic groups studied. While health care costs were relatively small for most patients, the cost of care for some patients was considerable.",success
20595459,False,"Comparative Study;Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Sarcoidosis is a systemic granulomatous disorder of unknown cause that occurs among men and women of all races. In the United States, black women are most frequently and most severely affected. There have been few epidemiologic studies of sarcoidosis focusing on black women. In this article, we present data on incidence, prevalence, and clinical characteristics of sarcoidosis among participants in the Black Women's Health Study, a cohort study of 59,000 black women from across the United States. Data on incident disease and potential risk factors are obtained through biennial questionnaires. Follow-up has been > 80% through six completed cycles. There were 685 prevalent cases of sarcoidosis at baseline in 1995 and 435 incident cases reported during 611,585 person-years of follow-up through 2007, for an average annual incidence rate of 71/100,000 and a current prevalence of 2.0%. The sarcoid diagnosis was confirmed in 96% of self-reported cases for whom medical records or physician checklists were obtained. The most frequently affected site was the lung. Most patients also had extrapulmonary involvement, with the most common sites being lymph nodes, skin, and eyes. Prednisone had the highest prevalence of use, followed by inhaled corticosteroids. This study confirms previous reports of high incidence and prevalence of sarcoidosis among black women, as well as the extent of extrapulmonary disease, frequent need for steroid therapy, and comorbid conditions in this population. The prospective identification of sarcoidosis cases from a defined population will enable a valid assessment of risk factors for incident disease as follow-up continues.",success
25527698,True,"Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"This study was designed to assess the epidemiology, characteristics, and outcome of cardiac sarcoidosis (CS) in Finland. We identified in retrospect all adult (>18 years of age) patients diagnosed with histologically confirmed CS in Finland between 1988 and 2012. A total of 110 patients (71 women) 51±9 years of age (mean±SD) were found and followed up for outcome events to the end of 2013. The annual detection rate of CS increased >20-fold during the 25-year period, reaching 0.31 in 1×10(5) adults between 2008 and 2012. The 2012 prevalence of CS was 2.2 in 1×10(5). Nearly two thirds of patients had clinically isolated CS. Altogether, 102 of the 110 patients received immunosuppressive therapy, and 56 received an intracardiac defibrillator. Left ventricular function was impaired (ejection fraction <50%) in 65 patients (59%) at diagnosis and showed no overall change over 12 months of steroid therapy. During follow-up (median, 6.6 years), 10 patients died of a cardiac cause, 11 patients underwent transplantation, and another 11 patients suffered an aborted sudden cardiac death. The Kaplan-Meier estimates for 1-, 5-, and 10-year transplantation-free cardiac survival were 97%, 90%, and 83%, respectively. Heart failure at presentation predicted poor outcome (log-rank P=0.0001) with a 10-year transplantation-free cardiac survival of only 53%. The detection rate of CS has increased markedly in Finland over the last 25 years. With current therapy, the prognosis of CS appears better than generally considered, but patients presenting with heart failure still have poor long-term outcome.",success
36924191,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cardiac sarcoidosis (CS) results from epithelioid cell granulomas infiltrating the myocardium and predisposing to conduction disturbances, ventricular tachyarrhythmias, and heart failure. Manifest CS, however, constitutes only the top of an iceberg as advanced imaging uncovers cardiac involvement 4 to 5 times more commonly than what is clinically detectable. Definite diagnosis of CS requires myocardial biopsy and histopathology, but a sufficient diagnostic likelihood can be achieved by combining extracardiac histology of sarcoidosis with clinical manifestations and findings on cardiac imaging. CS can appear as the first or only organ manifestation of sarcoidosis or on top of pre-existing extracardiac disease. Due to the lack of controlled trials, the care of CS is based on observational evidence of low quality. Currently, the treatment involves corticosteroid-based, tiered immunosuppression to control myocardial inflammation with medical and device-based therapy for symptomatic atrioventricular block, ventricular tachyarrhythmias, and heart failure. Recent outcome data indicate 90% to 96% 5-year survival in manifest CS with the 10-year figures ranging from 80% to 90%. Major progress in the care of CS awaits the key to its molecular-genetic pathogenesis and large-scale controlled clinical trials.",success
37062472,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Although sex- and race-based patterns have been described in the extracardiac organ involvement of sarcoidosis, cardiac sarcoidosis (CS)-specific studies are lacking. We studied CS presentation, treatment and outcomes based on sex and race in a tertiary-center cohort. Multivariable adjusted Cox proportional hazards and survival analyses were performed for primary composite outcomes (left ventricular assist device, heart transplantation, all-cause death) and for secondary outcomes (ventricular arrhythmia and all-cause death. We identified 252 patients with CS (108 female, 109 Black). At presentation with CS, females vs males (P = 0.001) and Black vs White individuals (P = 0.001) more commonly had symptomatic heart failure (HF), with HF most common in Black females (ANOVA P < 0.001). Treatment differences included more corticosteroid use (90% vs 79%; P = 0.020), higher 1-year prednisone dosage (13 vs 10 mg; P = 0.003) and less frequent early steroid-sparing agent use in males (29% vs 40%; P = 0.05). Black participants more frequently received a steroid-sparing agent (75% vs 60%; P = 0.023). Composite outcome-free survival did not differ by sex or race. Male sex had an adjusted hazard ratio of 2.34 (95% CI 1.13, 4.80; P = 0.021) for ventricular arrhythmia. CS course may differ by sex and race and may contribute to distinct clinical CS phenotypes.",success
27469375,False,Journal Article,,,,,,,,True,"Approximately 5% of patients with sarcoidosis will have clinically manifest cardiac involvement presenting with one or more of ventricular arrhythmias, conduction abnormalities, and heart failure. Cardiac presentations can be the first (and/or an unrecognized) manifestation of sarcoidosis in a variety of circumstances. Cardiac symptoms are usually dominant over extra-cardiac as most patients with clinically manifest disease have minimal extra-cardiac disease and up to two-thirds have isolated cardiac sarcoidosis (CS). It is estimated that another 20-25% of pulmonary/systemic sarcoidosis patients have asymptomatic cardiac involvement (clinically silent disease). The extent of left ventricular dysfunction seems to be the most important predictor of prognosis among patients with clinically manifest CS. In addition, the extent of myocardial late gadolinium enhancement is emerging as an important prognostic factor. The literature shows some controversy regarding outcomes for patients with clinically silent CS and larger studies are needed. Immunosuppression therapy (usually with corticosteroids) has been suggested for the treatment of clinically manifest CS despite minimal data supporting it. Fluorodeoxyglucose Positron Emission Tomography imaging is often used to detect active disease and guide immunosuppression. Patients with clinically manifest disease often need device therapy, typically with implantable cardioverter defibrillators.",success
36752432,False,Systematic Review;Meta-Analysis;Journal Article,,,,,,,,True,"Sarcoidosis is a complex multisystem inflammatory disorder, with approximately 5% of patients having overt cardiac involvement. Patients with cardiac sarcoidosis are at an increased risk of both ventricular arrhythmias and sudden cardiac death. Previous studies have shown that the presence of late gadolinium enhancement (LGE) on cardiac magnetic resonance (CMR) is associated with an increased risk of mortality and ventricular arrhythmias and may be useful in predicting prognosis. This systematic review and meta-analysis assessed the value of LGE on CMR imaging in predicting prognosis for patients with known or suspected cardiac sarcoidosis. The authors searched the Embase and MEDLINE databases from inception to March 2022 for studies reporting individuals with known or suspected cardiac sarcoidosis referred for CMR with LGE. Outcomes were defined as all-cause mortality, ventricular arrhythmia, or a composite outcome of either death or ventricular arrhythmias. The primary analysis evaluated these outcomes according to the presence of LGE. A secondary analysis evaluated outcomes specifically according to the presence of biventricular LGE. Thirteen studies were included (1,318 participants) in the analysis, with an average participant age of 52.0 years and LGE prevalence of 13% to 70% over a follow-up of 3.1 years. Patients with LGE on CMR vs those without had higher odds of ventricular arrhythmias (odds ratio [OR]: 20.3; 95% CI: 8.1-51.0), all-cause mortality (OR: 3.45; 95% CI: 1.6-7.3), and the composite of both (OR: 9.2; 95% CI: 5.1-16.7). Right ventricular LGE is invariably accompanied by left ventricular LGE. Biventricular LGE is also associated with markedly increased odds of ventricular arrhythmias (OR: 43.6; 95% CI: 16.2-117.2). Patients with known or suspected cardiac sarcoidosis with LGE on CMR have significantly increased odds of both ventricular arrhythmias and all-cause mortality. The presence of biventricular LGE may confer additional prognostic information regarding arrhythmogenic risk.",success
35550245,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,True,"Sarcoidosis is a systemic disease characterized by granulomatous inflammation. Cardiac involvement is associated with increased morbidity. However, differences in clinical characteristics and outcomes based on initial sarcoidosis organ manifestation in patients with cardiac sarcoidosis (CS) have not been described. A retrospective cohort of 252 patients with CS at an urban, quaternary medical center was studied. Presentation, treatment and outcomes of de novo CS and prior ECS groups were compared. Survival free of primary composite outcome (left ventricular assist device implantation, orthotopic heart transplantation (OHT), or death) was assessed. There were 124 de novo CS patients and 128 with prior ECS at time of CS diagnosis. De novo CS patients were younger at CS diagnosis (p = 0.020). De novo CS patients had a more advanced cardiac presentation: lower left ventricular ejection fraction (LVEF) (p < 0.001), more frequent sustained ventricular arrhythmias (VA) (p = 0.001), and complete heart block (p = 0.001). During follow-up, new VA (p < 0.001), ventricular tachycardia ablation (p < 0.001), and OHT (p = 0.003) were more common in the de novo CS group. Outcome free survival was significantly shorter for de novo CS patients (p = 0.005), with increased hazard of primary composite outcome (p = 0.034) and development of new VA (p = 0.027) when compared to ECS patients. Overall mortality was similar between groups. Patients presenting with CS as their first recognized organ manifestation of sarcoidosis have an increased risk of adverse cardiac outcomes as compared to those with a prior history of ECS. Improved awareness and diagnosis of CS is warranted for earlier recognition.",success
34246375,False,"Editorial;Research Support, Non-U.S. Gov't;Comment",,,,,,,,False,,success
25512195,False,Journal Article,,,,,,,,True,"Isolated cardiac sarcoidosis (iCS) is difficult to diagnose in patients without histologic evidence of sarcoidosis. We aimed to clarify the clinical characteristics of iCS, including imaging features on cardiac magnetic resonance imaging (MRI) and (18)F-fluoro-2-deoxyglucose positron-emission tomography/computerized tomography (FDG-PET/CT) scans. We also reviewed the therapeutic effect of corticosteroids and determined the long-term prognosis. We retrospectively reviewed 83 consecutive patients with suspicious CS from 1997 to 2013. Systemic sarcoidosis with CS (sCS, n = 30) and iCS (n = 11) were diagnosed according to clinical criteria. In iCS cases, sarcoidosis was not detected in any other organs. The clinical features did not significantly differ between sCS and iCS cases, except for ejection fraction, which was lower in iCS (P = .025). Nine sCS and 4 iCS cases showed late gadolinium enhancement, and the lesions tended to be on the epicardial side (76.9% P = .011) and septal wall (52.9% P < .001). The coefficient of variance for the myocardial standardized uptake value of FDG-PET/CT was higher in sCS (0.32 ± 0.13; n = 19) and iCS (0.32 ± 0.09; n = 7) than in control cases (n = 31; P < .001). B-Type natriuretic peptide level was improved after prednisolone treatment in both groups. Kaplan-Meier curve indicated that prognosis was not different between sCS and iCS cases. The clinical cardiac characteristics of iCS cases were similar to those of sCS. Cardiac MRI and FDG-PET, noninvasive imaging modalities, could be useful modalities to detect myocardial involvement in the cases with definite or suspected iCS.",success
26775110,False,Letter,,,,,,,,False,,success
21535250,False,Journal Article,,,,,,,,True,"Cardiac sarcoidosis (CS) without clinically apparent extracardiac disease may escape detection because of the poor sensitivity of endomyocardial biopsy (EMB). We set out to analyse our experience of repeated and imaging-guided biopsies in clinically isolated CS. We retrospectively reviewed the medical records, laboratory test results, imaging studies and pathological analyses of 74 patients with either histologically proven or clinically probable CS at our institution between January 2000 and December 2010. Fifty-two patients had histologically proven CS, of whom 33 (26 women) had disease that was clinically isolated to the heart. Sarcoidosis was detected in the first EMB in 10 of the 31 patients who underwent biopsy. CS was found by repeated EMBs, targeted by cardiac imaging, in seven additional patients, and 11 patients were diagnosed by sampling 18-F-fluorodeoxyglucose position emission tomography-positive mediastinal lymph nodes at mediastinoscopy. Together, the first biopsy (cardiac or mediastinal lymph node) provided the diagnosis in 34%, the second biopsy in 31% and the third in 22% of biopsied patients with isolated CS. Four (13%) of the remaining diagnosis were made after cardiac transplantation and one in a patient who did not undergo biopsy) at autopsy after sudden cardiac death. Cardiac sarcoidosis may present without clinically apparent disease in other organs. At least two-thirds of patients remain undiagnosed after a single EMB session. The detection rate can be improved by repeated and imaging-guided cardiac or mediastinal lymph-node biopsies. Nevertheless, false-negative biopsy results remain a problem in CS patients with no apparent extracardiac disease.",success
36599603,False,"Letter;Research Support, N.I.H., Extramural",,,,,,,,False,,success
37194596,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,False,,success
31959111,False,Journal Article,,,,,,,,True,"Sarcoidosis is a systemic granulomatous disease that may affect the myocardium. This study evaluated the diagnostic and prognostic value of 2-dimensional speckle tracking echocardiography in cardiac sarcoidosis (CS). Eighty-three patients with extracardiac, biopsy-proven sarcoidosis and definite/probable diagnosis of cardiac involvement diagnosed from January 2005 through December 2016 were included. Strain parameters in early stages of CS, in a subgroup of 23 CS patients with left ventricular ejection fraction (LVEF) within normal limits (LVEF> 52% for men: > 54% for women, mean value: 57.3% ± 3.8%) and no wall motion abnormalities was compared with 97 controls (1:4) without cardiac disease. LV and right ventricular (RV) global longitudinal (GLS), circumferential (GCS), and radial (GRS) strain and strain rate (SR) analyses were performed with TomTec software and correlated with cardiac outcomes (including heart failure and arrhythmias). This study was approved by the Mayo Clinic Institutional Review Board, and all patients gave informed written consent to participate. Mean age of CS patients was 53.6 ± 10.8 years, and 34.9% were women. Mean LVEF was 43.2% ± 12.4%; LV GLS, - 12.4% ± 3.7%; LV GCS, - 17.1% ± 6.5%; LV GRS, 29.3% ± 12.8%; and RV wall GLS, 14.6% ± 6.3%. In the 23 patients with early stage CS with normal LVEF and RV systolic function, strain parameters were significantly reduced when compared with controls (respectively: LV GLS, - 15.9% ± 2.5% vs - 18.2% ± 2.7% [P = .001]; RV GLS, - 16.9% ± 4.5% vs - 24.1% ± 4.0% [P < .001]). A LV GLS value of - 16.3% provided 82.2% sensitivity and 81.2% specificity for the diagnosis of CS (AUC 0.91), while a RV value of - 19.9% provided 88.1% sensitivity and 86.7% specificity (AUC 0.93). Hospital admission and heart failure significantly correlated to impaired LV GLS (> - 14%). Reduced strain values in the LV GLS and RV GLS can be used in the diagnostic algorithm in patients with suspicion of cardiac sarcoidosis. These values also correlate with adverse cardiovascular events.",success
25431267,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Limited data exist on the risk of developing cardiac sarcoidosis (CS) and/or adverse events in sarcoidosis patients. Using LV global longitudinal strain (GLS), an emerging sensitive parameter of LV function, we evaluated the prevalence of subclinical cardiac dysfunction in sarcoidosis and investigated whether LVGLS predicts adverse outcomes in this population. A total of 130 patients with proven sarcoidosis undergoing echocardiography at our referral centre were identified. Following exclusion of those with evidence of CS (n = 14) or other pre-existing structural heart disease (n = 16), 100 patients (55 ± 13 years, 48% male, 90% pulmonary involvement) and 100 age- and gender-matched controls were included. LVGLS was measured by speckle-tracking analysis. The primary endpoint was a composite of all-cause mortality, heart failure hospitalization, device implantation, new arrhythmias, or future development of CS on advanced cardiac imaging modalities. LVGLS was significantly impaired in sarcoidosis patients compared with controls (-17.3 ± 2.5 vs. -20.0 ± 1.6%, P < 0.001). Overall, 27 patients (27%) reached the endpoint during a median follow-up of 35 months. On Cox proportional hazards model analysis, abnormal 24-h Holter, larger LV end-diastolic diameters, and more impaired LVGLS were significantly associated with the endpoint; however, only LVGLS remained independently associated on multivariate analysis [hazard ratio (HR) 1.4, 95% confidence interval (CI) 1.1-1.7, P = 0.006]. Patients with LVGLS less than -17.3% were significantly more likely to be free of the primary endpoint (log-rank P = 0.01). LVGLS is impaired in sarcoidosis patients, suggesting subclinical cardiac dysfunction despite the absence of conventional evidence of cardiac disease, and is independently associated with occurrence of cardiac events and/or development of CS.",success
8989146,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Contrast medium-enhanced magnetic resonance images of acute, reperfused infarcts have shown hypoenhanced and hyperenhanced regions in areas of injured myocardium. The precise mechanisms that lead to these altered enhancement patterns are unknown. This study was designed to evaluate possible mechanisms and to relate altered enhancement patterns to myocardial perfusion and viability. Thirteen rabbits underwent in situ coronary artery occlusion and reperfusion followed by isolated perfusion with cardioplegic solution. T1-weighted spin-echo images were acquired continuously during step changes in perfusate Gd-DTPA concentration. Regional blood flow was also measured by use of radioactive microspheres in all rabbits. There were marked differences in Gd-DTPA wash-in and washout time constants (wash-in, 0.8 +/- 0.1, 2.1 +/- 02, and 16.3 +/- 2.4 minutes, P < .001; washout, 1.6 +/- 0.1, 4.8 +/- 0.5, and 31.1 +/- 3.3 minutes, P < .001) in normal, infarct rim, and infarct core regions, respectively, resulting in differential enhancement of these regions. Microsphere flows in the infarct rim and core were 42.9 +/- 4.0% and 12.0 +/- 1.6% of normal myocardium and correlated well with washout time constants (r = .86, y = 0.77x - 0.002, P < .001), suggesting that these time constants index the severity of microvascular damage. In addition, spatial maps of washout time constants were produced. The extent of regions with abnormal time constants correlated well with triphenyltetrazolium chloride-determined infarct size (r = .94, y = 0.95x + 4.17, P < .001). In contrast-enhanced magnetic resonance images of acute, reperfused rabbit infarcts, differential image intensity is primarily due to regional differences in contrast agent wash-in and washout time constants. These regional differences in time constants also indicate the extent and severity of myocardial injury.",success
35579526,False,"Journal Article;Meta-Analysis;Systematic Review;Research Support, Non-U.S. Gov't",,,,,,,,True,"Background There is limited consensus regarding the relative diagnostic performance of cardiac MRI and fluorodeoxyglucose (FDG) PET for cardiac sarcoidosis. Purpose To perform a systematic review and meta-analysis to compare the diagnostic accuracy of cardiac MRI and FDG PET for cardiac sarcoidosis. Materials and Methods Medline, Ovid Epub, Cochrane Central Register of Controlled Trials, Embase, Emcare, and Scopus were searched from inception until January 2022. Inclusion criteria included studies that evaluated the diagnostic accuracy of cardiac MRI or FDG PET for cardiac sarcoidosis in adults. Data were independently extracted by two investigators. Summary accuracy metrics were obtained by using bivariate random-effects meta-analysis. Meta-regression was used to assess the effect of different covariates. Risk of bias was assessed using the Quality Assessment Tool for Diagnostic Accuracy Studies-2 tool. The study protocol was registered a priori in the International Prospective Register of Systematic Reviews (Prospero protocol CRD42021214776). Results Thirty-three studies were included (1997 patients, 687 with cardiac sarcoidosis); 17 studies evaluated cardiac MRI (1031 patients) and 26 evaluated FDG PET (1363 patients). Six studies directly compared cardiac MRI and PET in the same patients (303 patients). Cardiac MRI had higher sensitivity than FDG PET (95% vs 84%; <i>P</i> = .002), with no difference in specificity (85% vs 82%; <i>P</i> = .85). In a sensitivity analysis restricted to studies with direct comparison, point estimates were similar to those from the overall analysis: cardiac MRI and FDG PET had sensitivities of 92% and 81% and specificities of 72% and 82%, respectively. Covariate analysis demonstrated that sensitivity for FDG PET was highest with quantitative versus qualitative evaluation (93% vs 76%; <i>P</i> = .01), whereas sensitivity for MRI was highest with inclusion of T2 imaging (99% vs 88%; <i>P</i> = .001). Thirty studies were at risk of bias. Conclusion Cardiac MRI had higher sensitivity than fluorodeoxyglucose PET for diagnosis of cardiac sarcoidosis but similar specificity. Limitations, including risk of bias and few studies with direct comparison, necessitate additional study. © RSNA, 2022 <i>Online supplemental material is available for this article.</i>",success
31070111,False,"Journal Article;Meta-Analysis;Research Support, N.I.H., Extramural;Systematic Review",,,,,,,,True,"Background In patients with suspected cardiac sarcoidosis, late gadolinium enhancement on cardiovascular magnetic resonance imaging and/or <sup>18</sup>F-fluorodeoxyglucose uptake on positron emission tomography are often used to reach a clinical diagnosis of cardiac sarcoidosis. On the basis of data from the imaging literature of clinical cardiac sarcoidosis, no specific features of myocardial involvement are regarded as pathognomonic for cardiac sarcoidosis. Thus, a diagnosis of cardiac sarcoidosis is challenging to make. There has been no systematic analysis of histologically diagnosed cardiac sarcoidosis for patterns of myocardial involvement. We hypothesized that certain patterns of myocardial involvement are more frequent in histologically diagnosed cardiac sarcoidosis. Methods and Results We performed a systematic review and meta-analysis of gross pathological images from the published literature of patients with histologically diagnosed cardiac sarcoidosis who underwent autopsy or cardiac transplantation. Thirty-three eligible articles provided images of 49 unique hearts. Analysis of these hearts revealed certain features of myocardial involvement in >90% of cases: left ventricular (LV) subepicardial, LV multifocal, septal, and right ventricular free wall involvement. In contrast, other patterns were seen in 0% to 6% of cases: absence of gross LV myocardial involvement, isolated LV midmyocardial involvement, isolated LV subendocardial involvement, isolated LV transmural involvement, absence of septal involvement, or isolated involvement of only one LV level. Conclusions In this systematic review and meta-analysis of histologically diagnosed cardiac sarcoidosis, we identified certain features of myocardial involvement that occurred frequently and others that occurred rarely or never. These patterns could aid the interpretation of cardiovascular magnetic resonance imaging and positron emission tomography imaging and improve the diagnosis and the prognostication of patients with suspected cardiac sarcoidosis.",success
36103165,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,True,"In patients with sarcoidosis with suspected cardiac involvement, late gadolinium enhancement (LGE) on cardiovascular magnetic resonance imaging (CMR) identifies those with an increased risk of adverse outcomes. However, these outcomes are experienced by only a minority of patients with LGE, and identifying this subgroup may improve treatment and outcomes in these patients. To assess whether CMR phenotypes based on left ventricular ejection fraction (LVEF) and LGE in patients with suspected cardiac sarcoidosis (CS) are associated with adverse outcomes during follow-up. This cohort study included consecutive patients with histologically proven sarcoidosis who underwent CMR for the evaluation of suspected CS from 2004 to 2020 with a median follow-up of 4.3 years at an academic medical center in Minnesota. Demographic data, medical history, comorbidities, medications, and outcome data were collected blinded to CMR data. CMR phenotypes were identified based on LVEF and LGE presence and features. LGE was classified as pathology-frequent or pathology-rare based on the frequency of cardiac damage features on gross pathology assessment of the hearts of patients with CS who had sudden cardiac death or cardiac transplant. Composite of ventricular arrhythmic events and composite of heart failure events. Among 504 patients (mean [SD] age, 54.1 [12.5] years; 242 [48.0%] female and 262 [52.0%] male; 2 [0.4%] American Indian or Alaska Native, 6 [1.2%] Asian, 90 [17.9%] Black or African American, 399 [79.2%] White, 5 [1.0%] of 2 or more races (including the above-mentioned categories and Native Hawaiian or Other Pacific Islander), and 2 [0.4%] of unknown race; 4 [0.8%] Hispanic or Latino, 498 [98.8%] not Hispanic or Latino, and 2 [0.4%] of unknown ethnicity), 4 distinct CMR phenotypes were identified: normal LVEF and no LGE (n = 290; 57.5%), abnormal LVEF and no LGE (n = 53; 10.5%), pathology-frequent LGE (n = 103; 20.4%), and pathology-rare LGE (n = 58; 11.5%). The phenotype with pathology-frequent LGE was associated with a high risk of arrhythmic events (hazard ratio [HR], 12.12; 95% CI, 3.62-40.57; P < .001) independent of LVEF and extent of left ventricular late gadolinium enhancement (LVLGE). It was also associated with a high risk of heart failure events (HR, 2.49; 95% CI, 1.19-5.22; P = .02) independent of age, pulmonary hypertension, LVEF, right ventricular ejection fraction, and LVLGE extent. Risk of arrhythmic events was greater with an increasing number of pathology-frequent LGE features. The absence of the pathology-frequent LGE phenotype was associated with a low risk of arrhythmic events, even in the presence of LGE or abnormal LVEF. This cohort study found that a CMR phenotype involving pathology-frequent LGE features was associated with a high risk of arrhythmic and heart failure events in patients with sarcoidosis. The findings indicate that CMR phenotypes could be used to optimize clinical decision-making for treatment options, such as implantable cardioverter-defibrillators.",success
36624560,False,Journal Article,,,,,,,,True,"Giant cell myocarditis (GCM) is an inflammatory cardiomyopathy akin to cardiac sarcoidosis (CS). We decided to study the findings of GCM on cardiac magnetic resonance (CMR) imaging and to compare GCM with CS. CMR studies of 18 GCM patients were analyzed and compared with 18 CS controls matched for age, sex, left ventricular (LV) ejection fraction and presenting cardiac manifestations. The analysts were blinded to clinical data. On admission, the duration of symptoms (median) was 0.2 months in GCM vs. 2.4 months in CS (P = 0.002), cardiac troponin T was elevated (>50 ng/L) in 16/17 patients with GCM and in 2/16 with CS (P < 0.001), their respective median plasma B-type natriuretic propeptides measuring 4488 ng/L and 1223 ng/L (P = 0.011). On CMR imaging, LV diastolic volume was smaller in GCM (177 ± 32 mL vs. 211 ± 58 mL, P = 0.014) without other volumetric or wall thickness measurements differing between the groups. Every GCM patient had multifocal late gadolinium enhancement (LGE) in a distribution indistinguishable from CS both longitudinally, circumferentially, and radially across the LV segments. LGE mass averaged 17.4 ± 6.3% of LV mass in GCM vs 25.0 ± 13.4% in CS (P = 0.037). Involvement of insertion points extending across the septum into the right ventricular wall, the ""hook sign"" of CS, was present in 53% of GCM and 50% of CS. In GCM, CMR findings are qualitatively indistinguishable from CS despite myocardial inflammation being clinically more acute and injurious. When matched for LV dysfunction and presenting features, LV size and LGE mass are smaller in GCM.",success
32613384,False,Journal Article,,,,,,,,True,"Cardiac sarcoidosis (CS) is an increasingly recognized condition, but cardiac magnetic resonance (CMR) image interpretation in these patients may be challenging as findings are often non-specific. The main objective of this study was to investigate the inter-reader agreement for the overall interpretation of CMR for the diagnosis of CS in an experienced reference center and investigate factors that may lead to discrepancies between readers. Consecutive patients undergoing CMR imaging to investigate for CS were included. CMR images were independently reviewed by two readers, blinded to all clinical, imaging and demographic information. The readers classified each scan as ""consistent with cardiac sarcoidosis"", ""not consistent with cardiac sarcoidosis"" or ""indeterminate"". Inter-reader agreement was assessed using κ-statistics. When there was disagreement on the overall interpretation, a third reader reviewed the images. Also, two readers independently commented on the presence of edema, presence of LGE (both ventricles) and quantified the extent of left ventricular LGE. 87 patients (43 women, mean age 54.3 ± 12.2 years) were included in the study. There was agreement regarding the overall interpretation in 72 of 87 (83%) CMR scans. The κ value was 0.64, indicating moderate agreement. There was similar moderate agreement in the interpretation of LGE parameters. In an experienced referral center, we found moderate agreement between readers in the interpretation of CMR in patients with suspected CS. Physicians should be aware of this inter-observer variability in interpretation of CMR studies in patients with suspected CS.",success
26926267,False,Journal Article;Review,,,,,,,,True,"Sarcoidosis is a multisystem disorder of unknown cause, and cardiac sarcoidosis affects at least 25% of patients and accounts for substantial mortality and morbidity from this disease. Cardiac sarcoidosis may present with heart failure, left ventricular systolic dysfunction, AV block, atrial or ventricular arrhythmias, and sudden cardiac death. Cardiac involvement can be challenging to detect and diagnose because of the focal nature of the disease, as well as the fact that clinical criteria have limited diagnostic accuracy. Nevertheless, the diagnosis of cardiac sarcoidosis can be enhanced by integrating both clinical and imaging findings. This article reviews the various roles that different imaging modalities provide in the evaluation and management of patients with known or suspected cardiac sarcoidosis.",success
28765228,False,Journal Article;Consensus Development Conference,,,,,,,,False,,success
29086496,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, N.I.H., Intramural;Review",,,,,,,,True,"Intermittent fasting (IF) is a term used to describe a variety of eating patterns in which no or few calories are consumed for time periods that can range from 12 hours to several days, on a recurring basis. This review is focused on the physiological responses of major organ systems, including the musculoskeletal system, to the onset of the metabolic switch: the point of negative energy balance at which liver glycogen stores are depleted and fatty acids are mobilized (typically beyond 12 hours after cessation of food intake). Emerging findings suggest that the metabolic switch from glucose to fatty acid-derived ketones represents an evolutionarily conserved trigger point that shifts metabolism from lipid/cholesterol synthesis and fat storage to mobilization of fat through fatty acid oxidation and fatty acid-derived ketones, which serve to preserve muscle mass and function. Thus, IF regimens that induce the metabolic switch have the potential to improve body composition in overweight individuals. Moreover, IF regimens also induce the coordinated activation of signaling pathways that optimize physiological function, enhance performance, and slow aging and disease processes. Future randomized controlled IF trials should use biomarkers of the metabolic switch (e.g., plasma ketone levels) as a measure of compliance and of the magnitude of negative energy balance during the fasting period.",success
30088196,False,Journal Article;Systematic Review,,,,,,,,True,"<sup>18</sup>F-fluorodeoxyglucose (<sup>18</sup>F-FDG) positron emission tomography (PET) is used in the diagnosis and management of patients with cardiac sarcoidosis (CS). Various preparation protocols have been proposed to minimise myocardial <sup>18</sup>F-FDG uptake and improve scan readability. The aim of this systematic review was to identify the optimal dietary prescription for suppression of physiological <sup>18</sup>F-FDG myocardial uptake to enhance clinical diagnosis of CS. MEDLINE and PubMed databases identified 13 studies meeting inclusion criteria for review. Articles were assessed using the Australian National Health and Medical Research Council levels of evidence and categorised as sarcoidosis (human) or non-sarcoidosis (human, animal). Visual uptake scales (qualitative) and/or standardised uptake values (SUV) (quantitative) were used in all the studies reviewed. Nine of 11 human studies showed statistically significant improvements in PET scan interpretation with carbohydrate-restricted diets compared with fasting only, and when carbohydrates were restricted for a longer period of time. Two animal studies showed statistically significant improvements following very low carbohydrate diet preparation (0.01% and 0.4% carbohydrate diets) compared with higher carbohydrate diets. Variation in measures used, dietary prescriptions, fasting times, species and study quality makes result comparison and applicability difficult. Definitive dietary recommendations are not possible based on current evidence.",success
20013165,True,"Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"Low-carbohydrate (LC) and high-fat, low-carbohydrate (HFLC) dietary preparations may enhance (18)F-FDG-PET-based imaging of small, inflamed structures near the heart by suppressing myocardial FDG signal. We compared myocardial (18)F-FDG uptake in patients randomized to LC, HFLC, and unrestricted (UR) preparations prior to (18)F-FDG-PET. We randomized 63 outpatients referred for oncologic (18)F-FDG-PET to LC, HFLC, or UR dietary preparations (1:1:1 allocation) starting the evening before PET. After eating dinner according to instructions, UR and LC patients fasted until FDG injection (mean time 745 minutes for UR, 899 minutes for LC), and HFLC patients drank a fatty drink 60-70 minutes prior to FDG injection. Attenuation-corrected PET imaging was performed 60 minutes after FDG administration. Maximal myocardial standard uptake values (MyoSUV(max)) were systematically measured in axial view and compared between the three groups. Using UR patients as reference, mean MyoSUV(max) was lower in LC patients (3.3 +/- 2.7 vs 6.2 +/- 5.2, P = .03) but not in HFLC patients (5.5 +/- 4.2, P = .63). Ratios of MyoSUV(max) to liver SUV(max), calculated to control for background uptake, were not significantly different amongst the groups (1.9 +/- 2.1 LC, 2.6 +/- 2.3 HFLC, 3.6 +/- 3.5 UR). In this small randomized controlled trial using UR diet as reference, LC dietary preparation followed by extended fasting resulted in significant myocardial uptake suppression.",success
21732228,False,Journal Article,,,,,,,,True,"Fluoro-deoxy-glucose (FDG) can be used to visualize inflammation in atherosclerotic plaques in coronary arteries, if myocardial FDG uptake is adequately suppressed. Prolonged fasting for suppressing myocardial FDG uptake is inconsistent. We evaluated the feasibility to consistently suppress myocardial FDG uptake with a low carbohydrate high fat protein permitted (LCHFPP) diet. This was a prospective study. 50 patients were included in fasting group (>12 hours fasting) and 60 patients were included into LCHFPP diet. Fasting group had no special dietary preparation. Patients in LCHFPP diet group were asked to consume LCHFPP diet the night before and 4 hours prior to the study. Visual analysis of myocardial FDG uptake was done on maximum intensity projection image. Using CT images for localization, the ability to delineate possible FDG uptake in the left coronary artery was assessed in the corresponding PET image and the studies were classified as ""interpretable"" or ""Not interpretable"". 60 patients (mean age 47 years) from LCHFPP diet group and 50 patients (mean age 49.9 years) from fasting group were included. None of the patients were known diabetics. The mean blood glucose level was 96 mg/dL. Forty-eight patients had consumed LCHFPP diet as per protocol. Twelve had consumed LCHFPP diet only on the night before the study (non-compliant). The average duration of fasting in compliant patients was 5.9 ± 0.9 hours in the diet group and 14.6 hours in fasting group. In LCHFPP diet group, the myocardial FDG uptake was classified as complete suppression in 31; minimal uptake in 15; moderate inhomogenous uptake in 8 and homogenous intense uptake in 6 patients. Fifty-four of the 60 patients had interpretable study. When non-compliant patients were excluded, 84% of the patients had significant FDG uptake suppression and 94% of the studies were classified as interpretable. In the fasting group, complete myocardial suppression of FDG uptake was noticed in 16; minimal in 8; moderate inhomogenous in 15; and homogenous intense in 11 patients. 27 patients (54%) had interpretable study. Consistent and significant myocardial FDG uptake suppression is possible in most patients using LCHFPP diet. The LCHFPP diet, if taken as per protocol, leads to consistent myocardial FDG uptake suppression to allow for adequate evaluation of the left coronary artery inflammation in nearly all the patients. LCHFPP diet is also significantly more efficacious than prolonged (>12 hours) fasting protocol in suppressing myocardial FDG uptake.",success
27922863,False,Journal Article,,,,,,,,True,"A major obstacle in using FDG-PET/CT to diagnose cardiac sarcoidosis (CS) is the unpredictable physiological myocardial FDG uptake. We hypothesized that a prolonged 72-hour pretest high-fat, high-protein, and very-low-carbohydrate (HFHPVLC) diet preparation could suppress physiologic myocardial uptake of FDG and thus help to identify active CS. This retrospective study included 215 FDG-PET/CT tests from 207 patients with biopsy-proven sarcoidosis and clinical suspicion for CS between July 2014 and December 2015. The patients were classified into 2 groups. Group 1 included 12 FDG-PET/CT scans from 12 patients who had 24-hour or less pretest HFHPVLC diet preparation. Group 2 included 203 FDG-PET/CT scans with 72-hour HFHPVLC diet before FDG-PET/CT. Nonadherent patients and patients with cancer were excluded. Cardiac FDG uptake was classified as: ""none"" and ""ringlike diffuse at base"" (negative for CS), ""focal"" (positive for CS), and ""diffuse"" (indeterminate for CS). FDG uptake in myocardial lesions was measured as SUVmax and compared with SUVmean of mediastinal blood pool. Final diagnoses were made with consensus among physicians in view of all available clinical information including cardiac MRI and echocardiogram results. In group 1, there were 1 (1/12, 8.3%) positive, 5 (5/12, 41.7%) indeterminate, and 6 (6/12, 50.0%) negative for CS. In group 2, 10 patients were excluded (6 patients because of noncompliance with diet, 2 patients with concurrent diagnosis of cancers, 2 patients because of insulin and steroid use within 4 hours before PET/CT); the remaining 185 patients had 193 FDG PET/CT tests (8 repeats), of which there were 19 (19/193, 9.8%) positive, 7 indeterminate (7/193, 3.6%), and 167 (167/193, 86.7.%) negative for CS. The SUVmax of PET-positive myocardial lesions ranges from 3.4 to 12.5, whereas mediastinal blood pool SUVmean ranges from 1.1 to 3.6. The indeterminate rate was significantly lower in group 2 compared with group 1 (P < 0.001). The prolonged 72-hour HFHPVLC diet preparation protocol successfully suppressed physiological myocardial FDG uptake and may permit a more sensitive and accurate method of diagnosing active CS using FDG PET/CT.",success
33771904,False,Journal Article,,,,,,,,True,"<b>Rationale:</b> A definitive dietary preparation recommendation is not possible based on literature in achievement of myocardial suppression for diagnosis of cardiac sarcoidosis (CS) with <sup>18</sup>F-FDG PET/CT. Our goal is to compare three different dietary preparations in achievement of the best myocardial suppression and CS diagnosis. <b>Methods:</b> We retrospectively reviewed and compared three dietary preparations used at our institution. Three different diets were applied from 03/2014 to 12/2019. 24-h ketogenic diet with overnight fasting (<i>n</i> = 94); 18h-fasting (<i>n</i> = 44); 72-h daytime ketogenic diet with 3-day overnight fasting (<i>n</i> = 98). The interpretation of initial reports was recorded, and an independent radiologist (observer) retrospectively re-evaluated each case regarding CS diagnosis (Negative, Positive, Indeterminant) and myocardial suppression (Complete, Failed, Partial). Interobserver agreement was analyzed. We measured MaxSUV from bloodpool, liver, and the most suppressed normal myocardium. <b>Results:</b> We identified superior myocardial suppression with the 72-h preparation indicated by a higher bloodpool/myocardium and liver/myocardium ratios (P<0.001). Myocardial suppression rates for 72-h ketogenic diet, 24-h ketogenic diet and 18-h fasting preparations are as follows; Complete myocardial suppression: 96.9%/68.1%/52.3%, Failed myocardial suppression: 0%/23.4%/25%, Partial myocardial suppression: 3.1%/8.5%/22.7%) (P<0.001). The 72-hour preparation had significantly fewer ""indeterminant"" and ""positive"" exams. CS diagnosis rates for 72-h ketogenic diet, 24-h ketogenic diet and 18-h fasting preparations are as follows; Negative: 82.7%/52.1%/27.3%, Indeterminant: 2.0%/24.5%/40.9%, Positive: 15.3%/23.4%/31.8% (P<0.001). High agreement was present with the observer and the report (κ=0.88) <b>Conclusion:</b> A 72-h daytime ketogenic diet with 3-day overnight fasting, achieved substantially superior myocardial suppression versus 24-h ketogenic diet with overnight fasting and 18h-fasting using <sup>18</sup>F-FDG PET/CT. This 72-h preparation results in significantly fewer ""indeterminant"" and potentially ""false positive"" CS results.",success
29335272,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Although cardiac magnetic resonance (CMR) and positron emission tomography (PET) detect different pathological attributes of cardiac sarcoidosis (CS), the complementary value of these tests has not been evaluated. Our objective was to determine the value of combining CMR and PET in assessing the likelihood of CS and guiding patient management. In this retrospective study, we included 107 consecutive patients referred for evaluation of CS by both CMR and PET. Two experienced readers blinded to all clinical data reviewed CMR and PET images and categorized the likelihood of CS as no (<10%), possible (10%-50%), probable (50%-90%), or highly probable(>90%) based on predefined criteria. Patient management after imaging was assessed for all patients and across categories of increasing CS likelihood. A final clinical diagnosis for each patient was assigned based on a subsequent review of all available imaging, clinical, and pathological data. Among 107 patients (age, 55±11 years; left ventricular ejection fraction, 43±16%), 91 (85%) had late gadolinium enhancement, whereas 82 (76%) had abnormal F18-fluorodeoxyglucose uptake on PET, suggesting active inflammation. Among the 91 patients with positive late gadolinium enhancement, 60 (66%) had abnormal F18-fluorodeoxyglucose uptake. When PET data were added to CMR, 48 (45%) patients were reclassified as having a higher or lower likelihood of CS, most of them (80%) being correctly reclassified when compared with the final diagnosis. Changes in immunosuppressive therapies were significantly more likely among patients with highly probable CS. Among patients with suspected CS, combining CMR and PET provides complementary value for estimating the likelihood of CS and guiding patient management.",success
26111805,False,Comparative Study;Journal Article,,,,,,,,True,"Complete heart block (CHB) caused by myocardial inflammation is a serious consequence of cardiac sarcoidosis (CS) that requires early diagnosis for effective anti-inflammatory treatment. This study aimed to clarify the cardiac magnetic resonance imaging (MRI) and (18)F-fluoro-2-deoxyglucose positron emission tomography ((18)F-FDG PET) manifestations of newly diagnosed CS with CHB and to assess whether certain imaging features could predict responders to corticosteroid therapy. Fifteen newly diagnosed CS patients with CHB and 17 without CHB were examined. We defined abnormal (18)F-FDG uptake on (18)F-FDG PET and increased T2-weighted signal on cardiac MRI as signs of myocardial inflammation and delayed enhancement (DE) on cardiac MRI as a sign of myocardial fibrosis. Ten CHB+ patients were then treated with corticosteroids. The CHB+ group showed higher (18)F-FDG uptake and increased T2-weighted signal in the interventricular septum, which involves the electrical pathway of atrioventricular conduction, than the CHB- group (P = .001 and P < .0001, respectively), whereas there was no group difference in DE (P = .232). Six corticosteroid-treated patients recovered from CHB; all had exhibited increased T2-weighted signal, (18)F-FDG uptake, and DE in the interventricular septum before therapy. In contrast, among the 4 patients without recovery, 2 showed no abnormal (18)F-FDG uptake and 3 had no increased T2-weighted signal in the interventricular septum, but all showed DE. The 2 patients without recovery with abnormal (18)F-FDG uptake showed wall thinning in the interventricular septum. Focal inflammation in the interventricular septum was associated with CHB and might predict recovery from CHB after corticosteroids if it coexists with preserved wall thickness.",success
31177817,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Background The diagnostic yield of cardiac sarcoidosis (CS) by endomyocardial biopsy is limited. Fluorodeoxyglucose (FDG) positron emission tomography (PET) and cardiac magnetic resonance imaging (MRI) may facilitate noninvasive diagnosis, but the accuracy of this approach is not well defined. We aimed to correlate findings from FDG PET and cardiac MRI with histological findings from explanted hearts of patients who underwent cardiac transplantation. Methods We analyzed the explanted heart histology for all patients who underwent cardiac transplant at our center from April 2008 to July 2018 and had pretransplant FDG PET (n=18) or cardiac MRI (n=31). The likelihood of CS based on FDG PET or cardiac MRI was categorized in a blinded fashion using a previously published method. RESULTS: Using a CS probable cutoff for FDG PET resulted in a sensitivity of 100.0% (95% CI, 54.1%-100.0%) and a specificity of 33.3% (95% CI, 9.9%-65.1%). Three of the 9 CS probable by FDG PET cases were found to be arrhythmogenic cardiomyopathy. The test characteristics of cardiac MRI are more challenging to comment on using our data as there was only one confirmed case of CS on post-transplant histological assessment. Of the 8 CS highly probable or probable cases by cardiac MRI, 3 were found to be dilated cardiomyopathy, and 2 were found to be end-stage hypertrophic cardiomyopathy. Conclusions FDG PET and cardiac MRI can help facilitate the diagnosis of CS in patients with advanced heart failure with a high degree of sensitivity but lower specificity.",success
27277502,False,Journal Article,,,,,,,,True,"Although the number of clinical applications for fluorine-18 fluorodeoxyglucose (<sup>18</sup>F-FDG) cardiac positron emission tomography (PET) has continued to grow, there remains a lack of consensus regarding the ideal method of suppressing normal myocardial glucose utilization for image optimization. This review describes various patient preparation protocols that have been used as well as the success rates achieved in different studies. Collectively, the available literature supports using a high-fat, no-carbohydrate diet for at least two meals with a fast of 4-12 hours prior to <sup>18</sup>F-FDG PET imaging and suggests that isolated fasting for less than 12 hours and supplementation with food or drink just prior to imaging should be avoided. Each institution should adopt a protocol and continuously monitor its effectiveness with a goal to achieve adequate myocardial suppression in greater than 80% of patients.",success
28217949,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Dilated cardiomyopathy (DCM) is the most common cardiomyopathy and causes left ventricular enlargement and contractile dysfunction, with a poor prognosis. The mechanisms underlying the disease process have not been precisely identified, but recent evidence has suggested that the activation of myocardial inflammation is involved in the deterioration associated with the condition. Biopsy samples from 182 consecutive DCM patients were immunohistochemically stained with antibodies specific to CD3 (T lymphocytes), CD68 (whole macrophages), and CD163 (M2 macrophages), and each type of infiltrating cell was counted. Masson's trichrome staining was used to determine the collagen area fraction (CAF) in each sample. Patients were followed up for 6.9 ± 2.4 years, and their clinical data were obtained for analysis. Median (interquartile range) numbers of myocardial CD3, CD68, and CD163-cell infiltrates were 8.1 (4.0-14.2)/mm<sup>2</sup> , 22.3 (12.1-36.0)/mm<sup>2</sup> , and 6.5 (2.0-14.0)/mm<sup>2</sup> , respectively. Patients with higher counts of infiltrating CD3-, CD68-, and CD163-positive cells had significantly poorer outcomes (P = 0.007, P = 0.011, and P = 0.022, respectively). A high CD163-positive infiltrate count was independently associated with worse outcome in multivariate Cox regression analysis (hazard ratio 1.77, P = 0.004), and multivariate linear regression analysis revealed that the CD163 cell count was an independent determinant of CAF (P < 0.001). It was found that DCM with increased myocardial immune activation was associated with poor long-term outcome. The association between M2 macrophages and collagen formation suggests the phenotypic polarization of macrophages toward M2 may be associated with ventricular remodelling in DCM.",success
30089553,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"This study sought to characterize the electroanatomic (EAM) substrate in patients with cardiac sarcoidosis (CS) and ventricular tachycardia and its relationship to imaging findings of inflammation and fibrosis. CS is characterized by coexistence of active inflammation and replacement fibrosis. A total of 42 patients with CS based on established criteria and ventricular tachycardia underwent high-density EAM mapping. Abnormal electrograms (EGM) were collected and independently classified as multicomponent fractionated, isolated, late, and split according to standard criteria and regardless of the peak-to-peak bipolar/unipolar voltage. A total of 29 patients (69%) underwent pre-procedural cardiac magnetic resonance (CMR) and positron emission tomography (PET)/computed tomography (CT). The distribution of EAM substrate was correlated with regions of late gadolinium enhancement (LGE) on CMR and increased 18F-fluorodeoxyglucose uptake on PET/CT. Of 21,451 bipolar and unipolar EGM, 4,073 (19%) were classified as abnormal with a predominant distribution in the basal perivalvular segments and interventricular septum. Using the standard bipolar (<1.5 mV) and unipolar (<8.3 mV for left ventricle <5.5 mV for the right) voltage cutoff values, 40% and 22% of the abnormal EGM were located outside the EAM low-voltage areas, respectively. LGE was present in 26 of 29 patients (90%), whereas abnormal 18F-fluorodeoxyglucose uptake in 14 of 29 patients (48%) with imaging. Segments with abnormal EGM had more LGE-evident scar transmurality [median: 24% (interquartile range [IQR]: 4% to 40%) vs. median: 5% (IQR: 0% to 15%); p < 0.001] and lower metabolic activity (median: 20 g glucose [IQR: 14 g to 30 g] vs. median: 29 g glucose [IQR: 18 g to 39 g]; p < 0.001). Overall, the agreement between the presence of abnormal EGM was higher with the presence of LGE (κ = 0.51; p < 0.001) than with the presence of active inflammation (κ = -0.12; p = 0.003). In patients with CS and ventricular tachycardia, pre-procedural imaging with CMR and PET/CT can be useful in detecting EAM abnormalities that are potential targets for substrate ablation. Abnormal EGM were more likely located in segments with more scar transmurality (LGE) at CMR and a lower degree of inflammation on PET.",success
24819193,False,Consensus Development Conference;Journal Article,,,,,,,,False,,success
31597819,False,Journal Article;Practice Guideline,,,,,,,,False,,success
32240829,False,Comparative Study;Letter,,,,,,,,True,"The diagnosis of cardiac sarcoidosis (CS) is challenging. Because of the current limitations of endomyocardial biopsy as a reference standard, physicians rely on advanced cardiac imaging, multidisciplinary evaluation, and diagnostic criteria to diagnose CS. To compare the 3 main available diagnostic criteria in patients clinically judged to have CS. We prospectively included patients clinically judged to have CS by a multidisciplinary sarcoidosis team from November 2016 to October 2017. We included only incident cases (diagnosis of CS within 1 year of inclusion). We applied retrospectively the following diagnostic criteria: the World Association of Sarcoidosis and Other Granulomatous Diseases (WASOG), the Heart Rhythm Society (HRS), and the Japanese Circulation Society (JCS) 2016 criteria. We identified 69 patients. Diagnostic criteria classified patients as follows: WASOG as highly probable (1.4%), probable (52.2%), possible (0%), some criteria (40.6%), and no criteria (5.8%); HRS as histological diagnosis (1.4%), probable (52.2%), some criteria (40.6%), and no criteria (5.8%); JCS as histological diagnosis (1.4%), clinical diagnosis (58%), some criteria (39.1%), and no criteria (1.4%). Concordance was high between WASOG and HRS (κ = 1) but low between JCS and the others (κ = 0.326). A high proportion of patients clinically judged to have CS are unable to be classified according to the 3 main diagnostic criteria. There is low concordance between JCS criteria and the other 2 criteria (WASOG and HRS).",success
23733916,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Endomyocardial biopsy (EMB) is often considered when the pathogenesis of heart failure cannot be determined by noninvasive testing. Uncertainty remains about the diagnostic and clinical use of EMB in various clinical scenarios. We examined the characteristics of a cohort of patients with unexplained heart failure who underwent EMB at a tertiary care medical center. We categorized each patient into a clinical scenario as outlined by the 2007 AHA/ACC/ESC guidelines and determined the number of times EMB provided a diagnosis or altered the clinical course. A total of 851 patients underwent EMB from 2000-2009. Overall, 25.5% of EMBs provided a diagnosis and 22.7% of EMBs changed clinical course. Heart failure associated with unexplained restrictive cardiomyopathy was the most common clinical scenario, comprising 33.6% (286/851) of EMBs, and 84 (29.4%) of these EMBs were diagnostic. EMB for unexplained heart failure of <2 weeks duration had a diagnostic yield at 35% (39/109). There were 4 uncommon scenarios where EMB had a high diagnostic and clinical yield. There were 16 complications for an overall rate of 1.9%. We confirm that EMB is useful in acute onset unexplained cardiomyopathy. We demonstrate a role for EMB in suspected infiltrative disease and in the management of rare clinical scenarios, such as suspected hypersensitivity myocarditis, anthracycline cardiomyopathy, cardiac tumors, and arrhythmogenic right ventricular dysplasia/cardiomyopathy. Our results suggest low use of EMB in chronic heart failure that responds to usual care.",success
23246240,False,Case Reports;Journal Article,,,,,,,,True,"Diagnosing isolated cardiac sarcoidosis can be challenging, and requires a high index of suspicion. We report a case of a young woman who presented with sustained ventricular tachycardia, intermittent atrioventricular block and epsilon wave on electrocardiogram. Although the patient fulfilled Task Force criteria for arrhythmogenic right ventricular cardiomyopathy, sarcoidosis was suspected because of the presence of intermittent atrioventricular block. As illustrated in this report, the use of electroanatomic mapping-guided endomyocardial biopsy can be decisive in achieving the diagnosis and is a valuable approach in cases of suspected isolated cardiac sarcoidosis.",success
25194292,False,Journal Article,,,,,,,,True,"The aim of this study was to describe the method used to perform electrogram-guided EMB and correlate electrogram characteristics with pathological and clinical outcomes. Endomyocardial biopsy (EMB) is valuable in determining the underlying etiology of a cardiomyopathy. The sensitivity, however, for focal disorders, such as lymphocytic myocarditis and cardiac sarcoidosis (CS), is low. The sensitivity of routine fluoroscopically guided EMB is low. Abnormal intracardiac electrograms are seen at sites of myocardial disease. However, the exact value of electrogram-guided EMB is unknown. We report 11 patients who underwent electrogram-guided EMB for evaluation of myocarditis and CS. Of 40 total biopsy specimens taken from 11 patients, 19 had electrogram voltage <5 mV, all of which resulted in histopathologic abnormality (100% specificity and positive predictive value). A voltage amplitude cutoff value of 5 mV had substantially higher sensitivity (70% vs. 26%) and negative predictive value (62%) than 1.5 mV. Abnormal electrogram appearance at biopsy site had good sensitivity (67%) and specificity (92%) in predicting abnormal myocardium. Normal signals with voltage >5 mV signified normal myocardium with no significant diagnostic yield. Biopsy results guided therapy in all patients, including 5 with active myocarditis or CS, all of whom subsequently received immunosuppressive therapy. There were no procedural complications. In patients with suspected myocarditis or CS, electrogram-guided EMB targeting sites with abnormal or low-amplitude electrograms may increase the diagnostic yield for detecting abnormal pathological findings.",success
26411357,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Histologic proof of granulomatous inflammation is prerequisite for the diagnosis of cardiac sarcoidosis (CS). Because of the limited sensitivity of endomyocardial biopsy (EMB), confirmation of sarcoidosis often has to be acquired from extracardiac biopsies. We set out to review our experience of F-18-fluorodeoxyglucose positron emission tomography (F-18-FDG PET) in guiding extracardiac tissue biopsies in suspected CS. We included in this work 68 consecutive patients with proved CS who had undergone cardiac F-18-FDG PET with (n = 57) or without whole-body imaging as part of initial diagnostic evaluation. Their hospital charts, imaging studies, and diagnostic biopsies were reviewed in retrospect. Whole-body PET images showed extracardiac foci of abnormally high F-18-FDG uptake in 39 of 57 patients, of whom 38 had involvement of mediastinal lymph nodes (MLN). Parallel F-18-FDG uptake was found in other lymph nodes (n = 10), lungs (n = 9), liver (n = 3), spleen (n = 2), and thyroid gland (n = 1). Adding the mediastinal findings at cardiac PET without whole-body imaging, abnormal F-18-FDG uptake in MLN was found in totally 43 of the 68 patients with CS (63%). Histology of systemic sarcoidosis was known at presentation of cardiac symptoms in 8 patients. Of the 60 patients with missing histology, 24 patients underwent mediastinoscopy for sampling of PET-positive MLN, most often (n = 20) after nondiagnostic EMB; microscopy revealed diagnostic noncaseating granulomatous inflammation in 24 of the 24 cases (sensitivity 100%). In the remaining 36 patients, sarcoidosis histology was confirmed by EMB (n = 30), by biopsy of lungs (n = 2) or peripheral lymph nodes (n = 2), or at autopsy (n = 1) or post-transplantation (n = 1). In conclusion, MLN accumulate F-18-FDG at PET in most patients with CS and provide a highly productive source for diagnostic biopsies either primarily or subsequent to nondiagnostic EMB.",success
30003460,False,Journal Article;Systematic Review,,,,,,,,True,"Electroanatomic mapping (EAM) has been utilized as a modality to improve the sensitivity of endomyocardial biopsy (EMB). We sought to systematically review published medical literature on the efficacy and safety of EAM-guided EMB. We searched Ovid MEDLINE, Ovid Embase, Ovid CDR, Cochrane Central, Scopus, and Web of Science for studies where EAM was used for EMB. Data abstracted included demographics, indications, final diagnoses, histology findings, and technical details of biopsy extraction. Test characteristics including sensitivity (Se), specificity (Sp), and area under curve (AUC) were calculated on a per-patient and per-biopsy level. Seventeen studies (9 case series, 8 case reports) were included in this systematic review. EAM-guided EMB was performed in 148 patients and results of 207 individual biopsies were available for analysis. The most common indications for EAM-guided EMB were suspected arrhythmogenic right ventricular cardiomyopathy (ARVC), myocarditis, and cardiac sarcoidosis (CS). The pooled sensitivity and specificity for EAM-guided EMB for the diagnosis of cardiomyopathies (ARVC, myocarditis, CS, and other specific diagnoses) were 92 and 58% on per-biopsy analysis and 100 and 39% on per-patient analysis. Among the individual components of abnormal EGMs, abnormal unipolar EGM had the best AUC on per-biopsy (0.81, 95% CI 0.68-0.90) and per-patient analysis (0.84, 95% CI 0.68-0.92). EAM-guided EMB appears safe. Adverse events included 1 hemopericardium, 2 minimal asymptomatic pericardial effusions, and 1 femoral hematoma. EAM-guided EMB is a safe and efficacious method and might improve test characteristics over conventional fluoroscopy-guided biopsy.",success
32293205,False,"Journal Article;Practice Guideline;Research Support, Non-U.S. Gov't",,,,,,,,True,"<b>Background:</b> The diagnosis of sarcoidosis is not standardized but is based on three major criteria: a compatible clinical presentation, finding nonnecrotizing granulomatous inflammation in one or more tissue samples, and the exclusion of alternative causes of granulomatous disease. There are no universally accepted measures to determine if each diagnostic criterion has been satisfied; therefore, the diagnosis of sarcoidosis is never fully secure.<b>Methods:</b> Systematic reviews and, when appropriate, meta-analyses were performed to summarize the best available evidence. The evidence was appraised using the Grading of Recommendations, Assessment, Development, and Evaluation approach and then discussed by a multidisciplinary panel. Recommendations for or against various diagnostic tests were formulated and graded after the expert panel weighed desirable and undesirable consequences, certainty of estimates, feasibility, and acceptability.<b>Results:</b> The clinical presentation, histopathology, and exclusion of alternative diagnoses were summarized. On the basis of the available evidence, the expert committee made 1 strong recommendation for baseline serum calcium testing, 13 conditional recommendations, and 1 best practice statement. All evidence was very low quality.<b>Conclusions:</b> The panel used systematic reviews of the evidence to inform clinical recommendations in favor of or against various diagnostic tests in patients with suspected or known sarcoidosis. The evidence and recommendations should be revisited as new evidence becomes available.",success
32792073,False,Journal Article,,,,,,,,True,"It is estimated that 5% of patients with sarcoidosis have clinically manifest cardiac involvement, although autopsy and imaging studies suggest a significantly higher prevalence of cardiac involvement. There is a paucity of contemporary data on the risk of adverse cardiac outcomes, particularly heart failure (HF), in patients with sarcoidosis. The purpose of this study was to examine the long-term risk of HF and other adverse cardiac outcomes in patients with sarcoidosis compared with matched control subjects. In this cohort study, all patients age ≥18 years with newly diagnosed sarcoidosis (1996 to 2016) were identified through Danish nationwide registries and matched 1:4 by age, sex, and comorbidities with control subjects from the background population without sarcoidosis. Of the 12,042 patients diagnosed with sarcoidosis, 11,834 patients were matched with 47,336 subjects from the background population (median age: 42.8 years [25th to 75th percentile: 33.1 to 55.8 years], 54.3% men). The median follow-up was 8.2 years. Absolute 10-year risks of outcomes were as follows: HF: 3.18% (95% confidence interval [CI]: 2.83% to 3.57%) for sarcoidosis patients and 1.72% (95% CI: 1.58% to 1.86%) for the background population; the composite of ICD implantation, ventricular arrhythmias, and cardiac arrest: 0.96% (95% CI: 0.77% to 1.18%) for sarcoidosis patients and 0.45% (95% CI: 0.38% to 0.53%) for the background population; the composite of pacemaker implantation, atrioventricular block, and sinoatrial dysfunction: 0.94% (95% CI: 0.75% to 1.16%) for sarcoidosis patients and 0.51% (95% CI: 0.44% to 0.59%) for the background population; atrial fibrillation or flutter: 3.44% (95% CI: 3.06% to 3.84%) for sarcoidosis patients and 2.66% (95% CI: 2.49% to 2.84%) for the background population; and all-cause mortality: 10.88% (95% CI: 10.23% to 11.55%) for sarcoidosis patients and 7.43% (95% CI: 7.15% to 7.72%) for the background population. Patients with sarcoidosis had a higher associated risk of HF and other adverse cardiac outcomes compared with matched control subjects.",success
15893188,False,Duplicate Publication;Journal Article,,,,,,,,True,"This study analyzed the accuracy of gadolinium-enhanced cardiovascular magnetic resonance (CMR) for the diagnosis of cardiac sarcoidosis (CS). The diagnosis of CS was made according to the guidelines of the Japanese Ministry of Health and Welfare (1993); CMR has not been incorporated into the guidelines, and the diagnostic accuracy of CMR for the diagnosis of CS has not yet been evaluated. We performed an analysis of 12-lead electrocardiograms (ECGs), 24-h ambulatory ECGs, echocardiograms, thallium scintigrams, and gadolinium-enhanced CMR studies in 58 biopsy-proven pulmonary sarcoidosis patients assessed for CS. The diagnostic accuracy of CMR for CS was determined using modified Japanese guidelines as the gold standard. The diagnosis of CS was made in 12 of 58 patients (21%); CMR revealed late gadolinium enhancement (LGE), mostly involving basal and lateral segments (73%), in 19 patients. In 8 of the 19 patients, scintigraphy was normal, while patchy LGE was present. The sensitivity and specificity of CMR were 100% (95% confidence interval, 78% to 100%) and 78% (95% confidence interval, 64% to 89%), and the positive and negative predictive values were 55% and 100%, respectively, with an overall accuracy of 83%. In patients with sarcoidosis, CMR is a useful diagnostic tool to determine cardiac involvement. New diagnostic guidelines should include CMR.",success
26763280,False,Journal Article,,,,,,,,True,"Cardiac sarcoidosis is associated with an increased risk of heart failure and sudden death, but its risk in patients with preserved left ventricular ejection fraction is unknown. Using cardiovascular magnetic resonance in patients with extracardiac sarcoidosis and preserved left ventricular ejection fraction, we sought to (1) determine the prevalence of cardiac sarcoidosis or associated myocardial damage, defined by the presence of late gadolinium enhancement (LGE), (2) quantify their risk of death/ventricular tachycardia (VT), and (3) identify imaging-based covariates that predict who is at greatest risk of death/VT. Parameters of left and right ventricular function and LGE burden were measured in 205 patients with left ventricular ejection fraction >50% and extracardiac sarcoidosis who underwent cardiovascular magnetic resonance for LGE evaluation. The association between covariates and death/VT in the entire group and within the LGE+ group was determined using Cox proportional hazard models and time-dependent receiver-operator curves analysis. Forty-one of 205 patients (20%) had LGE; 12 of 205 (6%) died or had VT during follow-up; of these, 10 (83%) were in the LGE+ group. In the LGE+ group (1) the rate of death/VT per year was >20× higher than LGE- (4.9 versus 0.2%, P<0.01); (2) death/VT were associated with a greater burden of LGE (14±11 versus 5±5%, P<0.01) and right ventricular dysfunction (right ventricular EF 45±12 versus 53±28%, P=0.04). LGE burden was the best predictor of death/VT (area under the receiver-operating characteristics curve, 0.80); for every 1% increase of LGE burden, the hazard of death/VT increased by 8%. Sarcoidosis patients with LGE are at significant risk for death/VT, even with preserved left ventricular ejection fraction. Increased LGE burden and right ventricular dysfunction can identify LGE+ patients at highest risk of death/VT.",success
33176455,False,Journal Article;Review;Video-Audio Media;Consensus Development Conference,,,,,,,,True,"Myocarditis is an inflammatory disease of the heart that may occur because of infections, immune system activation, or exposure to drugs. The diagnosis of myocarditis has changed due to the introduction of cardiac magnetic resonance imaging. We present an expert consensus document aimed to summarize the common terminology related to myocarditis meanwhile highlighting some areas of controversies and uncertainties and the unmet clinical needs. In fact, controversies persist regarding mechanisms that determine the transition from the initial trigger to myocardial inflammation and from acute myocardial damage to chronic ventricular dysfunction. It is still uncertain which viruses (besides enteroviruses) cause direct tissue damage, act as triggers for immune-mediated damage, or both. Regarding terminology, myocarditis can be characterized according to etiology, phase, and severity of the disease, predominant symptoms, and pathological findings. Clinically, acute myocarditis (AM) implies a short time elapsed from the onset of symptoms and diagnosis (generally <1 month). In contrast, chronic inflammatory cardiomyopathy indicates myocardial inflammation with established dilated cardiomyopathy or hypokinetic nondilated phenotype, which in the advanced stages evolves into fibrosis without detectable inflammation. Suggested diagnostic and treatment recommendations for AM and chronic inflammatory cardiomyopathy are mainly based on expert opinion given the lack of well-designed contemporary clinical studies in the field. We will provide a shared and practical approach to patient diagnosis and management, underlying differences between the European and US scientific statements on this topic. We explain the role of histology that defines subtypes of myocarditis and its prognostic and therapeutic implications.",success
19017339,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Case studies indicate that cardiac sarcoid may mimic the clinical presentation of arrhythmogenic right ventricular dysplasia/cardiomyopathy (ARVD/C); however, the incidence and clinical predictors to diagnose cardiac sarcoid in patients who meet International Task Force criteria for ARVD/C are unknown. Patients referred for evaluation of left bundle branch block (LBBB)-type ventricular arrhythmia and suspected ARVD/C were prospectively evaluated by a standardized protocol including right ventricle (RV) cineangiography-guided myocardial biopsy. Sixteen patients had definite ARVD/C and four had probable ARVD/C. Three patients were found to have noncaseating granulomas on biopsy consistent with sarcoid. Age, systemic symptoms, findings on chest X-ray or magnetic resonance imaging (MRI), type of ventricular arrhythmia, RV function, ECG abnormalities, and the presence or duration of late potentials did not discriminate between sarcoid and ARVD/C. Left ventricular dysfunction (ejection fraction <50%) was present in 3/3 patients with cardiac sarcoid, but only 2/17 remaining patients with definite or probable ARVD/C (P = 0.01). In this prospective study of consecutive patients with suspected ARVD/C evaluated by a standard protocol including biopsy, the incidence of cardiac sarcoid was surprisingly high (15%). Clinical features, with the exception of left ventricular dysfunction and histological findings, did not discriminate between the two entities.",success
24585727,True,"Journal Article;Multicenter Study;Observational Study;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cardiac sarcoidosis (CS) may show overlap in the clinical presentation with arrhythmogenic right ventricular dysplasia/cardiomyopathy (ARVD/C). We sought to investigate patients with CS who were misdiagnosed with ARVD/C and identify clinical features to distinguish these 2 groups. Among patients enrolled in the Johns Hopkins ARVD/C registry, 15 patients with definite 2010 diagnostic criteria for ARVD/C were subsequently diagnosed with CS. Forty-two pathogenic desmosomal mutation carriers with definite ARVD/C based on the 2010 diagnostic criteria served as a control group. Patients with CS were older at the age of symptom onset, more likely to have comorbidities, and develop heart failure symptoms over time (P<0.05). Electrocardiographically, PR interval prolongation and high-grade atrioventricular block were exclusively associated with CS (P<0.05). HV interval prolongation and increased number of ventricular tachycardias induced were also associated with CS (P<0.05). Radiographically, significant left ventricular dysfunction, myocardial delayed enhancement of the septum, and mediastinal lymphadenopathy were more often see in those with CS (P<0.05). The 2010 diagnostic criteria for ARVD/C have limited discrimination in distinguishing between ARVD/C and CS. Despite the overlay in clinical presentation, older age of symptom onset, presence of cardiovascular comorbidities, nonfamilial pattern of disease, PR interval prolongation, high-grade atrioventricular block, significant left ventricular dysfunction, myocardial delayed enhancement of the septum, and mediastinal lymphadenopathy should raise the suspicion for CS.",success
31902242,False,Journal Article,,,,,,,,True,"Fulminant myocarditis (FM) is an uncommon syndrome characterized by sudden and severe diffuse cardiac inflammation often leading to death resulting from cardiogenic shock, ventricular arrhythmias, or multiorgan system failure. Historically, FM was almost exclusively diagnosed at autopsy. By definition, all patients with FM will need some form of inotropic or mechanical circulatory support to maintain end-organ perfusion until transplantation or recovery. Specific subtypes of FM may respond to immunomodulatory therapy in addition to guideline-directed medical care. Despite the increasing availability of circulatory support, orthotopic heart transplantation, and disease-specific treatments, patients with FM experience significant morbidity and mortality as a result of a delay in diagnosis and initiation of circulatory support and lack of appropriately trained specialists to manage the condition. This scientific statement outlines the resources necessary to manage the spectrum of FM, including extracorporeal life support, percutaneous and durable ventricular assist devices, transplantation capabilities, and specialists in advanced heart failure, cardiothoracic surgery, cardiac pathology, immunology, and infectious disease. Education of frontline providers who are most likely to encounter FM first is essential to increase timely access to appropriately resourced facilities, to prevent multiorgan system failure, and to tailor disease-specific therapy as early as possible in the disease process.",success
35084701,False,Journal Article,,,,,,,,True,"<sup>18</sup>F-flurodeoxyglycose (FDG)/<sup>13</sup>N-ammonia positron emission tomography/computed tomography (PET/CT) is frequently utilized to evaluate cardiac sarcoidosis (CS) but findings can reflect other forms of myocardial inflammation or altered myocardial metabolic activity. Herein, we present five cases where cardiac PET findings suggested CS, but right ventricular endomyocardial biopsy samples revealed ATTR-type cardiac amyloidosis.",success
36697326,False,Practice Guideline;Journal Article;Consensus Development Conference,,,,,,,,False,,success
23623644,False,Journal Article;Meta-Analysis;Systematic Review,,,,,,,,True,"There are no published clinical consensus guidelines or systematic evaluation supporting the use of corticosteroids for the treatment of cardiac sarcoidosis. The purpose of this study was to systematically review the published data on corticosteroid treatment of cardiac sarcoidosis. Studies were identified from MEDLINE, EMBASE, Cochrane Controlled Trials Register, Cochrane Database of Systematic Reviews, and National Institutes of Health Clinical Trials.gov database. The quality of included articles was rated using Scottish Intercollegiate Guidelines Network 50. Outcomes examined were atrioventricular (AV) conduction, left ventricular function, ventricular arrhythmias, and mortality. A total of 1491 references were retrieved and 10 publications met the inclusion criteria. There were no randomized trials and all publications were of poor to fair quality. In the 10 reports, 257 patients received corticosteroids and 42 patients did not. There were 57 patients with AV conduction disease treated with corticosteroids, with 27/57 (47.4%) improving. In contrast, 16 patients were not treated with corticosteroids and 0/16 improved. Four publications reported on left ventricular function recovery, 2 reported on ventricular arrhythmia burden, and 9 reported on mortality. However, the data quality were too limited to draw conclusions for any of these outcomes. Our systematic review identified 10 publications reporting outcomes after corticosteroid therapy. The best data relates to AV conduction recovery and corticosteroids appeared to be beneficial. It is not possible to draw clear conclusions about the utility of corticosteroids for the other outcomes. There is a clear need for large multicentre prospective registries and trials in this patient population.",success
15619415,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Forty-three patients with cardiac sarcoidosis were studied echocardiographically before and after (mean follow-up 88 months) steroid therapy to determine the effectiveness of corticosteroids to prevent left ventricular (LV) remodeling and improve LV contractility. In patients with initial LV ejection fractions (LVEFs) >or=55%, long-term steroid therapy showed preventive effects for LV remodeling and LV function. Patients with LVEF <54% showed significant reductions of LV volumes and LVEF improvement. However, in patients with LVEFs <30%, steroid therapy resulted in neither LV volume reductions nor improved LVEFs. In the early or middle stage of the disease, steroid therapy may be protective or therapeutic in preventing LV remodeling and preserving LV function. However, it may not be as effective in the late stage.",success
35701237,False,Journal Article,,,,,,,,True,"Cardiac sarcoidosis (CS) is an important cause of cardiomyopathy. The trajectory of left ventricular ejection fraction (LVEF) in patients with CS undergoing treatment remains unclear. Patients with CS who were treated with corticosteroids and who underwent transthoracic echocardiography were studied. Baseline characteristics, treatment, echocardiographic data (including baseline to follow-up change in LVEF), and outcomes were retrospectively evaluated. Among 100 patients, 55 had baseline reduced LVEF (<50%), and 45 had preserved LVEF (≥50%). At follow-up, 82% of patients demonstrated stable or improved LVEF. Change in LVEF was significantly higher in the baseline reduced than in the preserved LVEF group (5% [interquartile range 0 to 15] vs 0% [interquartile range -10% to 5%], p = 0.001). There was no difference in corticosteroid exposure or use of heart failure guideline-directed medical therapy between patients who did experience improvement in LVEF and those who did not experience improvement in LVEF. On multivariable analysis, baseline reduced LVEF (Odds ratio 54.89, 95% confidence interval 3.84 to 785.09, p = 0.003) and complete heart block (Odds ratio 28.88, 95% confidence interval 2.17 to 383.74, p = 0.011) at presentation were significantly associated with reduced LVEF after treatment. In conclusion, most patients with CS treated with corticosteroids maintain or improve LV systolic function. Cardiac characteristics at presentation impact prognosis in CS, despite treatment.",success
11703997,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cardiac involvement is an important prognostic factor in sarcoidosis, but reliable indicators of mortality risk in cardiac sarcoidosis are unstudied in a large number of patients. To determine the significant predictors of mortality and to assess the efficacy of corticosteroids, we analyzed clinical findings, treatment, and prognosis in 95 Japanese patients with cardiac sarcoidosis. Twenty of these 95 patients had cardiac sarcoidosis proven by autopsy; none of these patients had received corticosteroids. We assessed 12 clinical variables as possible predictors of mortality by Cox proportional hazards model in 75 steroid-treated patients. During the mean follow-up of 68 months, 29 patients (73%) died of congestive heart failure and 11 (27%) experienced sudden death. Kaplan-Meier survival curves showed 5-year survival rates of 75% in the steroid-treated patients and of 89% in patients with a left ventricular ejection fraction > or = 50%, whereas there was only 10% 5-year survival rate in autopsy subjects. There was no significant difference in survival curves of patients treated with a high initial dose (> 30 mg) and a low initial dose (> or = 30 mg) of prednisone. Multivariate analysis identified New York Heart Association functional class (hazard ratio 7.72 per class I increase, p = 0.0008), left ventricular end-diastolic diameter (hazard ratio 2.60/10 mm increase, p = 0.02), and sustained ventricular tachycardia (hazard ratio 7.20, p = 0.03) as independent predictors of mortality. In conclusion, the severity of heart failure was one of the most significant independent predictors of mortality for cardiac sarcoidosis. Starting corticosteroids before the occurrence of systolic dysfunction resulted in an excellent clinical outcome. A high initial dose of prednisone may not be essential for treatment of cardiac sarcoidosis.",success
31954655,False,Comparative Study;Letter,,,,,,,,False,,success
36357136,False,Journal Article,,,,,,,,True,"Patients with suspected cardiac sarcoidosis frequently undergo fluorodeoxyglucose (FDG)-positron emission tomography (PET)/computed tomography (CT) imaging to assess disease activity at baseline and after treatment initiation. This study investigated the effect of immunosuppressive therapy and biopsy status to achieve complete treatment response (CTR), partial treatment response (PTR), or no response (NR) on myocardial FDG-PET/CT. This study analyzed 83 patients with suspected cardiac sarcoidosis (aged 53 ± 1.8 years, 71% were male, 69% were White, 61% had a history of biopsy-confirmed sarcoidosis) who were treatment naive, had evidence of myocardial FDG at baseline, and underwent repeat PET imaging after treatment initiation. CTR was graded visually, and PTR/NR were measured both visually and quantitatively using the total glycolytic activity. Patients were also evaluated for the occurrence of death, sustained ventricular arrhythmias, and heart failure admissions. Overall, 59 patients (71%) achieved CTR/PTR (30%/41%) at follow-up scan (P = 0.04). Total glycolytic activity and visual estimate of PTR/NR had excellent agreement (κ = 0.86 [95% CI: 0.72-0.99]; P < 0.0001). In patients receiving prednisone only, the highest rates of CTR/PTR were observed in patients initiated on moderate or high dose (P < 0.01). In a regression model, moderate prednisone start dose (P = 0.03) was more strongly associated with achieving CTR/PTR than was high prednisone start dose. However, the latter patients were tapered faster between start dose and follow-up scan (P < 0.01). After a median follow-up of 4.7 (IQR: 3.1-7.8) years, patients who were biopsy-proven (vs non-biopsy-proven; P = 0.029) and with preserved left ventricular function (P = 002) were less likely to experience major adverse cardiac events. Outcomes based on treatment response status (CTR vs PTR vs NR; P = 0.23) were not significantly different. Among patients with suspected sarcoidosis and evidence of myocardial inflammation, treatment response by serial FDG-PET was variable, but a favorable response was more common when using moderate-to-high intensity prednisone dose. Biopsy-proven individuals and those with preserved systolic function were less likely to experience adverse outcomes during follow-up.",success
24488991,False,Editorial;Comment,,,,,,,,False,,success
23988768,False,Journal Article,,,,,,,,True,"Infliximab is effective as a third-line therapeutic for severe sarcoidosis; however, long-term efficacy is unknown. The aim of this study was to assess the relapse rate after discontinuation of infliximab in sarcoidosis patients and predict relapse by analysis of the activity marker soluble interleukin (IL)-2 receptor (sIL-2R) and maximum standardised uptake value (SUVmax) of (18)F-fluorodeoxyglucose positron emission tomography (FDG PET). In this retrospective cohort study, the proportion of relapse was analysed using the Kaplan-Meier method and predicting factors were studied using Cox regression. 47 sarcoidosis patients who started infliximab therapy were included in the risk analysis. Kaplan-Meier analysis revealed a median time to relapse of 11.1 months and showed that 25% of the cohort relapsed within 4 months. Both mediastinal SUVmax ≥ 6.0 on FDG PET (hazard ratio 3.77, p<0.001) and serum sIL-2R ≥ 4000 pg · mL(-1) (hazard ratio 2.24, p=0.033) at start of therapy predicted relapse. In multivariate analysis, a mediastinal SUVmax ≥ 6.0 at initiation of therapy was an independent predictor of relapse (hazard ratio 4.33, p<0.001). The majority of patients that discontinued infliximab therapy relapsed. High serum sIL-2R and high SUVmax on FDG PET at initiation of therapy were significant predictors of relapse. These results suggest close monitoring of patients in this category when they discontinue infliximab treatment.",success
31538835,False,Journal Article,,,,,,,,True,"Background Long-term corticosteroid therapy is the standard of care for treatment of cardiac sarcoidosis (CS). The efficacy of long-term corticosteroid-sparing immunosuppression in CS is unknown. The goal of this study was to assess the efficacy of methotrexate with or without adalimumab for long-term disease suppression in CS, and to assess recurrence and adverse event rates after immunosuppression discontinuation. Methods and Results Retrospective chart review identified treatment-naive CS patients at a single academic medical center who received corticosteroid-sparing maintenance therapy. Demographics, cardiac uptake of 18-fluorodeoxyglucose, and adverse cardiac events were compared before and during treatment and between those with persistent or interrupted immunosuppression. Twenty-eight CS patients were followed for a mean 4.1 (SD 1.5) years. Twenty-five patients received 4 to 8 weeks of high-dose prednisone (>30 mg/day), followed by taper and maintenance therapy with methotrexate±low-dose prednisone (low-dose prednisone, <10 mg/day). Adalimumab was added in 19 patients with persistently active CS or in those with intolerance to methotrexate. Methotrexate±low-dose prednisone resulted in initial reduction (88%) or elimination (60%) of 18-fluorodeoxyglucose uptake, and patients receiving adalimumab-containing regimens experienced improved (84%) or resolved (63%) 18-fluorodeoxyglucose uptake. Radiologic relapse occurred in 8 of 9 patients after immunosuppression cessation, 4 patients on methotrexate-containing regimens, and in no patients on adalimumab-containing regimens. Conclusions Corticosteroid-sparing regimens containing methotrexate with or without adalimumab is an effective maintenance therapy in patients after an initial response is confirmed. Disease recurrence in patients on and off immunosuppression support need for ongoing radiologic surveillance regardless of immunosuppression regimen.",success
34166800,False,Journal Article,,,,,,,,True,"Cardiac sarcoidosis (CS) is a major cause of morbidity and mortality in patients with systemic sarcoidosis. Steroid-sparing agents are increasingly used, despite a lack of randomized trials or published guidelines to direct treatment. This retrospective study included 77 patients with CS treated with prednisone monotherapy (n = 32) or a combination with mycophenolate mofetil (n = 45) between 2003 and 2018. Baseline characteristics and clinical outcomes were evaluated. The mean patient age was 53 ± 11 years at CS diagnosis, 66.2% were male, and 35.1% were Black. The total exposure to maximum prednisone dose (initial prednisone dose × days at dose) was lower in the combination therapy group (1440 mg [interquartile range (IQR), 1200-2760 mg] vs 2710 mg [IQR, 1200-5080 mg]; P = .06). On <sup>18</sup>F-fluorodeoxyglucose positron emission tomography scans, both groups demonstrated a significant decrease in the cardiac maximum standardized uptake value after treatment: a median decrease of 3.9 (IQR 2.7-9.0, P = .002) and 2.9 (IQR 0-5.0, P = .001) for prednisone monotherapy and combination therapy, respectively. Most patients experienced improvement or complete resolution in qualitative cardiac <sup>18</sup>F-fluorodeoxyglucose uptake (92.3% and 70.4% for the prednisone and combination therapy groups, respectively). Mycophenolate mofetil was well tolerated. Mycophenolate mofetil in combination with prednisone for the treatment of CS may minimize corticosteroid exposure and decrease cardiac inflammation without significant adverse effects.",success
34320381,False,Journal Article,,,,,,,,True,"We sought to examine the effect of anti-B-cell therapy (rituximab) on cardiac inflammation and function in corticosteroid-refractory cardiac sarcoidosis. Cardiac sarcoidosis (CS) is a rare cause of cardiomyopathy characterized by granulomatous inflammation involving the myocardium. Although typically responsive to corticosteroid treatment, there is a critical need for identifying effective steroid-sparing agents for disease control. Despite increasing evidence on the role of B cells in the pathogenesis of sarcoidosis, there is limited data on the efficacy of anti-B-cell therapy, specifically rituximab, for controlling CS. We reviewed the clinical experience at a tertiary care referral center of all patients with CS who received rituximab after failing to improve with initial immunosuppression therapy, which included corticosteroids. Fluorodeoxyglucose positron emission tomography (FDG PET/CT) images before and after rituximab treatment were evaluated. All images were interpreted by 2 experienced nuclear medicine trained physicians. We identified 7 patients (5 men, 2 women; mean age at diagnosis, 49.0 ± 7.9 years) with active CS who were treated with rituximab. The median length of follow-up was 5.1 years. All individuals, but 1, had received prior steroid-sparing agents in addition to corticosteroids. Rituximab was administered either as 1000 mg intravenously ×1 or ×2 doses, separated by 2 weeks. Repeat dosing, if appropriate, was considered after 6 months. All tolerated the infusions well. Inflammation as assessed by maximum standardized uptake value on cardiac FDG PET/CT uptake significantly decreased in 6 of 7 patients (median 6.0-4.5, Wilcoxon signed rank z -1.8593, W 3), whereas the left ventricular ejection fraction improved or stabilized in 4 patients but decreased in 3. The mean left ventricular ejection fraction was 40.1% and 43.3% before and after treatment, respectively (P = .28). Three patients reported improved physical capacity, and 5 patients showed improved arrhythmic burden on Holter monitoring or implantable cardioverter-defibrillator interrogation. One patient subsequently developed a fungal catheter-associated infection and sepsis requiring discontinuation. Rituximab was well-tolerated and seemed to decrease inflammation, as assessed by cardiac FDG PET/CT in all but 1 patient with active CS. These data suggest that rituximab may be a promising therapeutic option for CS, which deserves merits further study.",success
12796126,True,"Clinical Trial;Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"Preclinical and preliminary clinical data have suggested that tumor necrosis factor-alpha (TNFalpha) may play a role in the evolution and progression of heart failure and that inhibition of TNFalpha may favorably modify the course of the disease. We evaluated the efficacy and safety of infliximab, a chimeric monoclonal antibody to TNFalpha, in patients with moderate-to-severe heart failure. One hundred fifty patients with stable New York Heart Association class III or IV heart failure and left ventricular ejection fraction <or=35% were randomly assigned to receive placebo (n=49), infliximab 5 mg/kg (n=50), or infliximab 10 mg/kg (n=51) at 0, 2, and 6 weeks after randomization and were followed-up prospectively for 28 weeks. Neither dose of infliximab improved clinical status at 14 weeks, the primary endpoint of the study, despite suppression of inflammatory markers (C-reactive protein and interleukin-6) and a modest increase in ejection fraction in the patients receiving 5 mg/kg (P=0.013). Furthermore, after 28 weeks, 13, 10, and 20 patients were hospitalized for any reason in the placebo, 5 mg/kg infliximab, and 10 mg/kg infliximab groups, respectively. The combined risk of death from any cause or hospitalization for heart failure through 28 weeks was increased in the patients randomized to 10 mg/kg infliximab (hazard ratio 2.84, 95% confidence interval 1.01 to 7.97; nominal P=0.043). Short-term TNFalpha antagonism with infliximab did not improve and high doses (10 mg/kg) adversely affected the clinical condition of patients with moderate-to-severe chronic heart failure.",success
33778595,False,Journal Article,,,,,,,,True,"To compare the contributions of cardiac MRI and PET in the diagnosis and management of cardiac sarcoidosis (CS), with particular reference to quantitative measures. This is a retrospective, observational study of 31 patients (mean age, 45.7 years) with proven extracardiac sarcoidosis and possible CS who were investigated with fluorine 18 fluorodeoxyglucose (FDG) PET/CT and cardiac MRI. Patients were treated at physicians' discretion with repeat combined imaging after an interval of 102-770 days (median, 228 days). Significant myocardial FDG uptake was shown on visit 1 (myocardial maximum standardized uptake value [SUV<sub>max</sub>] > 3.6) in 17 of 22 patients who were subsequently treated. Myocardial SUV<sub>max</sub> decreased at follow-up (6.5 to 4.0; <i>P</i> < .01) and was matched by significant decreases in FDG-avid lung and mediastinal node disease. A volumetric measure of myocardium above a threshold SUV (cardiac metabolic volume) decreased from a mean of 42.5 to a mean of 4.1 (<i>P</i> < .001). This was associated with significant improvement in the left ventricular ejection fraction (LVEF) (45.8 increasing to 50.9; <i>P</i> < .031). There was no change in volume of late gadolinium enhancement at treatment. Patients who were untreated showed no change in any FDG PET or cardiac MRI parameter. Myocardial FDG uptake in patients suspected of having CS is presumed to represent active inflammation. When treated with corticosteroids, this resolved or regressed at follow-up, with an improvement in LVEF and FDG-avid thoracic disease. Patients who were untreated showed no change in any parameter. Quantification of FDG-avid myocardium using cardiac metabolic volume is proposed as a useful objective measure for assessing response to therapy.© RSNA, 2020See also commentary by Gutberlet in this issue.",success
24140661,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"This study sought to relate imaging findings on positron emission tomography (PET) to adverse cardiac events in patients referred for evaluation of known or suspected cardiac sarcoidosis. Although cardiac PET is commonly used to evaluate patients with suspected cardiac sarcoidosis, the relationship between PET findings and clinical outcomes has not been reported. We studied 118 consecutive patients with no history of coronary artery disease, who were referred for PET, using [(18)F]fluorodeoxyglucose (FDG) to assess for inflammation and rubidium-82 to evaluate for perfusion defects (PD), following a high-fat/low-carbohydrate diet to suppress normal myocardial glucose uptake. Blind readings of PET data categorized cardiac findings as normal, positive PD or FDG, positive PD and FDG. Images were also used to identify whether findings of extra-cardiac sarcoidosis were present. Adverse events (AE)-death or sustained ventricular tachycardia (VT)-were ascertained by electronic medical records, defibrillator interrogation, patient questionnaires, and telephone interviews. Among the 118 patients (age 52 ± 11 years; 57% males; mean ejection fraction: 47 ± 16%), 47 (40%) had normal and 71 (60%) had abnormal cardiac PET findings. Over a median follow-up of 1.5 years, there were 31 (26%) adverse events (27 VT and 8 deaths). Cardiac PET findings were predictive of AE, and the presence of both a PD and abnormal FDG (29% of patients) was associated with hazard ratio of 3.9 (p < 0.01) and remained significant after adjusting for left ventricular ejection fraction (LVEF) and clinical criteria. Extra-cardiac FDG uptake (26% of patients) was not associated with AE. The presence of focal PD and FDG uptake on cardiac PET identifies patients at higher risk of death or VT. These findings offer prognostic value beyond Japanese Ministry of Health and Welfare clinical criteria, the presence of extra-cardiac sarcoidosis and LVEF.",success
35363500,False,Journal Article;Review,,,,,,,,True,"The ""2022 AHA/ACC/HFSA Guideline for the Management of Heart Failure"" replaces the ""2013 ACCF/AHA Guideline for the Management of Heart Failure"" and the ""2017 ACC/AHA/HFSA Focused Update of the 2013 ACCF/AHA Guideline for the Management of Heart Failure."" The 2022 guideline is intended to provide patient-centric recommendations for clinicians to prevent, diagnose, and manage patients with heart failure. A comprehensive literature search was conducted from May 2020 to December 2020, encompassing studies, reviews, and other evidence conducted on human subjects that were published in English from MEDLINE (PubMed), EMBASE, the Cochrane Collaboration, the Agency for Healthcare Research and Quality, and other relevant databases. Additional relevant clinical trials and research studies, published through September 2021, were also considered. This guideline was harmonized with other American Heart Association/American College of Cardiology guidelines published through December 2021. Structure: Heart failure remains a leading cause of morbidity and mortality globally. The 2022 heart failure guideline provides recommendations based on contemporary evidence for the treatment of these patients. The recommendations present an evidence-based approach to managing patients with heart failure, with the intent to improve quality of care and align with patients' interests. Many recommendations from the earlier heart failure guidelines have been updated with new evidence, and new recommendations have been created when supported by published data. Value statements are provided for certain treatments with high-quality published economic analyses.",success
30561613,False,Journal Article;Practice Guideline,,,,,,,,True,"Myocardial diseases are associated with an increased risk of potentially fatal cardiac arrhythmias and sudden cardiac death/cardiac arrest during exercise, including hypertrophic cardiomyopathy, dilated cardiomyopathy, left ventricular non-compaction, arrhythmogenic cardiomyopathy, and myo-pericarditis. Practicing cardiologists and sport physicians are required to identify high-risk individuals harbouring these cardiac diseases in a timely fashion in the setting of preparticipation screening or medical consultation and provide appropriate advice regarding the participation in competitive sport activities and/or regular exercise programmes. Many asymptomatic (or mildly symptomatic) patients with cardiomyopathies aspire to participate in leisure-time and amateur sport activities to take advantage of the multiple benefits of a physically active lifestyle. In 2005, The European Society of Cardiology (ESC) published recommendations for participation in competitive sport in athletes with cardiomyopathies and myo-pericarditis. One decade on, these recommendations are partly obsolete given the evolving knowledge of the diagnosis, management and treatment of cardiomyopathies and myo-pericarditis. The present document, therefore, aims to offer a comprehensive overview of the most updated recommendations for practicing cardiologists and sport physicians managing athletes with cardiomyopathies and myo-pericarditis and provides pragmatic advice for safe participation in competitive sport at professional and amateur level, as well as in a variety of recreational physical activities.",success
30378224,True,Journal Article;Multicenter Study,,,,,,,,True,"Cardiac sarcoidosis (CS) often presents with ventricular arrhythmias, heart block, and cardiomyopathy. The prognosis of CS with contemporary management is uncertain. We estimated the impact of baseline and treatment variables on left ventricular ejection fraction (LVEF), ventricular assist device placement, heart transplant, and death. We identified patients with CS seen from 1994-2014 at two large academic medical centres. All met the 2014 Heart Rhythm Society expert consensus criteria for diagnosis. From the 574 patients identified, 91 met inclusion criteria. Twenty-two (24.2%) were diagnosed by endomyocardial biopsy. Cardiomyopathy was the primary presentation in 47 patients (51.6%). Within 90 days of diagnosis, 41 patients (45.0%) received prednisone alone, 29 (31.9%) received alternative immunosuppression with or without prednisone, and 21 (23.1%) received no immunosuppression. During follow-up, 31 of 47 cardiomyopathy patients experienced improvement in LVEF, while 23 experienced decline in LVEF or clinical exacerbation, and 15 of 22 patients presenting with ventricular arrhythmia had recurrence. These results did not differ by treatment group. During a median follow-up of 44 months for our cohort, 14 patients reached the composite endpoint of ventricular assist device placement, heart transplant, or death. Survival without the composite outcome did not differ by treatment group, but was worse among patients presenting with cardiomyopathy (log-rank = 0.005). In a large series of CS subjects, rates of ventricular arrhythmia and heart failure events remain high with no treatment regimen clearly associated with better outcome. Patients with cardiomyopathy at diagnosis were more likely to reach the composite endpoint.",success
26776864,False,Journal Article;Practice Guideline,,,,,,,,False,,success
29482029,True,Journal Article;Multicenter Study,,,,,,,,True,"Patients with end-stage cardiomyopathy due to cardiac sarcoidosis (CS) may be referred for mechanical circulatory support (MCS) and heart transplantation (HT). We describe outcomes of patients with CS undergoing HT, focusing on the use of MCS as a bridge to transplant (BTT). Using the United Network for Organ Sharing Scientific Registry of Transplant Recipients, we identified all adult waitlisted patients and isolated HT recipients from 2006 to 2015. These were divided into those with and without CS and further divided into those who did or did not receive MCS as BTT. Outcomes included 1- and 5-year post-transplantation freedom from mortality and 5-year freedom from primary graft failure. Over the study period, 31,528 patients were listed for HT, 148 (0.4%) of whom had CS. Among the CS patients, 34 (23%) received MCS as BTT. 18,348 patients (58%) eventually underwent HT, including 67 (0.4%) with CS, 20 (30%) of whom had received BTT MCS. Compared with non-CS diagnoses, CS patients had similar 1-year (91% vs 90%; log rank P = .88) and 5-year (83% vs 77%; log rank P = .46) freedom from mortality. Survival was also similar between CS BTT and non-CS BTT groups at 1 year (89% vs 89%; log-rank P = .92) and 5 years (72% vs 75%; log-rank P = .77). Survivals after HT were similar between CS and non-CS patients out to 5 years, and were also similar between CS and non-CS BTT cohorts. Both HT and BTT MCS should be considered in patients with CS.",success
34260889,False,Journal Article;Review,,,,,,,,True,"The prevalence of sarcoidosis-related cardiomyopathy is increasing. Sarcoidosis impacts cardiac function through granulomatous infiltration of the heart, resulting in conduction disease, arrhythmia, and/or heart failure. The diagnosis of cardiac sarcoidosis (CS) can be challenging and requires clinician awareness as well as differentiation from overlapping diagnostic phenotypes, such as other forms of myocarditis and arrhythmogenic cardiomyopathy. Clinical manifestations, extracardiac involvement, histopathology, and advanced cardiac imaging can all lend support to a diagnosis of CS. The mainstay of therapy for CS is immunosuppression; however, no prospective clinical trials exist to guide management. Patients may progress to developing advanced heart failure or ventricular arrhythmia, for which ventricular assist device therapies or heart transplantation may be considered. The existing knowledge gaps in CS call for an interdisciplinary approach to both patient care and future investigation to improve mechanistic understanding and therapeutic strategies.",success
33048376,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The new heart transplantation (HT) allocation policy was introduced on 10/18/2018. Using the UNOS registry, we examined early outcomes following HT for restrictive cardiomyopathy, hypertrophic cardiomyopathy, cardiac sarcoidosis, or cardiac amyloidosis compared to the old system. Those listed who had an event (transplant, death, or waitlist removal) prior to 10/17/2018 were in Era 1, and those listed on or after 10/18/2018 were in Era 2. The primary endpoint was death on the waitlist or delisting due to clinical deterioration. A total of 1232 HT candidates were included, 855 (69.4%) in Era 1 and 377 (30.6%) in Era 2. In Era 2, there was a significant increase in the use of temporary mechanical circulatory support and a reduction in the primary endpoint, (20.9 events per 100 PY (Era 1) vs. 18.6 events per 100 PY (Era 2), OR 1.98, p = .005). Median waitlist time decreased (91 vs. 58 days, p < .001), and transplantation rate increased (119.0 to 204.7 transplants/100 PY for Era 1 vs Era 2). Under the new policy, there has been a decrease in waitlist time and waitlist mortality/delisting due to clinical deterioration, and an increase in transplantation rates for patients with infiltrative, hypertrophic, and restrictive cardiomyopathies without any effect on post-transplant 6-month survival.",success
34756511,True,Journal Article;Multicenter Study,,,,,,,,True,"Cardiac sarcoidosis (CS) is a progressive inflammatory cardiomyopathy that can lead to heart failure, arrhythmia, and death. There is limited data on Orthotopic Heart Transplantation (OHT) outcomes in patients with CS. Here we examine outcomes in patients with CS who have undergone OHT at centers throughout the United States from 1987 to 2019. This was an analysis of 63,947 adult patients undergoing OHT captured in the United Network for Organ Sharing (UNOS) registry. Patients were characterized as cardiac sarcoidosis (CS) or Non-CS. Baseline characteristics were compared using chi-square and Kruskal-Wallis Tests. Outcomes of interest included primary graft failure, patient survival, treated graft rejection, hospitalization for infection, and post-transplant malignancy. During the study period 227 patients with CS underwent OHT. Patients with CS were younger, had higher proportion of non-white patients, and received transplants at more urgent statuses. After multivariable modeling there was no difference in survival (HR 0.86, CI 0.59-1.3, p = 0.446) or graft failure (HR 0.849, CI 0.58-1.23, p = 0.394) between patients with CS and Non-CS. Patients with CS had lower odds of rejection (OR 0.558, CI 0.315- 0.985, p = 0.0444). Patients with CS had similar odds of hospitalization for infection and post-transplant malignancy, as Non-CS patients. Patients with CS and Non-CS had similar post OHT survival, odds of graft failure, hospitalizations for infection, and post-transplant malignancy. Results of this study confirm the role of heart transplantation as a viable option for patients with CS.",success
34474157,False,Journal Article,,,,,,,,True,"Durable mechanical circulatory support (MCS) therapy improves survival in patients with advanced heart failure. Knowledge regarding the outcomes experienced by patients with inflammatory cardiomyopathy (CM) who receive durable MCS therapy is limited. We compared patients with inflammatory CM with patients with idiopathic dilated CM enrolled in the STS-INTERMACS registry. Among 19,012 patients, 329 (1.7%) had inflammatory CM and 5978 had idiopathic dilated CM (31.4%). The patients with inflammatory CM were younger, more likely to be White, and women. These patients experienced more preoperative arrhythmias and higher use of temporary MCS. Patients with inflammatory CM had a higher rate of early adverse events (<3 months after device implant), including bleeding, arrhythmias, non-device-related infections, neurologic dysfunction, and respiratory failure. The rate of late adverse events (≥3 months) was similar in the 2 groups. Patients with inflammatory CM had a similar 1-year (80% vs 84%) and 2-year (72% vs 76%, P = .15) survival. Myocardial recovery resulting in device explant was more common among patients with inflammatory CM (5.5% vs 2.3%, P < .001). Patients with inflammatory CM who received durable MCS appear to have a similar survival compared with patients with idiopathic dilated CM despite a higher early adverse event burden. Our findings support the use of durable MCS in an inflammatory CM population.",success
12062749,False,Case Reports;Journal Article,,,,,,,,False,,success
19101239,False,Comparative Study;Journal Article,,,,,,,,True,"Three quinquagenarians who underwent insertion of a left ventricular assist device (LVAD) because of severe heart failure and for whom histologic examination of the left ventricular apical ""core"" (removed to insert the device) showed noncaseating giant cell granulomas typical of sarcoidosis are described. Later, cardiac transplantation showed widespread sarcoid granulomas in the walls of the right and left ventricles and ventricular septum in 2 patients and extensive scarring in the third patient in the absence of coronary narrowing. Previously, 11 patients who underwent cardiac transplantation because of cardiac sarcoidosis had been reported, and in 1 of these patients, diagnosis was also initially made by examination of the left ventricular core excised at the time of insertion of an LVAD. In conclusion, excision of a portion of left ventricular wall to enable insertion of a therapeutic device (LVAD) can also serve as the means of definitive diagnosis of the underlying cardiac condition.",success
29191297,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"This study sought to determine the accuracy of the pre-transplantation clinical diagnosis of heart disease in the United Network for Organ Sharing (UNOS) database. Because survival on the heart transplantation waitlist depends on underlying heart disease, a new allocation system will include the type of heart disease. Accuracy of the pre-transplantation clinical diagnosis and the effect of misclassification are unknown. We included all adults who received transplants at our center between January 2009 to December 2015. We compared the pre-transplantation clinical diagnosis at listing with pathology of the explanted heart and determined the potential effect of misclassification with the proposed allocation system. A total of 334 patients had the following clinical cardiac diagnoses at listing: 148 had dilated cardiomyopathy, 19 had restrictive cardiomyopathy, 103 had ischemic cardiomyopathy, 24 had hypertrophic cardiomyopathy, 11 had valvular disease, 16 had congenital heart disease (CHD), and 13 patients had a diagnosis of ""other."" Pathology of the explanted hearts revealed 82% concordance and 18% discordance (10% coding errors and 8% incorrect diagnosis). The most common incorrect diagnoses were sarcoidosis (66%), arrhythmogenic right ventricular dysplasia (60%), and other causes of predominately right-sided heart failure (33%). Among the misclassified diagnoses, 40% were listed as UNOS status 2, 8% remained at status 2 at transplantation, and only sarcoidosis and CHD were potentially at a disadvantage with the new allocation. There is high concordance between clinical and pathologic diagnosis, except for sarcoidosis and genetic diseases. Few misclassifications result in disadvantages to patients based on the new allocation system, but rare diseases like sarcoidosis remain problematic. To improve the UNOS database and enhance outcome research, pathology of the explanted hearts should be required post-transplantation.",success
33059834,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"Sarcoidosis is a complex disease with heterogeneous clinical presentations that can affect virtually any organ. Although the lung is typically the most common organ involved, combined pulmonary and cardiac sarcoidosis (CS) account for most of the morbidity and mortality associated with this disease. Pulmonary sarcoidosis can be asymptomatic or result in impairment in quality of life and end-stage, severe, and/or life-threatening disease. The latter outcome is seen almost exclusively in those with fibrotic pulmonary sarcoidosis, which accounts for 10% to 20% of pulmonary sarcoidosis patients. CS is problematic to diagnose and may cause significant morbidity and death from heart failure or ventricular arrhythmias. The diagnosis of CS usually requires surrogate cardiac imaging biomarkers, as endomyocardial biopsy has relatively low yield, even with directed electrophysiological mapping. Treatment of CS is often multifactorial, involving a combination of antigranulomatous therapy and pharmacotherapy for cardiac arrhythmias and/or heart failure in addition to device placement and cardiac transplantation.",success
33591816,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"Cardiac sarcoidosis is a component of an often multiorgan granulomatous disease of still uncertain cause. It is being recognized with increasing frequency, mainly as the result of heightened awareness and new diagnostic tests, specifically cardiac magnetic resonance imaging and <sup>18</sup>F-fluorodeoxyglucose positron emission tomography scans. The purpose of this case-based review is to highlight the potentially life-saving importance of making the early diagnosis of cardiac sarcoidosis using these new tools and to provide a framework for the optimal care of patients with this disease. We will review disease mechanisms as currently understood, associated arrhythmias including conduction abnormalities, and atrial and ventricular tachyarrhythmias, guideline-directed diagnostic criteria, screening of patients with extracardiac sarcoidosis, and the use of pacemakers and defibrillators in this setting. Treatment options, including those related to heart failure, and those which may help clarify disease mechanisms are included.",success
19660614,False,Comparative Study;Journal Article,,,,,,,,True,"The clinical diagnosis of cardiac sarcoidosis can be difficult and is largely dependent on newer imaging modalities. A retrospective search of sudden cardiac deaths was performed from a reference laboratory and statewide medical examiner system for a 12-year period. Planimetry was performed on gross photographs of transverse short-axis sections, and the phase of the lesion and the portion of myocardium extent was estimated histologically. Lesions were classified histologically as early (primarily lymphocytic), intermediate (primarily granulomatous), and late (primarily scar). A total of 41 cases were found, including 25 in which the death was ascribed to sarcoidosis of the heart (group 1) and 16 in which sudden death was due to other findings (group 2). No significant differences were found in age or activity at death, although gross scars and epicardial nodules were more frequent in group 1 (p <0.0001). In the hearts with gross scars, the ventricular septum had the largest percentage of involvement (32%) followed by the posterior wall (25%). Histologically, the intermediate phase predominated in group 1, and the late phase predominated in group 2. Approximately 50% of the cases in group 1 had involvement in the right ventricular apex and septum, suggesting a positive yield by biopsy. In conclusion, cardiac sarcoidosis causing sudden death is characterized by extensive active granulomas with a predilection for the subepicardium and ventricular septum.",success
21427276,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cardiac sarcoidosis (CS) and giant cell myocarditis (GCM) may present as high-degree atrioventricular block (AVB), but their proportion of the causal spectrum of AVB is not well-known. We investigated the prevalence of biopsy-verified CS and GCM in young and middle-aged adults undergoing pacemaker (PM) implantation for AVB. We used the PM registry of Helsinki University Central Hospital to identify all patients aged 18 to 55 years who underwent PM implantation for AVB between January 1999 and April 2009 and reviewed their medical records. In total, 133 patients had either second- or third-degree AVB as an indication for PM. Of them, 61 had a known cause for AVB, and they were excluded from further analyses. Among the remaining 72 patients with initially unexplained AVB, biopsy-verified CS or GCM was found in 14 (19%) and 4 (6%) patients, respectively. The majority (16/18, 89%) were women. Among the adult patients aged <55 years, the prevalence of CS and GCM combined was 14% (95% CI, 7.7% to 19.3%) of the whole AVB population and 25% (95% CI, 15% to 35%) of those with an initially unexplained AVB. Over an average of 48 months of follow-up, 7 (39%) of 18 patients with CS or GCM versus 1 of the 54 patients in whom AVB remained idiopathic, experienced either cardiac death, cardiac transplantation, ventricular fibrillation, or treated sustained ventricular tachycardia (P<0.001). CS and GCM explain ≥25% of initially unexplained AVB in young and middle-aged adults. These patients are at high risk for adverse cardiac events.",success
26002389,False,Journal Article;Review,,,,,,,,True,"Myocardial involvement in patients with sarcoidosis can be difficult to diagnose, and requires a high index of suspicion and low threshold for screening. The presentation of cardiac sarcoidosis is variable, and can range from asymptomatic electrocardiographic changes to sudden cardiac death. This review provides an overview of the arrhythmic consequences of cardiac sarcoidosis, with emphasis on the electrophysiologist's role in recognition, diagnostic testing, and management of this rare disease.",success
30586772,False,Journal Article;Practice Guideline,,,,,,,,False,,success
23667912,False,Journal Article,,,,,,,,True,"Cardiac sarcoidosis (CS) is known to be associated with congestive heart failure, conduction disorders, and tachyarrhythmias. Ventricular arrhythmias are the most feared cardiac manifestation because they often are unpredictable, may be the fi rst manifestation of the disease, and may be fatal. The propensity for the development of supraventricular arrhythmias (SVAs) in patients with CS has not been described. The aim of this study was to assess the prevalence as well as the predictors of SVA. We retrospectively investigated 100 patients with biopsy specimen-proven systemic sarcoidosis and evidence of cardiac involvement (defi ned by cardiac biopsy specimen, PET scan, or cardiac MRI). The mean follow-up was 5.8 3.6 years. ECG, Holter monitoring, implantable cardioverter defibrillator interrogations, or electrophysiology studies were used to document SVA. Echocardiographic data, demographics, and extracardiac involvement were recorded, and univariate and Poisson regressions were performed to compare characteristics of patients with and without documented SVA. The prevalence of SVA was 32%, and atrial fibrillation was the most common arrhythmia, comprising 18% of the total burden, followed by atrial tachycardias (7%), atrial fl utter (5%), and other supraventricular tachycardias (2%). Of the patients with SVA, 96% were symptomatic. Left atrial enlargement (LAE) was more frequent in the group with SVA, with an incidence of 267.8 per 1,000 person-years, and it significantly increased the likelihood of SVA on multivariate analysis (risk ratio, 6.12; 95% CI, 2.19-17.11). Diastolic dysfunction, systemic hypertension, and right atrial enlargement were predictors of SVA on univariate analysis. Left ventricular hypertrophy, right ventricular dysfunction, tricuspid valve disease, pulmonary hypertension, and pulmonary sarcoidosis were not associated with SVA on univariate analysis. The study systematically evaluated the frequency of SVA in a large number of patients with CS. SVA in patients with CS is frequent and associated with symptoms. LAE was clearly associated with the development of SVA in this patient population. The extent to which LAE predicts the occurrence of SVA in larger, more diverse CS populations should be evaluated prospectively.",success
32814465,True,"Clinical Trial;Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",NCT01477359,databank,NCT01477359,NCT01477359,NCT01477359,NCT01477359|databank,NCT01477359|databank,True,"Background Recent data have suggested a substantial incidence of atrial arrhythmias (AAs) in cardiac sarcoidosis (CS). Our study aims were to first assess how often AAs are the presenting feature of previously undiagnosed CS. Second, we used prospective follow-up data from implanted devices to investigate AA incidence, burden, predictors, and response to immunosuppression. Methods and Results This project is a substudy of the CHASM-CS (Cardiac Sarcoidosis Multicenter Prospective Cohort Study; NCT01477359). Inclusion criteria were presentation with clinically manifest cardiac sarcoidosis, treatment-naive status, and implanted with a device that reported accurate AA burden. Data were collected at each device interrogation visit for all patients and all potential episodes of AA were adjudicated. For each intervisit period, the total AA burden was obtained. A total of 33 patients met the inclusion criteria (aged 56.1±7.7 years, 45.5% women). Only 1 patient had important AAs as a part of the initial CS presentation. During a median follow-up of 49.1 months, 11 of 33 patients (33.3%) had device-detected AAs, and only 2 (6.1%) had a clinically significant AA burden. Both patients had reduced burden after CS was successfully treated and there was no residual fluorodeoxyglucose uptake on positron emission tomography scan. Conclusions First, we found that AAs are a rare presenting feature of clinically manifest cardiac sarcoidosis. Second, AAs occurred in a minority of patients at follow-up; the burden was very low in most patients. Only 2 patients had clinically significant AA burden, and both had a reduction after CS was treated. Registration URL: https://www.clini​caltr​ials.gov; unique identifier NCT01477359.",success
36075623,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"In cardiac sarcoidosis (CS), the risk and predictors of new-onset atrial fibrillation (AF) are poorly known. The authors evaluated the incidence and characteristics of AF in newly diagnosed CS. The authors studied 118 patients (78 women, mean age 50 years) with AF-naive CS having undergone cardiac <sup>18</sup>F-fluorodexoyglucose positron emission tomography (<sup>18</sup>F-FDG PET) at the time of diagnosis. Details of patient characteristics and medical or device therapy were collected from hospital charts. The PET scans were re-analyzed for presence of atrial and ventricular inflammation, and coincident cardiac magnetic resonance (CMR) studies and single-photon emission computed tomography (SPECT) perfusions were analyzed for cardiac structure and function, including the presence and extent of myocardial scarring. Detection of AF was based on interrogation of intracardiac devices and on ambulatory or 12-lead electrocardiograms. Altogether 34 patients (29%) suffered paroxysms of AF during follow-up (median, 3 years) with persistent AF developing in 7 patients and permanent AF in 4. The estimated 5-year incidence of AF was 55% (95% CI: 34%-72%) in the 39 patients with atrial <sup>18</sup>F-FDG uptake at the time of diagnosis vs 18% (95% CI: 10%-28%) in the 79 patients without atrial uptake (P < 0.001). In cause-specific Cox regression analysis, atrial uptake was an independent predictor of AF (P < 0.001) with HR of 6.01 (95% CI: 2.64-13.66). Other independent predictors were an increased left atrial maximum volume (P < 0.01) and history of sleep apnea (P < 0.01). Ventricular involvement by PET, SPECT, or CMR was nonpredictive. Symptoms of AF prompted electrical cardioversion in 12 patients (35%). Three of the 34 patients (9%) experiencing AF suffered a stroke versus none of those remaining free of AF. In newly diagnosed CS, future AF is relatively common and associated with atrial inflammation and enlargement on multimodality cardiac imaging.",success
24698290,False,Evaluation Study;Journal Article,,,,,,,,True,"We previously reported on the incidence and clinical implications of supraventricular arrhythmia in patients with cardiac sarcoidosis (CS). The role of catheter ablation for the management of atrial arrhythmia (AA) in this patient population is unknown. One hundred consecutive patients with CS were monitored for the incidence of supraventricular arrhythmias. Those with persistent symptoms despite optimal medical therapy proceeded to catheter ablation. Following ablation, all patients were followed serially with Holter monitoring or device interrogation. Thirty-two (32%) patients had symptomatic supraventricular arrhythmias. Nine (28%) patients had symptomatic AA requiring catheter ablation for clinical indications. Mean age was 55 ± 11.6 years. Five (56%) patients had atrial fibrillation (AF), of whom 2 also had cavotricuspid isthmus ablation. Four patients had isolated atrial flutter: 2 patients with left atrial flutter, and 2 patients with cavotricuspid flutter. All other arrhythmias were ablated in the left atrium. Mean duration of follow-up was 1.8 ± 1.9 years. One patient with atypical atrial flutter, and one patient with AF have had recurrence; the remaining patients remain in sinus rhythm. Our study suggests that AA in CS is frequently left atrial in origin. Catheter ablation appears to be effective and safe for the maintenance of sinus rhythm in patients with CS.",success
18456644,False,"Case Reports;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Sarcoidosis is a multisystem, granulomatous disease with occasional cardiac manifestations. The clinical course of patients with ventricular tachyarrhythmias as a primary presentation of sarcoidosis is mostly unknown. We describe nine patients (four males and five females) in whom sarcoidosis manifested as ventricular tachycardia (VT). The age of the patients was 53 +/- 10 years (range 33-68). The disease was diagnosed by endomyocardial biopsy in eight patients and by lymph node biopsy in one patient. The presenting arrhythmia varied from non-sustained VT to incessant VT and ventricular fibrillation. All patients received implantable cardioverter defibrillator (ICD) and anti-arrhythmic medication. High-dose steroid treatment was used in eight cases. During the follow-up (50 +/- 34 months), five patients underwent appropriate ICD therapies and non-sustained VT episodes were detected in four patients. Two patients developed incessant VT, which was treated by catheter ablation. One patient was referred for heart transplantation. Our data indicate that sarcoidosis can manifest as VT without any detectable systemic findings. This makes sarcoidosis an important diagnostic consideration in patients with VT of unknown origin. Arrhythmia control in cardiac sarcoidosis is difficult, and all modern treatments including high-dose steroids, anti-arrhythmic drugs, ICD, and catheter ablation are needed to suppress the arrhythmias.",success
24837644,False,Journal Article;Retracted Publication,,,,,,,,True,"Fatal arrhythmia is commonly observed in cardiac sarcoidosis, but clinical effects of a systematic treatment approach are still uncertain. This study sought to describe both clinical and electrophysiological characteristics and outcomes of systematic treatment approach to ventricular tachycardia (VT) associated with cardiac sarcoidosis. We enrolled 37 consecutive patients (11 men; age, 56±11 years) with a diagnosis of sustained VT associated with cardiac sarcoidosis. Clinical effects of a systematic treatment approach including medical therapy (both steroid and antiarrhythmic agents), in association with radiofrequency catheter ablation, were evaluated. All patients received antiarrhythmic agents, and 34 received steroid therapy. During a 39-month follow-up, 23 (62%) patients were free from any VT episodes with medical therapy. Multivariable Cox regression analyses revealed that the absence of gallium-67 myocardial uptake was an independent predictor for VT recurrence (hazard ratio, 7.51; 95% confidence interval, 1.65-34.26; P<0.01). Fourteen patients who experienced VT recurrences even while on drug therapy underwent radiofrequency catheter ablation. Electrophysiological study revealed that the mechanisms of VTs could be classified into 2 subgroups: Purkinje-related or scar-related VT. The QRS duration of VT was narrower in Purkinje-related than in scar-related VTs (157±23 versus 183±22 ms; P<0.05). After a 33-month follow-up subsequent to the radiofrequency catheter ablation, 6 of 14 patients experienced VT recurrence. The number of VTs sustained during electrophysiological study was higher in the patients with VT recurrence than in those without (3.7±1.4 versus 1.9±0.8; P<0.01). A systematic treatment approach to cardiac sarcoidosis with VT successfully suppressed VT recurrences in the majority of patients studied.",success
34787643,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Ventricular tachycardia (VT) is associated with high mortality in patients with cardiac sarcoidosis (CS), and medical management of CS-associated VT is limited by high failure rates. The role of catheter ablation has been investigated in small, single-center studies. To investigate outcomes associated with VT ablation in patients with CS. This cohort study from the Cardiac Sarcoidosis Consortium registry (2003-2019) included 16 tertiary referral centers in the US, Europe, and Asia. A total of 158 consecutive patients with CS and VT were included (33% female; mean [SD] age, 52 [11] years; 53% with ejection fraction [EF] <50%). Catheter ablation of CS-associated VT and, as appropriate, medical treatment. Immediate and short-term outcomes included procedural success, elimination of VT storm, and reduction in defibrillator shocks. The primary long-term outcome was the composite of VT recurrence, heart transplant (HT), or death. Complete procedural success (no inducible VT postablation) was achieved in 85 patients (54%). Sixty-five patients (41%) had preablation VT storm that did not recur postablation in 53 (82%). Defibrillator shocks were significantly reduced from a median (IQR) of 2 (1-5) to 0 (0-0) in the 30 days before and after ablation (P < .001). During median (IQR) follow-up of 2.5 (1.1-4.9) years, 73 patients (46%) experienced VT recurrence and 81 (51%) experienced the composite primary outcome. One- and 2-year rates of survival free of VT recurrence, HT, or death were 60% and 52%, respectively. EF less than 50% and myocardial inflammation on preprocedural 18F-fluorodeoxyglucose positron emission tomography were significantly associated with adverse prognosis in multivariable analysis for the primary outcome (HR, 2.24; 95% CI, 1.37-3.64; P = .001 and HR, 2.93; 95% CI, 1.31-6.55; P = .009, respectively). History of hypertension was associated with a favorable long-term outcome (adjusted HR, 0.51; 95% CI, 0.28-0.92; P = .02). In this observational study of selected patients with CS and VT, catheter ablation was associated with reductions in defibrillator shocks and recurrent VT storm. Preablation LV dysfunction and myocardial inflammation were associated with adverse long-term prognosis. These data support the role of catheter ablation in conjunction with medical therapy in the management of CS-associated VT.",success
30818090,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Ventricular arrhythmias (VAs) in cardiac sarcoidosis (CS) are frequently refractory to both antiarrhythmic drug (AAD) therapy and catheter ablation (CA). Cardiac sympathetic denervation (CSD) has been shown to reduce VA burden and implantable cardioverter-defibrillator (ICD) shocks in patients with nonischemic cardiomyopathy. We aimed to report our center's preliminary experience with CSD in patients with known or presumed CS and refractory VAs. Patients with CS and refractory VAs who underwent CSD at our institution were included. Patient characteristics, procedural outcomes, and number of arrhythmic events including ICD shocks pre- and post-CSD are reported. Five patients with CS (mean age 53 ± 11 years; 2 men [40%]; mean left ventricular ejection fraction 38% ± 11%) underwent CSD for VA refractory to AAD therapy and CA. Four of 5 patients underwent bilateral CSD; 1 patient underwent right-sided sympathectomy only because of poor intraoperative visualization on the left. Procedural complications included hemothorax in 1 patient and azygous vein injury in 1 patient. The median number of ICD shocks in the 6 months pre-CSD was 5. During a median follow-up of 26 months (range 5-29 months), the median number of ICD shocks post-CSD was 0; 1 patient had sustained VA that was below the threshold for device therapy, and 1 patient had symptomatic premature ventricular contractions; both underwent repeat CA. In addition, 1 patient required cardiac transplantation for progressive heart failure. CSD may be a feasible therapeutic adjunct for patients with CS and VA refractory to AAD therapy and CA.",success
29084731,False,Journal Article;Practice Guideline,,,,,,,,False,,success
32402485,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"This study investigated the association between left ventricular ejection fraction (LVEF) and the risk of ventricular arrhythmias (VA), heart transplantation, and death in cardiac sarcoidosis (CS). We identified 110 CS patients meeting 2014 Heart Rhythm Society (HRS) diagnostic criteria with baseline LVEF <35% (n = 32) or ≥35% (n = 78). The primary end point was sustained VA or sudden cardiac death (SCD), and secondary end points included risk of heart transplantation, death, or a composite. Logistic regression determined risk factors for VA/SCD, and Cox proportional hazards regression analysis was performed for secondary end points. Receiver operating curve analysis determined the best discrimination point of LVEF for each end point; sensitivity analyses evaluated the effects of higher LVEF on each end point. Over a follow-up of 2.6 (range 1.0 to 5.8) years, 49 (44.5%) CS patients experienced VA/SCD, including 19 of 32 (59.4%) with LVEF <35%, and 30 of 78 (38.5%) with LVEF ≥35%. After adjustment, LVEF <35% was not significantly associated with an increased risk of VA/SCD compared with LVEF ≥35% (odds ratio 1.3, 95% confidence intervals 0.5 to 3.7). Although LVEF <35% was associated with an increased risk of heart transplantation and death (28.1% vs 12.8%, p = 0.05), this was not significant after adjustment (hazard ratio 1.7, 95% confidence intervals 0.5 to 9.0, p = 0.53). In conclusion, patients with CS experience high rates of VA, SCD, and heart transplantation, even when LVEF is mildly impaired or normal. Patients with LVEF <35% are at particularly elevated risk of VA/SCD. Our findings highlight the imperative to investigate arrhythmia risk in all patients with CS, even in the setting of an otherwise reassuring LVEF.",success
31431050,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Implantable cardioverter-defibrillators are used to prevent sudden cardiac death in patients with cardiac sarcoidosis. The most recent recommendations for implantable cardioverter-defibrillator implantation in these patients are in the 2017 American Heart Association/American College of Cardiology/Heart Rhythm Society Guideline for Management of Patients With Ventricular Arrhythmias and the Prevention of Sudden Cardiac Death. These recommendations, based on observational studies or expert opinion, have not been assessed. We aimed to assess them. We performed a large retrospective cohort study of patients with biopsy-proven sarcoidosis and known or suspected cardiac sarcoidosis that underwent cardiovascular magnetic resonance imaging. Patients were followed for a composite end point of significant ventricular arrhythmia or sudden cardiac death. The discriminatory performance of the Guideline recommendations was tested using time-dependent receiver operating characteristic analyses. The optimal cutoff for the extent of late gadolinium enhancement predictive of the composite end point was determined using the Youden index. In 290 patients, the class I and IIa recommendations identified all patients who experienced the composite end point during a median follow-up of 3.0 years. Patients meeting class I recommendations had a significantly higher incidence of the composite end point than those meeting class IIa recommendations. Left ventricular ejection fraction (LVEF) >35% with >5.7% late gadolinium enhancement on cardiovascular magnetic resonance imaging was as sensitive as and significantly more specific than LVEF >35% with any late gadolinium enhancement. Patients meeting 2 class IIa recommendations, LVEF >35% with the need for a permanent pacemaker and LVEF >35% with late gadolinium enhancement >5.7%, had high annualized event rates. Excluding 2 class IIa recommendations, LVEF >35% with syncope and LVEF >35% with inducible ventricular arrhythmia, resulted in improved discrimination for the composite end point. We assessed the Guideline recommendations for implantable cardioverter-defibrillator implantation in patients with known or suspected cardiac sarcoidosis and identified topics for future research.",success
34864075,False,Journal Article;Meta-Analysis;Systematic Review,,,,,,,,True,"The utility of an electrophysiologic study (EPS) in the risk stratification of cardiac sarcoidosis (CS) patients is not clear. We conducted a systemic review and meta-analysis to evaluate the utility of EPS in the risk stratification of CS patients. We searched PubMed, Embase, and Scopus databases from their inception to 12/4/2020 with search terms ""Cardiac sarcoidosis"" And ""Electrophysiological studies OR ablation"". The first and second authors reviewed all the studies. We extracted the data of positive and negative EPS, and outcomes defined as ventricular arrhythmias, implantable cardioverter defibrillator therapy, death, left ventricular assist device placement, or heart transplantation. Risk of bias assessment was done by the Quality Assessment of Diagnostic Accuracy Studies-2 tool. Subgroup analysis of patients with left ventricular ejection fraction (LVEF) >35%, and probable CS, no prior ventricular tachycardia (VT) and LVEF >35% were performed. We found 544 articles after removing duplicates. A total of 52 full articles were reviewed, and eight studies were included in the meta-analysis. The pooled sensitivity and specificity (95% confidence interval) of EPS in predicting clinical outcomes were 0.70 (0.51-0.85) and 0.93 (0.85-0.97), respectively. Subgroup analysis of patients with LVEF >35% resulted in pooled sensitivity of 0.63 (0.29-0.88) and pooled specificity of 0.97 (0.92-0.99), and subgroup analysis of patients with probable CS, no prior VT, and LVEF >35% resulted in pooled sensitivity of 0.71 (0.33-0.93) and pooled specificity of 0.96 (0.88-0.99) in predicting adverse clinical outcomes. EPS is an effective risk stratification tool in patients with CS across all subgroups with high sensitivity and specificity.",success
32290254,False,Journal Article;Review,,,,,,,,True,"Sarcoidosis is a multisystem granulomatous disease with nonspecific clinical manifestations that commonly affects the pulmonary system and other organs including the eyes, skin, liver, spleen, and lymph nodes. Sarcoidosis usually presents with persistent dry cough, eye and skin manifestations, weight loss, fatigue, night sweats, and erythema nodosum. Sarcoidosis is not influenced by sex or age, although it is more common in adults (< 50 years) of African-American or Scandinavians decent. Diagnosis can be difficult because of nonspecific symptoms and can only be verified following histopathological examination. Various factors, including infection, genetic predisposition, and environmental factors, are involved in the pathology of sarcoidosis. Exposures to insecticides, herbicides, bioaerosols, and agricultural employment are also associated with an increased risk for sarcoidosis. Due to its unknown etiology, early diagnosis and detection are difficult; however, the advent of advanced technologies, such as endobronchial ultrasound-guided biopsy, high-resolution computed tomography, magnetic resonance imaging, and 18F-fluorodeoxyglucose positron emission tomography has improved our ability to reliably diagnose this condition and accurately forecast its prognosis. This review discusses the causes and clinical features of sarcoidosis, and the improvements made in its prognosis, therapeutic management, and the recent discovery of potential biomarkers associated with the diagnostic assay used for sarcoidosis confirmation.",success
27828754,False,Editorial,,,,,,,,False,,success
12919835,False,Journal Article,,,,,,,,True,"It is well established that sarcoidosis is a multisystem disorder of unknown cause(s). Practically no organ is immune to sarcoidosis. It subsides in most cases, but it may worsen and become chronic in others. Pulmonary problems may persist, but also devastating extrapulmonary complications may become apparent. Appropriate management of sarcoidosis is mandatory as it predominantly affects fairly young adults. This requires the attention of pulmonologists as well as specialists from other medical disciplines. Accordingly, when treating sarcoidosis patients, a multidisciplinary approach is recommended that focuses attention on somatic as well as psychosocial aspects of this erratic disorder. Specialists from all participating medical disciplines-including respiratory diseases-may benefit from a multidisciplinary approach and be stimulated to enhance their professional interest and knowledge of sarcoidosis. The benefit of such an approach should be explored in the near future.",success
34782085,False,Journal Article;Review,,,,,,,,True,"Sarcoidosis has a multitude of manifestations and affects the human body widely. Pulmonary complaints are most common; however, cardiac, optic, and neurologic manifestations carry high mortality and morbidity. Acute presentations in the emergency room can cause life-altering effects if not appropriately diagnosed and treated. Generally, less severe cases of sarcoidosis have a favorable prognosis and can be treated with steroid therapy. Resistant and more severe cases of the disease carry high mortality and morbidity. It is incredibly important to arrange specialty follow-up for these patients when needed. This review focuses on the acute presentations of sarcoidosis.",success
26130811,False,Journal Article;Review,,,,,,,,True,"Cardiac sarcoidosis is one of the most serious and unpredictable aspects of this disease state. Heart involvement frequently presents with arrhythmias or conduction disease, although myocardial infiltration resulting in congestive heart failure may also occur. The prognosis in cardiac sarcoidosis is highly variable, which relates to the heterogeneous nature of heart involvement and marked differences between racial groups. Electrocardiography and echocardiography often provide the first clue to the diagnosis, but advanced imaging studies using positron emission tomography and MRI, in combination with nuclear isotope perfusion scanning are now essential to the diagnosis and management of this condition. The identification of clinically occult cardiac sarcoidosis and the management of isolated and/or asymptomatic heart involvement remain both challenging and contentious. Corticosteroids remain the first treatment choice with the later substitution of immunosuppressive and steroid-sparing therapies. Heart transplantation is an unusual outcome, but when performed, the results are comparable or better than heart transplantation for other disease states. We review the epidemiology, developments in diagnostic techniques and the management of cardiac sarcoidosis.",success
32024123,False,Journal Article;Review,,,,,,,,True,"Sarcoidosis is a multisystem granulomatous disease, associated with significant morbidity and impaired quality of life. Treatment is aimed at recovering organ function, reducing symptom burden and improving quality of life. Because of the heterogeneity and variable disease course, a comprehensive, multidisciplinary approach to care is needed. Comprehensive care includes not only pharmacological interventions, but also supportive measures aimed at relieving symptoms and improving quality of life. The purpose of this review is to summarize the most recent knowledge regarding different aspects of care and propose a structured approach to sarcoidosis management.",success
33666856,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"This review highlights variability in prescribing of nonpharmacologic heart failure with reduced ejection fraction (HFrEF) therapies by race, ethnicity, and gender. The review also explores the evidence underlying these inequalities as well as potential mitigation strategies. There have been major advances in HF therapies that have led to improved overall survival of HF patients. However, racial and ethnic groups of color and women have not received equitable access to these therapies. Patients of color and women are less likely to receive nonpharmacologic therapies for HFrEF than White patients and men. Therapies including exercise rehabilitation, percutaneous transcatheter mitral valve repair, cardiac resynchronization therapy, heart transplant, and ventricular assist devices all have proven efficacy in patients of color and women but remain underprescribed. Outcomes with most nonpharmacologic therapy are similar or better among patients of color and women than White patients and men. System-level changes are urgently needed to achieve equity in access to nonpharmacologic HFrEF therapies by race, ethnicity, and gender.",success
34431374,False,"Editorial;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Comment",,,,,,,,False,,success
32787445,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Although care of patients with heart failure (HF) has improved in the past decade, important disparities in HF outcomes persist based on race/ethnicity. Age-adjusted HF-related cardiovascular disease death rates are higher for Black patients, particularly among young Black men and women whose rates of death are 2.6- and 2.97-fold higher, respectively, than White men and women. Similarly, the rate of HF hospitalization for Black men and women is nearly 2.5-fold higher when compared with Whites, with costs that are significantly higher in the first year after HF hospitalization. While the relative rate of HF hospitalization has improved for other race/ethnic minorities, the disparity in HF hospitalization between Black and White patients has not decreased during the last decade. Although access to care and socioeconomic status have been traditional explanations for the observed racial disparities in HF outcomes, contemporary data suggest that novel factors including genetic susceptibility as well as social determinants of health and implicit bias may play a larger role in health outcomes than previously appreciated. The purpose of this review is to describe the complex interplay of factors that influence racial disparities in HF incidence, prevalence, and disease severity, with a highlight on evolving knowledge that will impact the clinical care and address future research needs to improve HF disparities in Blacks.",success
32446323,True,Comparative Study;Journal Article;Randomized Controlled Trial,,,,,,,,True,"Three drug classes (mineralocorticoid receptor antagonists [MRAs], angiotensin receptor-neprilysin inhibitors [ARNIs], and sodium/glucose cotransporter 2 [SGLT2] inhibitors) reduce mortality in patients with heart failure with reduced ejection fraction (HFrEF) beyond conventional therapy consisting of angiotensin-converting enzyme (ACE) inhibitors or angiotensin receptor blockers (ARBs) and β blockers. Each class was previously studied with different background therapies and the expected treatment benefits with their combined use are not known. Here, we used data from three previously reported randomised controlled trials to estimate lifetime gains in event-free survival and overall survival with comprehensive therapy versus conventional therapy in patients with chronic HFrEF. In this cross-trial analysis, we estimated treatment effects of comprehensive disease-modifying pharmacological therapy (ARNI, β blocker, MRA, and SGLT2 inhibitor) versus conventional therapy (ACE inhibitor or ARB and β blocker) in patients with chronic HFrEF by making indirect comparisons of three pivotal trials, EMPHASIS-HF (n=2737), PARADIGM-HF (n=8399), and DAPA-HF (n=4744). Our primary endpoint was a composite of cardiovascular death or first hospital admission for heart failure; we also assessed these endpoints individually and assessed all-cause mortality. Assuming these relative treatment effects are consistent over time, we then projected incremental long-term gains in event-free survival and overall survival with comprehensive disease-modifying therapy in the control group of the EMPHASIS-HF trial (ACE inhibitor or ARB and β blocker). The hazard ratio (HR) for the imputed aggregate treatment effects of comprehensive disease-modifying therapy versus conventional therapy on the primary endpoint of cardiovascular death or hospital admission for heart failure was 0·38 (95% CI 0·30-0·47). HRs were also favourable for cardiovascular death alone (HR 0·50 [95% CI 0·37-0·67]), hospital admission for heart failure alone (0·32 [0·24-0·43]), and all-cause mortality (0·53 [0·40-0·70]). Treatment with comprehensive disease-modifying pharmacological therapy was estimated to afford 2·7 additional years (for an 80-year-old) to 8·3 additional years (for a 55-year-old) free from cardiovascular death or first hospital admission for heart failure and 1·4 additional years (for an 80-year-old) to 6·3 additional years (for a 55-year-old) of survival compared with conventional therapy. Among patients with HFrEF, the anticipated aggregate treatment effects of early comprehensive disease-modifying pharmacological therapy are substantial and support the combination use of an ARNI, β blocker, MRA, and SGLT2 inhibitor as a new therapeutic standard. None.",success
33736822,False,Editorial;Comment,,,,,,,,False,,success
33741769,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"This review discusses the current state of racial and ethnic inequities in heart failure burden, outcomes, and management. This review also frames considerations for bridging disparities to optimize quality heart failure care across diverse communities. Treatment options for heart failure have diversified and overall heart failure survival has improved with the advent of effective pharmacologic and nonpharmacologic therapies. With increased recognition, some racial/ethnic disparity gaps have narrowed whereas others in heart failure outcomes, utilization of therapies, and advanced therapy access persist or worsen. Racial and ethnic minorities have the highest incidence, prevalence, and hospitalization rates from heart failure. In spite of improved therapies and overall survival, the mortality disparity gap in African American patients has widened over time. Racial/ethnic inequities in access to cardiovascular care, utilization of efficacious guideline-directed heart failure therapies, and allocation of advanced therapies may contribute to disparate outcomes. Strategic and earnest interventions considering social and structural determinants of health are critically needed to bridge racial/ethnic disparities, increase dissemination, and implementation of preventive and therapeutic measures, and collectively improve the health and longevity of patients with heart failure.",success
29724363,True,"Journal Article;Multicenter Study;Observational Study;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"This study sought to determine whether the likelihood of receiving primary intensive care unit (ICU) care by a cardiologist versus a noncardiologist was greater for Caucasians than for African Americans admitted to an ICU for heart failure (HF). The authors further evaluated whether primary ICU care by a cardiologist is associated with higher in-hospital survival, irrespective of race. Increasing data demonstrate an association between better HF outcomes and care by a cardiologist. It is unclear if previously noted racial differences in cardiology care persist in an ICU setting. Using the Premier database, adult patients admitted to an ICU with a primary discharge diagnosis of HF from 2010 to 2014 were included. Hierarchical logistic regression models were used to determine the association between race and primary ICU care by a cardiologist, adjusting for patient and hospital variables. Cox regression with inverse probability weighting was used to assess the association between cardiology care and in-hospital mortality. Among 104,835 patients (80.3% Caucasians, 19.7% African Americans), Caucasians had higher odds of care by a cardiologist than African Americans (adjusted odds ratio: 1.42; 95% confidence interval: 1.34 to 1.51). Compared with a noncardiologist, primary ICU care by a cardiologist was associated with higher in-hospital survival (adjusted hazard ratio: 1.20, 95% confidence interval: 1.11 to 1.28). The higher likelihood of survival did not differ by patient race (interaction p = 0.32). Among patients admitted to an ICU for HF, African Americans were less likely than Caucasians to receive primary care by a cardiologist. Primary care by a cardiologist was associated with higher survival for both Caucasians and African Americans.",success
31658831,False,"Comparative Study;Journal Article;Observational Study;Research Support, N.I.H., Extramural",,,,,,,,True,"Racial inequities for patients with heart failure (HF) have been widely documented. HF patients who receive cardiology care during a hospital admission have better outcomes. It is unknown whether there are differences in admission to a cardiology or general medicine service by race. This study examined the relationship between race and admission service, and its effect on 30-day readmission and mortality Methods: We performed a retrospective cohort study from September 2008 to November 2017 at a single large urban academic referral center of all patients self-referred to the emergency department and admitted to either the cardiology or general medicine service with a principal diagnosis of HF, who self-identified as white, black, or Latinx. We used multivariable generalized estimating equation models to assess the relationship between race and admission to the cardiology service. We used Cox regression to assess the association between race, admission service, and 30-day readmission and mortality. Among 1967 unique patients (66.7% white, 23.6% black, and 9.7% Latinx), black and Latinx patients had lower rates of admission to the cardiology service than white patients (adjusted rate ratio, 0.91; 95% CI, 0.84-0.98, for black; adjusted rate ratio, 0.83; 95% CI, 0.72-0.97 for Latinx). Female sex and age >75 years were also independently associated with lower rates of admission to the cardiology service. Admission to the cardiology service was independently associated with decreased readmission within 30 days, independent of race. Black and Latinx patients were less likely to be admitted to cardiology for HF care. This inequity may, in part, drive racial inequities in HF outcomes.",success
22869644,False,"Journal Article;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"When fully implemented, the Affordable Care Act will expand the number of people with health insurance. This raises questions about the capacity of the health care workforce to meet increased demand. I used data on office-based physicians from the 2011 National Ambulatory Medical Care Survey Electronic Medical Records Supplement to summarize the percentage of physicians currently accepting any new patients. Although 96 percent of physicians accepted new patients in 2011, rates varied by payment source: 31 percent of physicians were unwilling to accept any new Medicaid patients; 17 percent would not accept new Medicare patients; and 18 percent of physicians would not accept new privately insured patients. Physicians in smaller practices and those in metropolitan areas were less likely than others to accept new Medicaid patients. Higher state Medicaid-to-Medicare fee ratios were correlated with greater acceptance of new Medicaid patients. The findings serve as a useful baseline from which to measure the anticipated impact of Affordable Care Act provisions that could boost Medicaid payment rates to primary care physicians in some states while increasing the number of people with health care coverage.",success
30947608,False,Journal Article;Meta-Analysis;Systematic Review,,,,,,,,True,"Medicaid patients are known to have reduced access to care compared with privately insured patients; however, quantifying this disparity with large controlled studies remains a challenge. This meta-analysis evaluates the disparity in health services accessibility of appointments between Medicaid and privately insured patients through audit studies of health care appointments and schedules. Audit studies evaluating different types of outpatient physician practices were selected. Studies were categorized based on the characteristics of the simulated patient scenario. The relative risk of appointment availability was calculated for all different types of audit scenario characteristics. As a secondary analysis, appointment availability was compared pre- versus post-Medicaid expansion. Overall, 34 audit studies were identified, which demonstrated that Medicaid insurance is associated with a 1.6-fold lower likelihood in successfully scheduling a primary care appointment and a 3.3-fold lower likelihood in successfully scheduling a specialty appointment when compared with private insurance. In this first meta-analysis comparing appointment availability between Medicaid and privately insured patients, we demonstrate Medicaid patients have greater difficulty obtaining appointments compared with privately insured patients across a variety of medical scenarios.",success
33459595,False,Journal Article,,,,,,,,True,"Biomedical science and federal funding for scientific research are not immune to the systemic racism that pervades American society. A groundbreaking analysis of NIH grant success revealed in 2011 that grant applications submitted to the National Institutes of Health in the US by African-American or Black Principal Investigators (PIs) are less likely to be funded than applications submitted by white PIs, and efforts to narrow this funding gap have not been successful. A follow-up study in 2019 showed that this has not changed. Here, we review those original reports, as well as the response of the NIH to these issues, which we argue has been inadequate. We also make recommendations on how the NIH can address racial disparities in grant funding and call on scientists to advocate for equity in federal grant funding.",success
31610713,False,"Congress;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Disparities in health outcomes for heart, lung, blood, and sleep-related health conditions are pervasive in the United States, with an unequal burden experienced among structurally disadvantaged populations. One reason for this disparity is that despite the existence of effective interventions that promote health equity, few have been translated and implemented consistently in the healthcare system. To achieve health equity, there is a dire need to implement and disseminate effective evidence-based interventions that account for the complex and multilayered social determinants of health among marginalized groups across healthcare settings. To that end, the National Heart, Lung, and Blood Institute's Center for Translation Research and Implementation Science invited early stage investigators to participate in the inaugural Saunders-Watkins Leadership Workshop in May of 2018 at the National Institutes of Health. The goals of the workshop were to: (1) present an overview of health equity research, including areas which require ongoing investigation; (2) review how the fields of health equity and implementation science are related; (3) demonstrate how implementation science could be utilized to advance health equity; and (4) foster early stage investigator career success in heart, lung, blood, and sleep-related research. Herein, we highlight key themes from the 2-day workshop and offer recommendations for the future direction of health equity and implementation science research in the context of heart, lung, blood, and sleep-related health conditions.",success
35789019,False,Journal Article;Review,,,,,,,,True,"Treatment options for patients with heart failure have improved rapidly over the last few decades. Data from large scale clinical trials demonstrate that medical and device therapies can improve quality of life, reduce hospitalizations for acute heart failure, and reduce mortality. However, the use of many of these therapies in routine practice is remarkably low. There are many reasons for suboptimal implementation of evidence-based therapies for heart failure, and we believe addressing the large gap between what can be accomplished in clinical trials versus routine practice is a critical and urgent public health issue. In this review, we outline reasons for this implementation gap and review recent studies attempting to address this issue. We also provide recommendations for future interventions and areas of clinical investigation to improve implementation for patients with heart failure.",success
33740999,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"There is growing urgency to tackle issues of equity and justice in the USA and worldwide. Health equity, a framing that moves away from a deficit mindset of what society is doing poorly (disparities) to one that is positive about what society can achieve, is becoming more prominent in health research that uses implementation science approaches. Equity begins with justice-health differences often reflect societal injustices. Applying the perspectives and tools of implementation science has potential for immediate impact to improve health equity. We propose a vision and set of action steps for making health equity a more prominent and central aim of implementation science, thus committing to conduct implementation science through equity-focused principles to achieve this vision in U.S. research and practice. We identify and discuss challenges in current health disparities approaches that do not fully consider social determinants. Implementation research challenges are outlined in three areas: limitations of the evidence base, underdeveloped measures and methods, and inadequate attention to context. To address these challenges, we offer recommendations that seek to (1) link social determinants with health outcomes, (2) build equity into all policies, (3) use equity-relevant metrics, (4) study what is already happening, (5) integrate equity into implementation models, (6) design and tailor implementation strategies, (7) connect to systems and sectors outside of health, (8) engage organizations in internal and external equity efforts, (9) build capacity for equity in implementation science, and (10) focus on equity in dissemination efforts. Every project in implementation science should include an equity focus. For some studies, equity is the main goal of the project and a central feature of all aspects of the project. In other studies, equity is part of a project but not the singular focus. In these studies, we should, at a minimum, ensure that we ""leave no one behind"" and that existing disparities are not widened. With a stronger commitment to health equity from funders, researchers, practitioners, advocates, evaluators, and policy makers, we can harvest the rewards of the resources being invested in health-related research to eliminate disparities, resulting in health equity.",success
36214131,False,Journal Article;Review,,,,,,,,True,"Reducing cardiovascular disease disparities will require a concerted, focused effort to better adopt evidence-based interventions, in particular, those that address social determinants of health, in historically marginalized populations (ie, communities excluded on the basis of social identifiers like race, ethnicity, and social class and subject to inequitable distribution of social, economic, physical, and psychological resources). Implementation science is centered around stakeholder engagement and, by virtue of its reliance on theoretical frameworks, is custom built for addressing research-to-practice gaps. However, little guidance exists for how best to leverage implementation science to promote cardiovascular health equity. This American Heart Association scientific statement was commissioned to define implementation science with a cardiovascular health equity lens and to evaluate implementation research that targets cardiovascular inequities. We provide a 4-step roadmap and checklist with critical equity considerations for selecting/adapting evidence-based practices, assessing barriers and facilitators to implementation, selecting/using/adapting implementation strategies, and evaluating implementation success. Informed by our roadmap, we examine several organizational, community, policy, and multisetting interventions and implementation strategies developed to reduce cardiovascular disparities. We highlight gaps in implementation science research to date aimed at achieving cardiovascular health equity, including lack of stakeholder engagement, rigorous mixed methods, and equity-informed theoretical frameworks. We provide several key suggestions, including the need for improved conceptualization and inclusion of social and structural determinants of health in implementation science, and the use of adaptive, hybrid effectiveness designs. In addition, we call for more rigorous examination of multilevel interventions and implementation strategies with the greatest potential for reducing both primary and secondary cardiovascular disparities.",success
37219339,False,"Review;Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"To provide guiding principles and recommendations for how approaches from the field of dissemination and implementation (D&I) science can advance healthcare equity. This article, part of a special issue sponsored by the Agency for Healthcare Research and Quality (AHRQ), is based on an outline drafted to support proceedings of the 2022 AHRQ Health Equity Summit and further revised to reflect input from Summit attendees. This is a narrative review of the current and potential applications of D&I approaches for understanding and advancing healthcare equity, followed by discussion and feedback with Summit attendees. We identified major themes in narrative and systematic reviews related to D&I science, healthcare equity, and their intersections. Based on our expertise, and supported by synthesis of published studies, we propose recommendations for how D&I science is relevant for advancing healthcare equity. We used iterative discussions internally and at the Summit to refine preliminary findings and recommendations. We identified four guiding principles and three D&I science domains with strong promise for accelerating progress toward healthcare equity. We present eight recommendations and more than 60 opportunities for action by practitioners, healthcare leaders, policy makers, and researchers. Promising areas for D&I science to impact healthcare equity include the following: attention to equity in the development and delivery of evidence-based interventions; the science of adaptation; de-implementation of low-value care; monitoring equity markers; organizational policies for healthcare equity; improving the economic evaluation of implementation; policy and dissemination research; and capacity building.",success
32692370,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Racial bias is associated with the allocation of advanced heart failure therapies, heart transplants, and ventricular assist devices. It is unknown whether gender and racial biases are associated with the allocation of advanced therapies among women. To determine whether the intersection of patient gender and race is associated with the decision-making of clinicians during the allocation of advanced heart failure therapies. In this qualitative study, 46 US clinicians attending a conference for an international heart transplant organization in April 2019 were interviewed on the allocation of advanced heart failure therapies. Participants were randomized to examine clinical vignettes that varied 1:1 by patient race (African American to white) and 20:3 by gender (women to men) to purposefully target vignettes of women patients to compare with a prior study of vignettes of men patients. Participants were interviewed about their decision-making process using the think-aloud technique and provided supplemental surveys. Interviews were analyzed using grounded theory methodology, and surveys were analyzed with Wilcoxon tests. Randomization to clinical vignettes. Thematic differences in allocation of advanced therapies by patient race and gender. Among 46 participants (24 [52%] women, 20 [43%] racial minority), participants were randomized to the vignette of a white woman (20 participants [43%]), an African American woman (20 participants [43%]), a white man (3 participants [7%]), and an African American man (3 participants [7%]). Allocation differences centered on 5 themes. First, clinicians critiqued the appearance of the women more harshly than the men as part of their overall impressions. Second, the African American man was perceived as experiencing more severe illness than individuals from other racial and gender groups. Third, there was more concern regarding appropriateness of prior care of the African American woman compared with the white woman. Fourth, there were greater concerns about adequacy of social support for the women than for the men. Children were perceived as liabilities for women, particularly the African American woman. Family dynamics and finances were perceived to be greater concerns for the African American woman than for individuals in the other vignettes; spouses were deemed inadequate support for women. Last, participants recommended ventricular assist devices over transplantation for all racial and gender groups. Surveys revealed no statistically significant differences in allocation recommendations for African American and white women patients. This national study of health care professionals randomized to clinical vignettes that varied only by gender and race found evidence of gender and race bias in the decision-making process for offering advanced therapies for heart failure, particularly for African American women patients, who were judged more harshly by appearance and adequacy of social support. There was no associated between patient gender and race and final recommendations for allocation of advanced therapies. However, it is possible that bias may contribute to delayed allocation and ultimately inequity in the allocation of advanced therapies in a clinical setting.",success
37228737,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Many clinical processes include multidisciplinary group decision-making, yet few methods exist to evaluate the presence of implicit bias during this collective process. Implicit bias negatively impacts the equitable delivery of evidence-based interventions and ultimately patient outcomes. Since implicit bias can be difficult to assess, novel approaches are required to detect and analyze this elusive phenomenon. In this paper, we describe how the de Groot Critically Reflective Diagnoses Protocol (DCRDP) can be used as a data analysis tool to evaluate group dynamics as an essential foundation for exploring how interactions can bias collective clinical decision-making. The DCRDP includes 6 distinct criteria: challenging groupthink, critical opinion sharing, research utilization, openness to mistakes, asking and giving feedback, and experimentation. Based on the strength and frequency of codes in the form of exemplar quotes, each criterion was given a numerical score of 1-4 with 1 representing teams that are interactive, reflective, higher functioning, and more equitable. When applied as a coding scheme to transcripts of recorded decision-making meetings, the DCRDP was revealed as a practical tool for examining group decision-making bias. It can be adapted to a variety of clinical, educational, and other professional settings as an impetus for recognizing the presence of team-based bias, engaging in reflexivity, informing the design and testing of implementation strategies, and monitoring long-term outcomes to promote more equitable decision-making processes in healthcare.",success
36931436,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,True,"Advanced heart failure (AHF) therapy allocation is vulnerable to bias related to subjective assessments and poor group dynamics. Our objective was to determine whether an implementation strategy for AHF team members could feasibly contribute to organizational and culture change supporting equity in AHF allocation. Using a pretest-posttest design, the strategy included an 8-week multicomponent training on bias reduction, standardized numerical social assessments, and enhanced group dynamics at an AHF center. Evaluations of organizational and cultural changes included pretest-posttest AHF team member surveys, transcripts of AHF meetings to assess group dynamics using a standardized scoring system, and posttest interviews guided by a framework for implementing a complex strategy. Results were analyzed with qualitative descriptive methods and Brunner-Munzel tests for relative effect (RE, RE >0.5 signals posttest improvement). The majority of survey metrics revealed potential benefit with RE >0.5. REs were >0.5 for 5 of 6 group dynamics metrics. Themes for implementation included (1) promoting equitable distribution of scarce resources, (2) requiring a change in team members' time investment to correct bias and change the meeting structure, (3) slowing and then accelerating the allocation process, and (4) adaptable beyond AHF and reinforceable with semi-annual trainings. An implementation strategy for AHF equity demonstrated the feasibility for organizational and culture changes.",success
25895742,False,Journal Article,,,,,,,,True,"Implementation science has progressed towards increased use of theoretical approaches to provide better understanding and explanation of how and why implementation succeeds or fails. The aim of this article is to propose a taxonomy that distinguishes between different categories of theories, models and frameworks in implementation science, to facilitate appropriate selection and application of relevant approaches in implementation research and practice and to foster cross-disciplinary dialogue among implementation researchers. Theoretical approaches used in implementation science have three overarching aims: describing and/or guiding the process of translating research into practice (process models); understanding and/or explaining what influences implementation outcomes (determinant frameworks, classic theories, implementation theories); and evaluating implementation (evaluation frameworks). This article proposes five categories of theoretical approaches to achieve three overarching aims. These categories are not always recognized as separate types of approaches in the literature. While there is overlap between some of the theories, models and frameworks, awareness of the differences is important to facilitate the selection of relevant approaches. Most determinant frameworks provide limited ""how-to"" support for carrying out implementation endeavours since the determinants usually are too generic to provide sufficient detail for guiding an implementation process. And while the relevance of addressing barriers and enablers to translating research into practice is mentioned in many process models, these models do not identify or systematically structure specific determinants associated with implementation success. Furthermore, process models recognize a temporal sequence of implementation endeavours, whereas determinant frameworks do not explicitly take a process perspective of implementation.",success
30180116,False,Journal Article;Scoping Review,,,,,,,,True,"Community health workers (CHWs) are becoming a well-recognized workforce to help reduce health disparities and improve health equity. Although evidence demonstrates the value of engaging CHWs in health care teams, there is a need to describe best practices for integrating CHWs into US health care settings. The use of existing health promotion and implementation theories could guide the research and implementation of health interventions conducted by CHWs. We conducted a standard 5-step scoping review plus stakeholder engagement to provide insight into this topic. Using PubMed, EMBASE, and Web of Science, we identified CHW intervention studies in health care settings published between 2000 and 2017. Studies were abstracted by 2 researchers for characteristics and reported use of theory. Our final review included 50 articles published between January 2000 and April 2017. Few studies used implementation theories to understand the facilitators and barriers to CHW integration. Those studies that incorporated implementation theories used RE-AIM, intervention mapping, cultural tailoring, PRECEDE-PROCEED, and the diffusion of innovation. Although most studies did not report using implementation theories, some constructs of implementation such as fidelity or perceived benefits were assessed. In addition, studies that reported intervention development often cited specific theories, such as the transtheoretical or health belief model, that helped facilitate the development of their program. Our results are consistent with other literature describing poor uptake and use of implementation theory. Further translation of implementation theories for CHW integration is recommended.",success
37532425,False,"Journal Article;Review;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Heart failure (HF) is a leading cause of death and disability in older adults. Despite decades of high-quality evidence to support their use, guideline-directed medical therapies (GDMTs) that reduce death and disease burden in HF have been suboptimally implemented. Approaches to closing care gaps have focused largely on strategies proven to be ineffective, whilst effective interventions shown to improve GDMT uptake have not been instituted. This review synthesizes implementation interventions that increase the uptake of GDMT, discusses barriers and facilitators of implementation, summarizes conceptual frameworks in implementation science that could improve knowledge uptake, and offers suggestions for trial design that could better facilitate end-of-trial implementation. We propose an evidence-to-care conceptual model that could foster the simultaneous generation of evidence and long-term implementation. By adopting principles of implementation science, policymakers, researchers, and clinicians can help reduce the burden of HF on patients and health care systems worldwide.",success
10474547,False,"Journal Article;Research Support, U.S. Gov't, P.H.S.;Review",,,,,,,,True,"Progress in public health and community-based interventions has been hampered by the lack of a comprehensive evaluation framework appropriate to such programs. Multilevel interventions that incorporate policy, environmental, and individual components should be evaluated with measurements suited to their settings, goals, and purpose. In this commentary, the authors propose a model (termed the RE-AIM model) for evaluating public health interventions that assesses 5 dimensions: reach, efficacy, adoption, implementation, and maintenance. These dimensions occur at multiple levels (e.g., individual, clinic or organization, community) and interact to determine the public health or population-based impact of a program or policy. The authors discuss issues in evaluating each of these dimensions and combining them to determine overall public health impact. Failure to adequately evaluate programs on all 5 dimensions can lead to a waste of resources, discontinuities between stages of research, and failure to improve public health to the limits of our capacity. The authors summarize strengths and limitations of the RE-AIM model and recommend areas for future research and application.",success
18468362,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Although numerous studies address the efficacy and effectiveness of health interventions, less research addresses successfully implementing and sustaining interventions. As long as efficacy and effectiveness trials are considered complete without considering implementation in nonresearch settings, the public health potential of the original investments will not be realized. A barrier to progress is the absence of a practical, robust model to help identify the factors that need to be considered and addressed and how to measure success. A conceptual framework for improving practice is needed to integrate the key features for successful program design, predictors of implementation and diffusion, and appropriate outcome measures. A comprehensive model for translating research into practice was developed using concepts from the areas of quality improvement, chronic care, the diffusion of innovations, and measures of the population-based effectiveness of translation. PRISM--the Practical, Robust Implementation and Sustainability Model--evaluates how the health care program or intervention interacts with the recipients to influence program adoption, implementation, maintenance, reach, and effectiveness. The PRISM model provides a new tool for researchers and health care decision makers that integrates existing concepts relevant to translating research into practice.",success
21513547,False,"Journal Article;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Improving the design and implementation of evidence-based practice depends on successful behaviour change interventions. This requires an appropriate method for characterising interventions and linking them to an analysis of the targeted behaviour. There exists a plethora of frameworks of behaviour change interventions, but it is not clear how well they serve this purpose. This paper evaluates these frameworks, and develops and evaluates a new framework aimed at overcoming their limitations. A systematic search of electronic databases and consultation with behaviour change experts were used to identify frameworks of behaviour change interventions. These were evaluated according to three criteria: comprehensiveness, coherence, and a clear link to an overarching model of behaviour. A new framework was developed to meet these criteria. The reliability with which it could be applied was examined in two domains of behaviour change: tobacco control and obesity. Nineteen frameworks were identified covering nine intervention functions and seven policy categories that could enable those interventions. None of the frameworks reviewed covered the full range of intervention functions or policies, and only a minority met the criteria of coherence or linkage to a model of behaviour. At the centre of a proposed new framework is a 'behaviour system' involving three essential conditions: capability, opportunity, and motivation (what we term the 'COM-B system'). This forms the hub of a 'behaviour change wheel' (BCW) around which are positioned the nine intervention functions aimed at addressing deficits in one or more of these conditions; around this are placed seven categories of policy that could enable those interventions to occur. The BCW was used reliably to characterise interventions within the English Department of Health's 2010 tobacco control strategy and the National Institute of Health and Clinical Excellence's guidance on reducing obesity. Interventions and policies to change behaviour can be usefully characterised by means of a BCW comprising: a 'behaviour system' at the hub, encircled by intervention functions and then by policy categories. Research is needed to establish how far the BCW can lead to more efficient design of effective interventions.",success
35101088,False,"Letter;Research Support, N.I.H., Extramural;Comment",,,,,,,,True,"There is increasing attention being given to opportunities and approaches to advance health equity using implementation science. To reduce disparities in health, it is crucial that an equity lens is integrated from the earliest stages of the implementation process. In this paper, we outline four key pre-implementation steps and associated questions for implementation researchers to consider that may help guide selection and design of interventions and associated implementation strategies that are most likely to reach and be effective in reducing health disparities among vulnerable persons and communities.",success
31171014,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"This paper describes the process and results of a refinement of a framework to characterize modifications to interventions. The original version did not fully capture several aspects of modification and adaptation that may be important to document and report. Additionally, the earlier framework did not include a way to differentiate cultural adaptation from adaptations made for other reasons. Reporting additional elements will allow for a more precise understanding of modifications, the process of modifying or adapting, and the relationship between different forms of modification and subsequent health and implementation outcomes. We employed a multifaceted approach to develop the updated FRAME involving coding documents identified through a literature review, rapid coding of qualitative interviews, and a refinement process informed by multiple stakeholders. The updated FRAME expands upon Stirman et al.'s original framework by adding components of modification to report: (1) when and how in the implementation process the modification was made, (2) whether the modification was planned/proactive (i.e., an adaptation) or unplanned/reactive, (3) who determined that the modification should be made, (4) what is modified, (5) at what level of delivery the modification is made, (6) type or nature of context or content-level modifications, (7) the extent to which the modification is fidelity-consistent, and (8) the reasons for the modification, including (a) the intent or goal of the modification (e.g., to reduce costs) and (b) contextual factors that influenced the decision. Methods of using the framework to assess modifications are outlined, along with their strengths and weaknesses, and considerations for research to validate these measurement strategies. The updated FRAME includes consideration of when and how modifications occurred, whether it was planned or unplanned, relationship to fidelity, and reasons and goals for modification. This tool that can be used to support research on the timing, nature, goals and reasons for, and impact of modifications to evidence-based interventions.",success
35483798,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The aim of this study was to assess trends in heart failure (HF) hospitalizations among young adults. Data are limited regarding clinical characteristics and outcomes of young adults hospitalized for HF. The National Inpatient Sample database was analyzed to identify adults aged 18 to 45 years who were hospitalized for HF between 2004 and 2018. In total, 767,180 weighted hospitalizations for HF in young adults were identified, equivalent to 4.32 (95% CI: 4.31-4.33) per 10,000 person-years. Overall HF hospitalizations per 10,000 U.S. population of young adults decreased from 2.43 in 2004 to 1.82 in 2012, followed by an increase to 2.51 in 2018. Black adults (50.1%) had a significantly higher proportion of HF hospitalizations compared with White (31.9%) and Hispanic adults (12.2%) throughout the study period. Nearly half of patients (45.8%) lived in zip codes in the lowest quartile of national household income. Overall, in-hospital mortality was 1.3%, which decreased over time; this trend was consistent by sex and race. The overall mean LOS (5.2 days) remained stable over time, while the mean inflation-adjusted cost increased from $12,449 in 2004 to $16,786 in 2018, with significant overall differences by race and sex. This longitudinal examination of U.S. clinical practice revealed that HF hospitalizations among young adults have increased since 2013. Approximately half of these patients are Black and reside in zip codes in the lowest quartile of national household income. Temporal trends showed decreased in-hospital mortality, stable adjusted lengths of stay, and increased inflation-adjusted costs, with significant racial differences in hospitalization rates.",success
28655709,False,Comparative Study;Journal Article,,,,,,,,True,"National heart failure (HF) hospitalization rates have not been appropriately age standardized by sex or race/ethnicity. Reporting hospital utilization trends by subgroup is important for monitoring population health and developing interventions to eliminate disparities. The National Inpatient Sample (NIS) was used to estimate the crude and age-standardized rates of HF hospitalization between 2002 and 2013 by sex and race/ethnicity. Direct standardization was used to age-standardize rates to the 2000 US standard population. Relative differences between subgroups were reported. The national age-adjusted HF hospitalization rate decreased 30.8% from 526.86 to 364.66 per 100 000 between 2002 and 2013. Although hospitalizations decreased for all subgroups, the ratio of the age-standardized rate for men compared with women increased from 20% greater to 39% (<i>P</i> trend=0.002) between 2002 and 2013. Black men had a rate that was 229% (<i>P</i> trend=0.141) and black women, 240% (<i>P</i> trend=0.725) with reference to whites in 2013 with no significant change between 2002 and 2013. Hispanic men had a rate that was 32% greater in 2002 and the difference narrowed to 4% (<i>P</i> trend=0.047) greater in 2013 relative to whites. For Hispanic women, the rate was 55% greater in 2002 and narrowed to 8% greater (<i>P</i> trend=0.004) in 2013 relative to whites. Asian/Pacific Islander men had a 27% lower rate in 2002 that improved to 43% (<i>P</i> trend=0.040) lower in 2013 relative to whites. For Asian/Pacific Islander women, the hospitalization rate was 24% lower in 2002 and improved to 43% (<i>P</i> trend=0.021) lower in 2013 relative to whites. National HF hospitalization rates have decreased steadily during the recent decade. Disparities in HF burden and hospital utilization by sex and race/ethnicity persist. Significant population health interventions are needed to reduce the HF hospitalization burden among blacks. An evaluation of factors explaining the improvements in the HF hospitalization rates among Hispanics and Asian/Pacific Islanders is needed.",success
37797885,False,Journal Article,,,,,,,,False,,success
32478025,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"RE-AIM is a widely adopted, robust implementation science (IS) framework used to inform intervention and implementation design, planning, and evaluation, as well as to address short-term maintenance. In recent years, there has been growing focus on the longer-term sustainability of evidence-based programs, policies and practices (EBIs). In particular, investigators have conceptualized sustainability as the continued health impact and delivery of EBIs over a longer period of time (e.g., years after initial implementation) and incorporated the complex and evolving nature of context. We propose a reconsideration of RE-AIM to integrate recent conceptualizations of sustainability with a focus on addressing dynamic context and promoting health equity. In this Perspective, we present an extension of the RE-AIM framework to guide planning, measurement/evaluation, and adaptations focused on enhancing sustainability. We recommend consideration of: (1) extension of ""maintenance"" within RE-AIM to include recent conceptualizations of dynamic, longer-term intervention sustainability and ""evolvability"" across the life cycle of EBIs, including adaptation and potential de-implementation in light of changing and evolving evidence, contexts, and population needs; (2) iterative application of RE-AIM assessments to guide adaptations and enhance long-term sustainability; (3) explicit consideration of equity and cost as fundamental, driving forces that need to be addressed across RE-AIM dimensions to enhance sustainability; and (4) use or integration of RE-AIM with other existing frameworks that address key contextual factors and examine multi-level determinants of sustainability. Finally, we provide testable hypotheses and detailed research questions to inform future research in these areas.",success
37061808,False,Journal Article,,,,,,,,False,,success
33201741,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",NCT03334188,databank,NCT03334188,NCT03334188,NCT03334188,NCT03334188|databank,NCT03334188|databank,True,"Major gaps exist in the routine initiation and dose up-titration of guideline-directed medical therapies (GDMT) for patients with heart failure with reduced ejection fraction. Without novel approaches to improve prescribing, the cumulative benefits of heart failure with reduced ejection fraction treatment will be largely unrealized. Direct-to-consumer marketing and shared decision making reflect a culture where patients are increasingly involved in treatment choices, creating opportunities for prescribing interventions that engage patients. The EPIC-HF (Electronically Delivered, Patient-Activation Tool for Intensification of Medications for Chronic Heart Failure with Reduced Ejection Fraction) trial randomized patients with heart failure with reduced ejection fraction from a diverse health system to usual care versus patient activation tools-a 3-minute video and 1-page checklist-delivered electronically 1 week before, 3 days before, and 24 hours before a cardiology clinic visit. The tools encouraged patients to work collaboratively with their clinicians to ""make one positive change"" in heart failure with reduced ejection fraction prescribing. The primary endpoint was the percentage of patients with GDMT medication initiations and dose intensifications from immediately preceding the cardiology clinic visit to 30 days after, compared with usual care during the same period. EPIC-HF enrolled 306 patients, 290 of whom attended a clinic visit during the study period: 145 were sent the patient activation tools and 145 were controls. The median age of patients was 65 years; 29% were female, 11% were Black, 7% were Hispanic, and the median ejection fraction was 32%. Preclinic data revealed significant GDMT opportunities, with no patients on target doses of β-blocker, sacubitril/valsartan, and mineralocorticoid receptor antagonists. From immediately preceding the cardiology clinic visit to 30 days after, 49.0% in the intervention and 29.7% in the control experienced an initiation or intensification of their GDMT (<i>P</i>=0.001). The majority of these changes were made at the clinician encounter itself and involved dose uptitrations. There were no deaths and no significant differences in hospitalization or emergency department visits at 30 days between groups. A patient activation tool delivered electronically before a cardiology clinic visit improved clinician intensification of GDMT. Registration: URL: https://www.clinicaltrials.gov; Unique identifier: NCT03334188.",success
32936209,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Optimal treatment of heart failure with reduced ejection fraction (HFrEF) is scripted by treatment guidelines, but many eligible patients do not receive guideline-directed medical therapy (GDMT) in clinical practice. To determine whether a remote, algorithm-driven, navigator-administered medication optimization program could enhance implementation of GDMT in HFrEF. In this case-control study, a population-based sample of patients with HFrEF was offered participation in a quality improvement program directed at GDMT optimization. Treating clinicians in a tertiary academic medical center who were caring for patients with heart failure and an ejection fraction of 40% or less (identified through an electronic health record-based search) were approached for permission to adjust medical therapy according to a sequential titration algorithm modeled on the current American College of Cardiology/American Heart Association heart failure guidelines. Navigators contacted participants by telephone to direct medication adjustment and conduct longitudinal surveillance of laboratory tests, blood pressure, and symptoms under supervision of a pharmacist, nurse practitioner, and heart failure cardiologist. Patients and clinicians declining to participate served as a control group. Navigator-led remote optimization of GDMT compared with usual care. Proportion of patients receiving GDMT in the intervention and control groups at 3 months. Of 1028 eligible patients (mean [SD] values: age, 68 [14] years; ejection fraction, 32% [8%]; and systolic blood pressure, 122 [18] mm Hg; 305 women (30.0%); 892 individuals [86.8%] in New York Heart Association class I and II), 197 (19.2%) participated in the medication optimization program, and 831 (80.8%) continued with usual care as directed by their treating clinicians (585 [56.9%] general cardiologists; 443 [43.1%] heart failure specialists). At 3 months, patients participating in the remote intervention experienced significant increases from baseline in use of renin-angiotensin system antagonists (138 [70.1%] to 170 [86.3%]; P < .001) and β-blockers (152 [77.2%] to 181 [91.9%]; P < .001) but not mineralocorticoid receptor antagonists (51 [25.9%] to 60 [30.5%]; P = .14). Doses for each category of GDMT also increased from baseline in the intervention group. Among the usual-care group, there were no changes from baseline in the proportion of patients receiving GDMT or the dose of GDMT in any category. Remote titration of GDMT by navigators using encoded algorithms may represent an efficient, population-level strategy for rapidly closing the gap between guidelines and clinical practice in patients with HFrEF.",success
35385798,True,"Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",NCT04514458,databank,NCT04514458,NCT04514458,NCT04514458,NCT04514458|databank,NCT04514458|databank,True,"The use of guideline-directed medical therapy (GDMT) is underprescribed in patients with heart failure with reduced ejection fraction (HFrEF). This study sought to examine whether targeted and tailored electronic health record (EHR) alerts recommending GDMT in eligible patients with HFrEF improves GDMT use. PROMPT-HF (PRagmatic trial Of Messaging to Providers about Treatment of Heart Failure) was a pragmatic, EHR-based, cluster-randomized comparative effectiveness trial. A total of 100 providers caring for patients with HFrEF were randomized to either an alert or usual care. The alert notified providers of individualized GDMT recommendations along with patient characteristics. The primary outcome was an increase in the number of GDMT classes prescribed at 30 days postrandomization. Providers were surveyed on knowledge of guidelines and user experience. The study enrolled 1,310 ambulatory patients with HFrEF from April to October 2021. Median age was 72 years; 31% were female; 18% were Black; and median left ventricular ejection fraction was 32%. At baseline, 84% of participants were receiving β-blockers, 71% received a renin-angiotensin-aldosterone system inhibitor, 29% received a mineralocorticoid receptor antagonist, and 11% received a sodium-glucose cotransporter-2 inhibitor. The primary outcome occurred in 176 of 685 (26%) participants in the alert arm vs 117 of 625 (19%) in the usual care arm, thus increasing GDMT class prescription by >40% after alert exposure (adjusted relative risk: 1.41; 95% CI: 1.03-1.93; P = 0.03). The number of patients needed to alert to result in an increase in addition of GDMT classes was 14. A total of 79% of alerted providers agreed that the alert was effective at enabling improved prescription of medical therapy for HF. A real-time, targeted, and tailored EHR-based alerting system for outpatients with HFrEF led to significantly higher rates of GDMT at 30 days when compared with usual care. This low-cost intervention can be rapidly integrated into clinical care and accelerate adoption of high-value therapies in heart failure. (PRagmatic trial Of Messaging to Providers about Treatment of Heart Failure [PROMPT-HF; NCT04514458]).",success
36356631,True,Randomized Controlled Trial;Journal Article,NCT03412201,databank,NCT03412201,NCT03412201,NCT03412201,NCT03412201|databank,NCT03412201|databank,True,"There is a paucity of evidence for dose and pace of up-titration of guideline-directed medical therapies after admission to hospital for acute heart failure. In this multinational, open-label, randomised, parallel-group trial (STRONG-HF), patients aged 18-85 years admitted to hospital with acute heart failure, not treated with full doses of guideline-directed drug treatment, were recruited from 87 hospitals in 14 countries. Before discharge, eligible patients were randomly assigned (1:1), stratified by left ventricular ejection fraction (≤40% vs >40%) and country, with blocks of size 30 within strata and randomly ordered sub-blocks of 2, 4, and 6, to either usual care or high-intensity care. Usual care followed usual local practice, and high-intensity care involved the up-titration of treatments to 100% of recommended doses within 2 weeks of discharge and four scheduled outpatient visits over the 2 months after discharge that closely monitored clinical status, laboratory values, and N-terminal pro-B-type natriuretic peptide (NT-proBNP) concentrations. The primary endpoint was 180-day readmission to hospital due to heart failure or all-cause death. Efficacy and safety were assessed in the intention-to-treat (ITT) population (ie, all patients validly randomly assigned to treatment). The primary endpoint was assessed in all patients enrolled at hospitals that followed up patients to day 180. Because of a protocol amendment to the primary endpoint, the results of patients enrolled on or before this amendment were down-weighted. This study is registered with ClinicalTrials.gov, NCT03412201, and is now complete. Between May 10, 2018, and Sept 23, 2022, 1641 patients were screened and 1078 were successfully randomly assigned to high-intensity care (n=542) or usual care (n=536; ITT population). Mean age was 63·0 years (SD 13·6), 416 (39%) of 1078 patients were female, 662 (61%) were male, 832 (77%) were White or Caucasian, 230 (21%) were Black, 12 (1%) were other races, one (<1%) was Native American, and one (<1%) was Pacific Islander (two [<1%] had missing data on race). The study was stopped early per the data and safety monitoring board's recommendation because of greater than expected between-group differences. As of data cutoff (Oct 13, 2022), by day 90, a higher proportion of patients in the high-intensity care group had been up-titrated to full doses of prescribed drugs (renin-angiotensin blockers 278 [55%] of 505 vs 11 [2%] of 497; β blockers 249 [49%] vs 20 [4%]; and mineralocorticoid receptor antagonists 423 [84%] vs 231 [46%]). By day 90, blood pressure, pulse, New York Heart Association class, bodyweight, and NT-proBNP concentration had decreased more in the high-intensity care group than in the usual care group. Heart failure readmission or all-cause death up to day 180 occurred in 74 (15·2% down-weighted adjusted Kaplan-Meier estimate) of 506 patients in the high-intensity care group and 109 (23·3%) of 502 patients in the usual care group (adjusted risk difference 8·1% [95% CI 2·9-13·2]; p=0·0021; risk ratio 0·66 [95% CI 0·50-0·86]). More adverse events by 90 days occurred in the high-intensity care group (223 [41%] of 542) than in the usual care group (158 [29%] of 536) but similar incidences of serious adverse events (88 [16%] vs 92 [17%]) and fatal adverse events (25 [5%] vs 32 [6%]) were reported in each group. An intensive treatment strategy of rapid up-titration of guideline-directed medication and close follow-up after an acute heart failure admission was readily accepted by patients because it reduced symptoms, improved quality of life, and reduced the risk of 180-day all-cause death or heart failure readmission compared with usual care. Roche Diagnostics.",success
36882134,True,"Randomized Controlled Trial;Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",NCT05275920,databank,NCT05275920,NCT05275920,NCT05275920,NCT05275920|databank,NCT05275920|databank,True,"Mineralocorticoid receptor antagonists (MRAs) are underprescribed for patients with heart failure with reduced ejection fraction (HFrEF). This study sought to compare effectiveness of 2 automated, electronic health record-embedded tools vs usual care on MRA prescribing in eligible patients with HFrEF. BETTER CARE-HF (Building Electronic Tools to Enhance and Reinforce Cardiovascular Recommendations for Heart Failure) was a 3-arm, pragmatic, cluster-randomized trial comparing the effectiveness of an alert during individual patient encounters vs a message about multiple patients between encounters vs usual care on MRA prescribing. This study included adult patients with HFrEF, no active MRA prescription, no contraindication to MRAs, and an outpatient cardiologist in a large health system. Patients were cluster-randomized by cardiologist (60 per arm). The study included 2,211 patients (alert: 755, message: 812, usual care [control]: 644), with average age 72.2 years, average ejection fraction 33%, who were predominantly male (71.4%) and White (68.9%). New MRA prescribing occurred in 29.6% of patients in the alert arm, 15.6% in the message arm, and 11.7% in the control arm. The alert more than doubled MRA prescribing compared to usual care (relative risk: 2.53; 95% CI: 1.77-3.62; P < 0.0001) and improved MRA prescribing compared to the message (relative risk: 1.67; 95% CI: 1.21-2.29; P = 0.002). The number of patients with alert needed to result in an additional MRA prescription was 5.6. An automated, patient-specific, electronic health record-embedded alert increased MRA prescribing compared to both a message and usual care. These findings highlight the potential for electronic health record-embedded tools to substantially increase prescription of life-saving therapies for HFrEF. (Building Electronic Tools to Enhance and Reinforce Cardiovascular Recommendations-Heart Failure [BETTER CARE-HF]; NCT05275920).",success
34313687,True,"Comparative Study;Journal Article;Multicenter Study;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",NCT03035474,databank,NCT03035474,NCT03035474,NCT03035474,NCT03035474|databank,NCT03035474|databank,True,"Adoption of guideline-directed medical therapy for patients with heart failure is variable. Interventions to improve guideline-directed medical therapy have failed to consistently achieve target metrics, and limited data exist to inform efforts to improve heart failure quality of care. To evaluate the effect of a hospital and postdischarge quality improvement intervention compared with usual care on heart failure outcomes and care. This cluster randomized clinical trial was conducted at 161 US hospitals and included 5647 patients (2675 intervention vs 2972 usual care) followed up after a hospital discharge for acute heart failure with reduced ejection fraction (HFrEF). The trial was performed from 2017 to 2020, and the date of final follow-up was August 31, 2020. Hospitals (n = 82) randomized to a hospital and postdischarge quality improvement intervention received regular education of clinicians by a trained group of heart failure and quality improvement experts and audit and feedback on heart failure process measures (eg, use of guideline-directed medical therapy for HFrEF) and outcomes. Hospitals (n = 79) randomized to usual care received access to a generalized heart failure education website. The coprimary outcomes were a composite of first heart failure rehospitalization or all-cause mortality and change in an opportunity-based composite score for heart failure quality (percentage of recommendations followed). Among 5647 patients (mean age, 63 years; 33% women; 38% Black; 87% chronic heart failure; 49% recent heart failure hospitalization), vital status was known for 5636 (99.8%). Heart failure rehospitalization or all-cause mortality occurred in 38.6% in the intervention group vs 39.2% in usual care (adjusted hazard ratio, 0.92 [95% CI, 0.81 to 1.05). The baseline quality-of-care score was 42.1% vs 45.5%, respectively, and the change from baseline to follow-up was 2.3% vs -1.0% (difference, 3.3% [95% CI, -0.8% to 7.3%]), with no significant difference between the 2 groups in the odds of achieving a higher composite quality score at last follow-up (adjusted odds ratio, 1.06 [95% CI, 0.93 to 1.21]). Among patients with HFrEF in hospitals randomized to a hospital and postdischarge quality improvement intervention vs usual care, there was no significant difference in time to first heart failure rehospitalization or death, or in change in a composite heart failure quality-of-care score. ClinicalTrials.gov Identifier: NCT03035474.",success
30806695,True,Comparative Study;Journal Article;Multicenter Study;Pragmatic Clinical Trial;Randomized Controlled Trial,NCT02112227,databank,NCT02112227,NCT02112227,NCT02112227,NCT02112227|databank,NCT02112227|databank,True,"Health care services that support the hospital-to-home transition can improve outcomes in patients with heart failure (HF). To test the effectiveness of the Patient-Centered Care Transitions in HF transitional care model in patients hospitalized for HF. Stepped-wedge cluster randomized trial of 2494 adults hospitalized for HF across 10 hospitals in Ontario, Canada, from February 2015 to March 2016, with follow-up until November 2016. Hospitals were randomized to receive the intervention (n = 1104 patients), in which nurse-led self-care education, a structured hospital discharge summary, a family physician follow-up appointment less than 1 week after discharge, and, for high-risk patients, structured nurse homevisits and heart function clinic care were provided to patients, or usual care (n = 1390 patients), in which transitional care was left to the discretion of clinicians. Primary outcomes were hierarchically ordered as composite all-cause readmission, emergency department (ED) visit, or death at 3 months; and composite all-cause readmission or ED visit at 30 days. Secondary outcomes were B-PREPARED score for discharge preparedness (range: 0 [most prepared] to 22 [least prepared]); the 3-Item Care Transitions Measure (CTM-3) for quality of transition (range: 0 [worst transition] to 100 [best transition]); the 5-level EQ-5D version (EQ-5D-5L) for quality of life (range: 0 [dead] to 1 [full health]); and quality-adjusted life-years (QALY; range: 0 [dead] to 0.5 [full health at 6 months]). Among eligible patients, all 2494 (mean age, 77.7 years; 1258 [50.4%] women) completed the trial. There was no significant difference between the intervention and usual care groups in the first primary composite outcome (545 [49.4%] vs 698 [50.2%] events, respectively; hazard ratio [HR], 0.99 [95% CI, 0.83-1.19]) or in the second primary composite outcome (304 [27.5%] vs 408 [29.3%] events, respectively; HR, 0.93 [95% CI, 0.73-1.18]). There were significant differences between the intervention and usual care groups in the secondary outcomes of mean B-PREPARED score at 6 weeks (16.6 vs 13.9; difference, 2.65 [95% CI, 1.37-3.92]; P < .001); mean CTM-3 score at 6 weeks (76.5 vs 70.3; difference, 6.16 [95% CI, 0.90-11.43]; P = .02); and mean EQ-5D-5L score at 6 weeks (0.7 vs 0.7; difference, 0.06 [95% CI, 0.01 to 0.11]; P = .02) and 6 months (0.7 vs 0.6; difference, 0.06 [95% CI, 0.01-0.12]; P = .02). There was no significant difference in mean QALY between groups at 6 months (0.3 vs 0.3; difference, 0.00 [95% CI, -0.02 to 0.02]; P = .98). Among patients with HF in Ontario, Canada, implementation of a patient-centered transitional care model compared with usual care did not improve a composite of clinical outcomes. Whether this type of intervention could be effective in other health care systems or locations would require further research. ClinicalTrials.gov Identifier: NCT02112227.",success
29754670,False,Journal Article;Meta-Analysis;Review,NCT02112227,databank,NCT02112227,NCT02112227,NCT02112227,NCT02112227|databank,NCT02112227|databank,True,"Heart Failure (HF) is a common cause of hospitalization in older adults. The transition from hospital to home is high-risk, and gaps in transitional care can increase the risk of re-hospitalization and death. Combining health care services supported by meta-analyses, we designed the PACT-HF transitional care model. Adopting an integrated Knowledge Translation (iKT) approach in which decision-makers and clinicians are partners in research, we implement and test the effectiveness of PACT-HF among patients hospitalized for HF. We use a pragmatic stepped wedge cluster randomized trial design to introduce the complex health service intervention to 10 large hospitals in a randomized sequence until all hospitals initiate the intervention. The goal is for all patients hospitalized with HF to receive self-care education, multidisciplinary care, and early follow-up with their health care providers; and in addition, for high-risk patients to receive post-discharge nurse-led home visits and outpatient care in Heart Function clinics. This requires integration of care across hospitals, home care agencies, and outpatient clinics in our publicly funded health care system. While hospitals are the unit of recruitment and analysis, patients (estimated sample size of 3200) are the unit of analysis. Primary outcomes are hierarchically ordered as time to composite all-cause readmissions / emergency department (ED) visits / death at 3 months and time to composite all-cause readmissions / ED visits at 30 days. In a nested study of 8 hospitals, we measure the patient-centered outcomes of Discharge Preparedness, Care Transitions Quality, and Quality Adjusted Life Years (QALY); and the 6-month health care resource use and costs. We obtain all clinical and cost outcomes via linkages to provincial administrative databases. This protocol describes the implementation and testing of a transitional care model comprising health care services informed by high-level evidence. The study adopts an iKT and pragmatic approach, uses a robust study design, links clinical trial data with outcomes held in administrative databases, and includes patient-reported outcomes. Findings will have implications on clinical practice, health care policy, and Knowledge Translation (KT) research methodology.",success
36755744,False,Journal Article,,,,,,,,True,"Hospitals and skilled nursing facilities (SNFs) are incentivized to reduce hospital readmissions among patients with heart failure (HF). We used the RE-AIM framework and mixed quantitative and qualitative data to evaluate the implementation of a multimodal HF management protocol (HFMP) administered in a SNF in 2021. Over 90% of eligible patients were enrolled in the HFMP (REACH). Of the 42 enrolled patients (61.9% female, aged 81.9 ± 8.9 years, 9.5% Medicaid), 2 (4.8%) were readmitted within 30 days of hospital discharge and 4 (9.5%) were readmitted within 30 days of SNF discharge compared with historical (2020) rates of 16.7% and 22.2%, respectively (a potential savings of $132,418-$176,573 in hospital costs) (EFFECTIVENESS). Although stakeholder feedback about ADOPTION and IMPLEMENTATION was largely positive, challenges associated with clinical data collection, documentation, and staff turnover were described. Findings will inform refinement of the HFMP to facilitate further testing and sustainability (MAINTENANCE).",success
33749610,False,Journal Article,NCT04028557,databank,NCT04028557,NCT04028557,NCT04028557,NCT04028557|databank,NCT04028557|databank,True,"Limited consideration of clinical decision support (CDS) design best practices, such as a user-centered design, is often cited as a key barrier to CDS adoption and effectiveness. The application of CDS best practices is resource intensive; thus, institutions often rely on commercially available CDS tools that are created to meet the generalized needs of many institutions and are not user centered. Beyond resource availability, insufficient guidance on how to address key aspects of implementation, such as contextual factors, may also limit the application of CDS best practices. An implementation science (IS) framework could provide needed guidance and increase the reproducibility of CDS implementations. This study aims to compare the effectiveness of an enhanced CDS tool informed by CDS best practices and an IS framework with a generic, commercially available CDS tool. We conducted an explanatory sequential mixed methods study. An IS-enhanced and commercial CDS alert were compared in a cluster randomized trial across 28 primary care clinics. Both alerts aimed to improve beta-blocker prescribing for heart failure. The enhanced alert was informed by CDS best practices and the Practical, Robust, Implementation, and Sustainability Model (PRISM) IS framework, whereas the commercial alert followed vendor-supplied specifications. Following PRISM, the enhanced alert was informed by iterative, multilevel stakeholder input and the dynamic interactions of the internal and external environment. Outcomes aligned with PRISM's evaluation measures, including patient reach, clinician adoption, and changes in prescribing behavior. Clinicians exposed to each alert were interviewed to identify design features that might influence adoption. The interviews were analyzed using a thematic approach. Between March 15 and August 23, 2019, the enhanced alert fired for 61 patients (106 alerts, 87 clinicians) and the commercial alert fired for 26 patients (59 alerts, 31 clinicians). The adoption and effectiveness of the enhanced alert were significantly higher than those of the commercial alert (62% vs 29% alerts adopted, P<.001; 14% vs 0% changed prescribing, P=.006). Of the 21 clinicians interviewed, most stated that they preferred the enhanced alert. The results of this study suggest that applying CDS best practices with an IS framework to create CDS tools improves implementation success compared with a commercially available tool. ClinicalTrials.gov NCT04028557; http://clinicaltrials.gov/ct2/show/NCT04028557.",success
33577160,False,Journal Article,,,,,,,,True,"The evidence supporting early postdischarge hospital follow-up is limited. We implemented a new, multidisciplinary, multistrategy heart failure (HF) team approach that included new clinic slots, predischarge nurse visit, providing a blood pressure cuff and scale, and cardiologist supervision. Pre- vs postintervention evaluation of outcomes in patients hospitalized with HF between September 1, 2010, and May 30, 2013. We utilized the RE-AIM (reach, effectiveness, adoption, implementation, maintenance) framework to evaluate the intervention. For the quantitative evaluation, we compared the proportion of patients in both groups who were scheduled for and completed a cardiology appointment within 7 days after hospitalization (""reach""). We created a Cox model to evaluate the ""effectiveness"" of the intervention period on a 30-day composite outcome (all-cause emergency department [ED] visit, all-cause hospitalization, or death). In qualitative evaluation, we describe the adoption, implementation, and maintenance of the intervention. Data for 261 patients were analyzed (preintervention, n = 142; post intervention, n = 119). The postintervention period was associated with a higher proportion of patients who were referred to (40% vs 12%; P < .001) and completed (24% vs 10%; P = .003) cardiology follow-up within 7 days of hospital discharge (reach) compared with the preintervention period. After adjustment, the postintervention period was associated with a reduced hazard of the 30-day composite end point (HR, 0.59; 95% CI, 0.37-0.96; P = .04) (effectiveness). The intervention succeeded in increasing referral to and completion of cardiology appointments within 7 days of discharge. In adjusted analysis, the intervention was associated with lower risk of 30-day all-cause ED visits, all-cause hospitalizations, or death.",success
32428430,False,Journal Article,NCT02344576,databank,NCT02344576,NCT02344576,NCT02344576,NCT02344576|databank,NCT02344576|databank,True,"<b>Background.</b> Despite demonstrated efficacy, patient decision aids (DAs) are rarely used in clinical practice in the absence of coverage mandates. Deciding whether to pursue a left ventricular assist device (LVAD) is a major, preference-sensitive decision-ideal for exploring implementation of a DA. <b>Methods.</b> We conducted a type II effectiveness-implementation hybrid trial at 6 LVAD programs using a stepped-wedge cluster-randomized design. Using the RE-AIM framework, we collected both quantitative and qualitative outcomes, including a checklist collected by study staff for each enrolled patient regarding DA use and interviews with LVAD program clinicians preintervention, 6 months postintervention, and at the conclusion of the study. <b>Results.</b> From June 2015 to January 2017, 248 patients and their caregivers were enrolled. A total of 69 interviews were conducted with 48 clinicians at 3 time points. The DA reached 95% of eligible patients. Adoption was 100%, as all sites approached agreed to participate in the trial. Interviews revealed several themes related to the implementation of the DA: clinicians had a strong desire to ensure patients were informed and embraced the DA. Despite this, they reported communication challenges among their team that impeded implementation. Five of the 6 sites have maintained use of the DA following the trial; 1 site reported concerns about decreased procedural volume with use of the DA as a reason for discontinuation. <b>Conclusions.</b> In this hybrid trial, a DA for patients considering LVADs and their caregivers demonstrated high reach. Adoption and implementation were facilitated by a strong desire to ensure that patients were well informed. Future dissemination research and practice should attend to concerns about procedure volume and coverage mandates and facilitate ongoing communication at sites using the DA.",success
29482225,True,"Journal Article;Randomized Controlled Trial;Research Support, N.I.H., Extramural",NCT02344576,databank,NCT02344576,NCT02344576,NCT02344576,NCT02344576|databank,NCT02344576|databank,True,"Shared decision making helps patients and clinicians elect therapies aligned with patients' values and preferences. This is particularly important for invasive therapies with considerable trade-offs. To assess the effectiveness of a shared decision support intervention for patients considering destination therapy left ventricular assist device (DT LVAD) placement. From 2015 to 2017, a randomized, stepped-wedge trial was conducted in 6 US LVAD implanting centers including 248 patients being considered for DT LVAD. After randomly varying time in usual care, sites were transitioned to an intervention consisting of clinician education and use of DT LVAD pamphlet and video patient decision aids. Follow up occurred at 1 and 6 months. Decision quality as measured by knowledge and values-choice concordance. In total, 135 patients were enrolled during control and 113 during intervention periods. At enrollment, 59 (23.8%) participants were in intensive care, 60 (24.1%) were older than 70 years, 39 (15.7%) were women, 45 (18.1%) were racial/ethnic minorities, and 62 (25.0%) were college graduates. Patient knowledge (mean test performance) during the decision-making period improved from 59.5% to 64.9% in the control group vs 59.1% to 70.0% in the intervention group (adjusted difference of difference, 5.5%; P = .03). Stated values at 1 month (scale 1 = ""do everything I can to live longer…"" to 10 = ""live with whatever time I have left…"") were a mean of 2.37 in control and 3.33 in intervention (P = .03). Patient-reported treatment choice at 1 month favored LVAD more in the control group (than in the intervention group (47 [59.5%] vs 95 [91.3%], P < .001). Correlation between stated values and patient-reported treatment choice at 1 month was stronger in the intervention group than in the control group (difference in Kendall's τ, 0.28; 95% CI, 0.05-0.45); however, there was no improved correlation between stated values and actual treatment received by 6 months for the intervention compared with the control group (difference in Kendall's τ, 0.01; 95% CI, -0.24 to 0.25). The adjusted rate of LVAD implantation by 6 months was higher for those in the control group (79.9%) than those in the intervention group (53.9%, P = .008), with significant variation by site. There were no differences in decision conflict, decision regret, or preferred control. A shared decision-making intervention for DT LVAD modestly improved patient decision quality as measured by patient knowledge and concordance between stated values and patient-reported treatment choice, but did not improve concordance between stated values and actual treatment received. The rate of implantation of LVADs was substantially lower in the intervention compared with the control group. clinicaltrials.gov Identifier: NCT02344576.",success
32421373,False,Journal Article,,,,,,,,True,"Use of hospice has grown among patients with heart failure; however, gaps remain in the ability of agencies to tailor services to meet their needs. This study describes the implementation of a cardiac home hospice program and insights for dissemination to other hospice programs. We conducted a multimethod analysis structured around the Reach Effectiveness Adoption Implementation and Maintenance (RE-AIM) framework. We used electronic medical records for our quantitative data source and interviews with hospice clinicians from a not-for-profit hospice agency (N = 32) for our qualitative data source. Reach-A total of 1273 participants were enrolled in the cardiac home hospice program, of which 57% were female and 42% were black or Hispanic with a mean age was 86 years. Effectiveness-The cardiac home hospice program increased hospice enrollment among patients with heart failure from 7.9% to 9.5% over 1 year (2016-2017). Adoption-Institutional factors that supported the program included the acute need to support medically complex patients at the end of life and an engaged clinical champion. Implementation-Program implementation was supported by interdisciplinary teams who engaged in care coordination. Maintenance-The program has been maintained for over 3 years. The cardiac home hospice program strengthened hospice clinicians' ability to confidently provide care for patients with heart failure, expanded awareness of their symptoms among clinicians, and was associated with increased enrollment of patients with heart failure over the study period. This RE-AIM evaluation provides lessons learned and strategies for future adoption, implementation, and maintenance of a cardiac home hospice program.",success
33147091,False,Journal Article,,,,,,,,True,"Despite the existence of expert recommendations that can improve morbidity and mortality, reduce the need for hospitalization or readmission, and enhance quality of life in patients with heart failure (HF), many patients do not receive optimal medical therapy (OMT). The goal of this initiative, titled RightSTEPS, was to help physicians take the right steps to apply-evidence-based HF management strategies in clinical practice. Using the PRECEDE-PROCEED Model aimed at improving the clinical behavior of the learner, the instructional design featured 23 online and live face-to-face activities offering up to 16 credit hours of CME/CNE credit. These activities were delivered sequentially in three phases: predisposing, enabling and reinforcing. The lessons provided concise, pragmatic, stepwise management strategies aimed at empowering clinicians to prescribe evidence-based, guideline-directed OMT for patients with HF. The predisposing and reinforcing online activities within the initiative reached a total of 71,510 learners with 23,902 successfully completed activities and post-tests; the enabling face-to-face activities reached a total audience of 763 clinicians. This initiative resulted in a statistically significant (<i>p</i> < 0.0001) increase in knowledge and competence related to HF OMT among the clinician learners. Furthermore, follow-up surveys indicated a commitment from learners to implement these guideline-directed strategies in their clinical practice. This initiative demonstrated that the design of the RightSTEPS curriculum, using the Precede-Proceed model with sequentially-delivered, blended learning, provides a methodological framework to help learners translate knowledge into improvements in clinical behavior with the potential to improve patient health outcomes.",success
30064970,False,Journal Article,,,,,,,,True,"Telemonitoring has shown promise for alleviating the burden of heart failure on individuals and health systems. However, real-world implementation of sustained programs is rare. The objective of this study was to evaluate the implementation of a mobile phone-based telemonitoring program, which has been implemented as part of standard care in a specialty heart function clinic by answering two research questions: (1) To what extent was the telemonitoring program successfully implemented? (2) What were the barriers and facilitators to implementing the telemonitoring program? We conducted a longitudinal single case study. The implementation success was evaluated using the following four implementation outcomes: adoption, penetration, feasibility, and fidelity. Semistructured interviews based on the Consolidated Framework for Implementation Research (CFIR) were conducted at 0, 4, and 12 months with 12 program staff members to identify the barriers and facilitators of the implementation. One year after the implementation, 98 patients and 8 clinicians were enrolled in the program. Despite minor technical issues, the intervention was used as intended. We obtained qualitative data from clinicians (n=8) and implementation staff members (n=4) for 24 CFIR constructs. A total of 12 constructs were facilitators clustered in the CFIR domains of inner setting (culture, tension for change, compatibility, relative priority, learning climate, leadership engagement, and available resources), characteristics of individuals (knowledge and beliefs about the intervention and self-efficacy), and process (engaging and reflecting and evaluating). In addition, we identified other notable facilitators from the characteristics of the intervention domain (relative advantage and adaptability) and the outer setting (patient needs and resources). Four constructs were perceived as minor barriers- the complexity of the intervention, cost, inadequate communication among high-level stakeholders, and the absence of a formal implementation plan. The remaining CFIR constructs had a neutral impact on the overall implementation. This is the first comprehensive evaluation of the implementation of a mobile phone-based telemonitoring program. Although the acceptability of the telemonitoring system was high, the strongest facilitators to the implementation success were related to the implementation context. By identifying what works and what does not in a real-world clinical context using a framework-guided approach, this work will inform the design of telemonitoring services and implementation strategies of similar telemonitoring interventions.",success
22356799,True,"Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",NCT00778986,databank,NCT00778986,NCT00778986,NCT00778986,NCT00778986|databank,NCT00778986|databank,True,"Previous trials of telemonitoring for heart failure management have reported inconsistent results, largely due to diverse intervention and study designs. Mobile phones are becoming ubiquitous and economical, but the feasibility and efficacy of a mobile phone-based telemonitoring system have not been determined. The objective of this trial was to investigate the effects of a mobile phone-based telemonitoring system on heart failure management and outcomes. One hundred patients were recruited from a heart function clinic and randomized into telemonitoring and control groups. The telemonitoring group (N = 50) took daily weight and blood pressure readings and weekly single-lead ECGs, and answered daily symptom questions on a mobile phone over 6 months. Readings were automatically transmitted wirelessly to the mobile phone and then to data servers. Instructions were sent to the patients' mobile phones and alerts to a cardiologist's mobile phone as required. Baseline questionnaires were completed and returned by 94 patients, and 84 patients returned post-study questionnaires. About 70% of telemonitoring patients completed at least 80% of their possible daily readings. The change in quality of life from baseline to post-study, as measured with the Minnesota Living with Heart Failure Questionnaire, was significantly greater for the telemonitoring group compared to the control group (P = .05). A between-group analysis also found greater post-study self-care maintenance (measured with the Self-Care of Heart Failure Index) for the telemonitoring group (P = .03). Brain natriuretic peptide (BNP) levels, self-care management, and left ventricular ejection fraction (LVEF) improved significantly for both groups from baseline to post-study, but did not show a between-group difference. However, a subgroup within-group analysis using the data from the 63 patients who had attended the heart function clinic for more than 6 months revealed the telemonitoring group had significant improvements from baseline to post-study in BNP (decreased by 150 pg/mL, P = .02), LVEF (increased by 7.4%, P = .005) and self-care maintenance (increased by 7 points, P = .05) and management (increased by 14 points, P = .03), while the control group did not. No differences were found between the telemonitoring and control groups in terms of hospitalization, mortality, or emergency department visits, but the trial was underpowered to detect differences in these metrics. Our findings provide evidence of improved quality of life through improved self-care and clinical management from a mobile phone-based telemonitoring system. The use of the mobile phone-based system had high adherence and was feasible for patients, including the elderly and those with no experience with mobile phones. ClinicalTrials.gov NCT00778986.",success
15252415,True,"Clinical Trial;Journal Article;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",,,,,,,,True,"The purpose of this study was to determine the effect of a tailored message intervention on heart failure readmission rates, quality of life, and health beliefs in persons with heart failure (HF). This randomized control trial provided a tailored message intervention during hospitalization and 1 week and 1 month after discharge.Theoretic framework The organizing framework was the Health Belief Model. Seventy persons with a primary diagnosis of chronic HF were included in the study. HF readmission rates and quality of life did not significantly differ between the treatment and control groups. Health beliefs, except for benefits of medications, significantly changed from baseline in the treatment group in directions posited by the Health Belief Model. A tailored message intervention changed the beliefs of the person with HF in regard to the benefits and barriers of taking medications, following a sodium-restricted diet, and self-monitoring for signs of fluid overload. Future research is needed to explore the effect of health belief changes on actual self-care behaviors.",success
22196835,True,"Journal Article;Randomized Controlled Trial;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Medication nonadherence contributes to hospitalization and mortality, yet there have been few interventions tested that improve adherence and reduce hospitalization and mortality in heart failure (HF). Our objective was to determine whether an education intervention improved medication adherence and cardiac event-free survival. A randomized controlled trial was conducted on 82 HF patients. The intervention was based on the theory of planned behavior (TPB) and included feedback of medication-taking behavior using the Medication Event Monitoring System (MEMS). Patients were assigned to one of three groups: 1) theory-based education plus MEMS feedback; 2) theory-based education only; or 3) usual care (control). Cardiac events were collected for 9 months. Patients in both intervention groups were more adherent over follow-up compared with the control group. In Cox regression, patients in either intervention group had a longer event-free survival compared with those in the control group before and after controlling age, marital status, financial status, ejection fraction, New York Heart Association functional class, angiotensin-converting enzyme inhibitor use, and presence or absence of a significant other during the intervention (P < .05). Use of an intervention based on the TPB improves medication adherence and outcomes in patients with HF and therefore offers promise as a clinically applicable intervention to help patients with HF to adhere to their prescribed regimen.",success
26400781,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,CD000313;CD003030;CD003329,,,,,True,"Training programmes in evidence-based practice (EBP) frequently fail to translate their content into practice change and care improvement. We linked multidisciplinary training in EBP to an initiative to decrease 30-day readmissions among patients admitted to a community teaching hospital for heart failure (HF). Hospital staff reflecting all services and disciplines relevant to care of patients with HF attended a 3-day innovative capacity building conference in evidence-based health care over a 3-year period beginning in 2009. The team, facilitated by a conference faculty member, applied a knowledge-to-action model taught at the conference. We reviewed published research, profiled our population and practice experience, developed a three-phase protocol and implemented it in late 2010. We tracked readmission rates, adverse clinical outcomes and programme cost. The protocol emphasized patient education, medication reconciliation and transition to community-based care. Senior administration approved a full-time nurse HF coordinator. Thirty-day HF readmissions decreased from 23.1% to 16.4% (adjusted OR = 0.64, 95% CI = 0.42-0.97) during the year following implementation. Corresponding rates in another hospital serving the same population but not part of the programme were 22.3% and 20.2% (adjusted OR = 0.87, 95% CI = 0.71-1.08). Adherence to mandated HF quality measures improved. Following a start-up cost of $15 000 US, programme expenses balanced potential savings from decreased HF readmissions. Training of a multidisciplinary hospital team in use of a knowledge translation model, combined with ongoing facilitation, led to implementation of a budget neutral programme that decreased HF readmissions.",success
25889199,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Identifying, developing, and testing implementation strategies are important goals of implementation science. However, these efforts have been complicated by the use of inconsistent language and inadequate descriptions of implementation strategies in the literature. The Expert Recommendations for Implementing Change (ERIC) study aimed to refine a published compilation of implementation strategy terms and definitions by systematically gathering input from a wide range of stakeholders with expertise in implementation science and clinical practice. Purposive sampling was used to recruit a panel of experts in implementation and clinical practice who engaged in three rounds of a modified Delphi process to generate consensus on implementation strategies and definitions. The first and second rounds involved Web-based surveys soliciting comments on implementation strategy terms and definitions. After each round, iterative refinements were made based upon participant feedback. The third round involved a live polling and consensus process via a Web-based platform and conference call. Participants identified substantial concerns with 31% of the terms and/or definitions and suggested five additional strategies. Seventy-five percent of definitions from the originally published compilation of strategies were retained after voting. Ultimately, the expert panel reached consensus on a final compilation of 73 implementation strategies. This research advances the field by improving the conceptual clarity, relevance, and comprehensiveness of implementation strategies that can be used in isolation or combination in implementation research and practice. Future phases of ERIC will focus on developing conceptually distinct categories of strategies as well as ratings for each strategy's importance and feasibility. Next, the expert panel will recommend multifaceted strategies for hypothetical yet real-world scenarios that vary by sites' endorsement of evidence-based programs and practices and the strength of contextual supports that surround the effort.",success
26249843,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Poor terminological consistency for core concepts in implementation science has been widely noted as an obstacle to effective meta-analyses. This inconsistency is also a barrier for those seeking guidance from the research literature when developing and planning implementation initiatives. The Expert Recommendations for Implementing Change (ERIC) study aims to address one area of terminological inconsistency: discrete implementation strategies involving one process or action used to support a practice change. The present report is on the second stage of the ERIC project that focuses on providing initial validation of the compilation of 73 implementation strategies that were identified in the first phase. Purposive sampling was used to recruit a panel of experts in implementation science and clinical practice (N = 35). These key stakeholders used concept mapping sorting and rating activities to place the 73 implementation strategies into similar groups and to rate each strategy's relative importance and feasibility. Multidimensional scaling analysis provided a quantitative representation of the relationships among the strategies, all but one of which were found to be conceptually distinct from the others. Hierarchical cluster analysis supported organizing the 73 strategies into 9 categories. The ratings data reflect those strategies identified as the most important and feasible. This study provides initial validation of the implementation strategies within the ERIC compilation as being conceptually distinct. The categorization and strategy ratings of importance and feasibility may facilitate the search for, and selection of, strategies that are best suited for implementation efforts in a particular setting.",success
36212610,False,Journal Article,,,,,,,,True,"Traditional research focuses on efficacy or effectiveness of interventions but lacks evaluation of strategies needed for equitable uptake, scalable implementation, and sustainable evidence-based practice transformation. The purpose of this introductory review is to describe key implementation science (IS) concepts as they apply to medication management and pharmacy practice, and to provide guidance on literature review with an IS lens. There are five key ingredients of IS, including: (1) evidence-based intervention; (2) implementation strategies; (3) IS theory, model, or framework; (4) IS outcomes and measures; and (5) stakeholder engagement, which is key to a successful implementation. These key ingredients apply across the three stages of IS research: (1) pre-implementation; (2) implementation; and (3) sustainment. A case example using a combination of IS models, PRISM (Practical, Robust Implementation and Sustainability model) and RE-AIM (Reach, Effectiveness, Adoption, Implementation, Maintenance), is included to describe how an IS study is designed and conducted. This case is a cluster randomized trial comparing two clinical decision support tools to improve guideline-concordant prescribing for patients with heart failure and reduced ejection fraction. The review also includes information on the Standards for Reporting Implementation Studies (StaRI), which is used for literature review and reporting of IS studies,as well as IS-related learning resources.",success
36378772,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Despite their unique contributions to heart failure (HF) care, home healthcare workers (HHWs) have unmet educational needs and many lack HF caregiving self-efficacy. To address this, we used a community-partnered approach to develop and pilot a HF training course for HHWs. We partnered with the Training and Employment Fund, a benefit fund of the largest healthcare union in the United States, to develop a 2-hour virtual HF training course that met HHWs' job-specific needs. English and Spanish-speaking HHWs interested in HF training, with access to Zoom, were eligible. We used a mixed methods design with pre/postsurveys and semi-structured interviews to evaluate the course: (a) feasibility, (b) acceptability, and (c) effectiveness (change in knowledge [Dutch Heart Failure Knowledge Scale range 0-15] and caregiving self-efficacy [HF Caregiver Self-efficacy Scale range 0-100]). Of the 210 HHWs approached, 100 were eligible and agreed, and 70 enrolled. Of them, 53 (employed by 15 different home care agencies) participated. Posttraining data showed significant improvements (pretraining mean [SD] versus posttraining mean [SD]; <i>P</i> value) in HF knowledge (11.21 [1.90] versus 12.21 [1.85]; <i>P</i>=0.0000) and HF caregiving self-efficacy (75.21 [16.57] versus 82.29 [16.49]; <i>P</i>=0.0017); the greatest gains occurred among those with the lowest pre-training scores. Participants found the course engaging, technically feasible, and highly relevant to their scope of care. We developed and piloted the first HF training course for HHWs, which was feasible, acceptable, and improved their HF knowledge and caregiving self-efficacy. Our findings warrant scalability to the workforce at large with a train-the-trainer model.",success
33564738,False,Journal Article,,,,,,,,True,"<b>Purpose:</b> Heart failure (HF) disproportionately impacts African Americans. We evaluated existing quality improvement (QI) initiatives and patient and provider perceptions of barriers to HF care to develop equity-centered QI recommendations. <b>Methods:</b> We performed a literature review, interviewed providers and patients (<i>N</i>=11), and conducted a root cause analysis at a safety net hospital in San Francisco, California. <b>Results:</b> We have identified four elements to foster a more equitable HF care model: screening for social determinants of health, technological innovation, optimization of space, and implicit bias training. <b>Conclusion:</b> QI initiatives for HF should integrate health equity elements in their design and implementation.",success
36846988,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,True,"Background US regulatory framework for advanced heart failure therapies (AHFT), ventricular assist devices, and heart transplants, delegate eligibility decisions to multidisciplinary groups at the center level. The subjective nature of decision-making is at risk for racial, ethnic, and gender bias. We sought to determine how group dynamics impact allocation decision-making by patient gender, racial, and ethnic group. Methods and Results We performed a mixed-methods study among 4 AHFT centers. For ≈ 1 month, AHFT meetings were audio recorded. Meeting transcripts were evaluated for group function scores using de Groot Critically Reflective Diagnoses protocol (metrics: challenging groupthink, critical opinion sharing, openness to mistakes, asking/giving feedback, and experimentation; scoring: 1 to 4 [high to low quality]). The relationship between summed group function scores and AHFT allocation was assessed via hierarchical logistic regression with patients nested within meetings nested within centers, and interaction effects of group function score with gender and race, adjusting for patient age and comorbidities. Among 87 patients (24% women, 66% White race) evaluated for AHFT, 57% of women, 38% of men, 44% of White race, and 40% of patients of color were allocated to AHFT. The interaction between group function score and allocation by patient gender was statistically significant (<i>P</i>=0.035); as group function scores improved, the probability of AHFT allocation increased for women and decreased for men, a pattern that was similar irrespective of racial and ethnic groups. Conclusions Women evaluated for AHFT were more likely to receive AHFT when group decision-making processes were of higher quality. Further investigation is needed to promote routine high-quality group decision-making and reduce known disparities in AHFT allocation.",success
20961442,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"The past decade has seen considerable interest in the development and evaluation of complex interventions to improve health. Such interventions can only have a significant impact on health and health care if they are shown to be effective when tested, are capable of being widely implemented and can be normalised into routine practice. To date, there is still a problematic gap between research and implementation. The Normalisation Process Theory (NPT) addresses the factors needed for successful implementation and integration of interventions into routine work (normalisation). In this paper, we suggest that the NPT can act as a sensitising tool, enabling researchers to think through issues of implementation while designing a complex intervention and its evaluation. The need to ensure trial procedures that are feasible and compatible with clinical practice is not limited to trials of complex interventions, and NPT may improve trial design by highlighting potential problems with recruitment or data collection, as well as ensuring the intervention has good implementation potential. The NPT is a new theory which offers trialists a consistent framework that can be used to describe, assess and enhance implementation potential. We encourage trialists to consider using it in their next trial.",success
30866982,True,"Journal Article;Multicenter Study;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Researchers could benefit from methodological advancements to advance uptake of new treatments while also reducing healthcare disparities. A comprehensive determinants framework for healthcare disparity implementation challenges is essential to accurately understand an implementation problem and select implementation strategies. We integrated and modified two conceptual frameworks-one from implementation science and one from healthcare disparities research to develop the Health Equity Implementation Framework. We applied the Health Equity Implementation Framework to a historical healthcare disparity challenge-hepatitis C virus (HCV) and its treatment among Black patients seeking care in the US Department of Veterans Affairs (VA). A specific implementation assessment at the patient level was needed to understand any barriers to increasing uptake of HCV treatment, independent of cost. We conducted a preliminary study to assess how feasible it was for researchers to use the Health Equity Implementation Framework. We applied the framework to design the qualitative interview guide and interpret results. Using quantitative data to screen potential participants, this preliminary study consisted of semi-structured interviews with a purposively selected sample of Black, rural-dwelling, older adult VA patients (N = 12), living with HCV, from VA medical clinics in the Southern part of the USA. The Health Equity Implementation Framework was feasible for implementation researchers. Barriers and facilitators were identified at all levels including the patient, provider (recipients), patient-provider interaction (clinical encounter), characteristics of treatment (innovation), and healthcare system (inner and outer context). Some barriers reflected general implementation issues (e.g., poor care coordination after testing positive for HCV). Other barriers were related to healthcare disparities and likely unique to racial minority patients (e.g., testimonials from Black peers about racial discrimination at VA). We identified several facilitators, including patient enthusiasm to obtain treatment because of its high cure rates, and VA clinics that offset HCV stigma by protecting patient confidentiality. The Health Equity Implementation Framework showcases one way to modify an implementation framework to better assess health equity determinants as well. Researchers may be able to optimize the scientific yield of research inquiries by identifying and addressing factors that promote or impede implementation of novel treatments in addition to eliminating healthcare disparities.",success
28806362,False,"Journal Article;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Health disparities are differences in health or health care between groups based on social, economic, and/or environmental disadvantage. Disparity research often follows 3 steps: detecting (phase 1), understanding (phase 2), and reducing (phase 3), disparities. Although disparities have narrowed over time, many remain. We argue that implementation science could enhance disparities research by broadening the scope of phase 2 studies and offering rigorous methods to test disparity-reducing implementation strategies in phase 3 studies. We briefly review the focus of phase 2 and phase 3 disparities research. We then provide a decision tree and case examples to illustrate how implementation science frameworks and research designs could further enhance disparity research. Most health disparities research emphasizes patient and provider factors as predominant mechanisms underlying disparities. Applying implementation science frameworks like the Consolidated Framework for Implementation Research could help disparities research widen its scope in phase 2 studies and, in turn, develop broader disparities-reducing implementation strategies in phase 3 studies. Many phase 3 studies of disparity-reducing implementation strategies are similar to case studies, whose designs are not able to fully test causality. Implementation science research designs offer rigorous methods that could accelerate the pace at which equity is achieved in real-world practice. Disparities can be considered a ""special case"" of implementation challenges-when evidence-based clinical interventions are delivered to, and received by, vulnerable populations at lower rates. Bringing together health disparities research and implementation science could advance equity more than either could achieve on their own.",success
31072580,False,"Letter;Research Support, N.I.H., Extramural",,,,,,,,False,,success
33111032,False,Journal Article,,,,,,,,True,"The coronavirus disease 2019 (COVID-19) pandemic is disproportionally affecting racial and ethnic minorities. In the United States, data show African American, Hispanic, and Native American populations are overrepresented among COVID-19 cases and deaths. As we speed through the discovery and translation of approaches to fight COVID-19, these disparities are likely to increase. Implementation science can help address disparities by guiding the equitable development and deployment of preventive interventions, testing, and, eventually, treatment and vaccines. In this study, we discuss three ways in which implementation science can inform these efforts: (1) quantify and understand disparities; (2) design equitable interventions; and (3) test, refine, and retest interventions.",success
26893401,False,Journal Article;Review,,,,,,,,True,"A key reason for the consistent gaps between evidence and practice across all areas of medicine is that there has been little attempt to identify or target factors critical for successful implementation of an evidence-based intervention. There is either no explicit implementation strategy or the strategy is based on a best guess rather than on a systematic assessment of crucial barriers and enablers. A different approach is needed to close the evidence-practice gap and thereby achieve the triple aim of improved health, improved patient experience and reduced healthcare costs. We present three fundamental principles of implementation science, which is a methodology that offers a systematic and comprehensive approach to improving healthcare practice and a series of 'how to' steps to conduct implementation science research. In an accompanying article, a scoping review of the types of implementation science research conducted in emergency medicine is reviewed, and several of the principles related to this review are discussed.",success
37431671,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",,,,,,,,True,"Coverage for cardiac rehabilitation (CR) for patients with heart failure with reduced ejection fraction was expanded in 2014, but contemporary referral and participation rates remain unknown. Patients hospitalized for heart failure with reduced ejection fraction (≤35%) in the American Heart Association Get With The Guidelines-Heart Failure registry from 2010 to 2020 were included, and CR referral status was described as yes, no, or not captured. Temporal trends in CR referral were assessed in the overall cohort. Patient and hospital-level predictors of CR referral were assessed using multivariable-adjusted logistic regression models. Additionally, CR referral and proportional utilization of CR within 1-year of referral were evaluated among patients aged >65 years with available Medicare administrative claims data who were clinically stable for 6-weeks postdischarge. Finally, the association of CR referral with the risk of 1-year death and readmission was evaluated using multivariable-adjusted Cox models. Of 69,441 patients with heart failure with reduced ejection fraction who were eligible for CR (median age 67 years; 33% women; 30% Black), 17,076 (24.6%) were referred to CR, and referral rates increased from 8.1% in 2010 to 24.1% in 2020 (<i>P</i><sub>trend</sub><0.001). Of 8310 patients with Medicare who remained clinically stable 6-weeks after discharge, the CR referral rate was 25.8%, and utilization of CR among referred patients was 4.1% (mean sessions attended: 6.7). Patients not referred were more likely to be older, of Black race, and with a higher burden of comorbidities. In adjusted analysis, eligible patients with heart failure with reduced ejection fraction who were referred to CR (versus not referred) had a lower risk of 1-year death (hazard ratio, 0.84 [95% CI, 0.70-1.00]; <i>P</i>=0.049) without significant differences in 1-year readmission. CR referral rates have increased from 2010 to 2020. However, only 1 in 4 patients are referred to CR. Among eligible patients who received CR referral, participation was low, with <1 of 20 participating in CR.",success
34911363,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Heart failure has a disproportionate burden on patients who are Black, Indigenous, and people of color (BIPOC), but not much is known about representation of these groups in randomized controlled trials (RCTs). We explored temporal trends in and RCT factors associated with the reporting of race and ethnicity data and the enrollment of BIPOC in heart failure RCTs. We searched MEDLINE, EMBASE, and CINAHL for heart failure RCTs published in journals with an impact factor ≥10 between January 1, 2000 and June 17, 2020. We used the Cochran-Armitage and Jonchkeere-Terpstra tests to examine temporal trends, and multivariable regression to assess the association between trial characteristics and outcomes. Of 414 RCTs meeting inclusion criteria, only 157 (37.9% [95% CI, 33.2%-2.8%]) reported race and ethnicity data. Among 158 200 participants in these 157 RCTs, 29 512 (18.7% [95% CI, 18.5%-18.9%]) were BIPOC. There was a temporal increase in reporting of race and ethnicity data (29.5% in 2000-2003 to 54.7% in 2016-2020, <i>P</i><0.001) and in enrollment of BIPOC (14.4% in 2000-2003 to 22.2% in 2016-2020, <i>P</i>=0.038). Trial leadership by a woman was independently associated with twice the odds of reporting race and ethnicity data (odds ratio, 2.0 [95% CI, 1.1-3.8]; <i>P</i>=0.028) and an 8.4% increase (95% CI, 1.9%-15.0%; <i>P</i>=0.013) in BIPOC enrollment. A minority of heart failure RCTs reported race and ethnicity data, and among these, BIPOC were under-enrolled relative to disease distribution. Both reporting of race and ethnicity as well as enrollment of BIPOC increased between 2000 and 2020. After multivariable adjustment, trials led by women had greater odds of reporting race and ethnicity and enrolling BIPOC. URL: https://www.crd.york.ac.uk/PROSPERO/; Unique identifier: CRD42021237497.",success
35150324,False,Journal Article;Review,,,,,,,,True,"As the long-standing and ubiquitous racial inequities of the United States reached national attention, the public health community has witnessed the rise of ""health equity tourism"". This phenomenon is the process of previously unengaged investigators pivoting into health equity research without developing the necessary scientific expertise for high-quality work. In this essay, we define the phenomenon and provide an explanation of the antecedent conditions that facilitated its development. We also describe the consequences of health equity tourism - namely, recapitulating systems of inequity within the academy and the dilution of a landscape carefully curated by scholars who have demonstrated sustained commitments to equity research as a primary scientific discipline and praxis. Lastly, we provide a set of principles that can guide novice equity researchers to becoming community members rather than mere tourists of health equity.",success
37054773,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Diversity in clinical trials (CTs) has the potential to improve health equity and close health disparities. Underrepresentation of historically underserved groups compromises the generalizability of trial findings to the target population, hinders innovation, and contributes to low accrual. The aim of this study was to establish a transparent and reproducible process for setting trial diversity enrollment goals informed by the disease epidemiology. An advisory board of epidemiologists with expertise in health disparities, equity, diversity, and social determinants of health was convened to evaluate and strengthen the initial goal-setting framework. Data sources used were the epidemiologic literature, US Census, and real-world data (RWD); limitations were considered and addressed where appropriate. A framework was designed to safeguard against the underrepresentation of historically medically underserved groups. A stepwise approach was created with Y/N decisions based on empirical data. We compared race and ethnicity distributions in the RWD of six diseases from Pfizer's portfolio chosen to represent different therapeutic areas (multiple myeloma, fungal infections, Crohn's disease, Gaucher disease, COVID-19, and Lyme disease) to the distributions in the US Census and established trial enrollment goals. Enrollment goals for potential CTs were based on RWD for multiple myeloma, Gaucher disease, and COVID-19; enrollment goals were based on the Census for fungal infections, Crohn's disease, and Lyme disease. We developed a transparent and reproducible framework for setting CT diversity enrollment goals. We note how limitations due to data sources can be mitigated and consider several ethical decisions in setting equitable enrollment goals.",success
33567860,False,"Editorial;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The Fish. The Pond. The Groundwater. Imagine that you have a personal pond filled with fish. When viewing your pond, you notice that one fish has died, floating belly-up. You decide that the fish must have been ill and think nothing more of it. The next day, you notice that half of the fish in your pond are now dead. You are alarmed and decide to contact the neighborhood management services to investigate your pond. Something must be wrong with the local system. The following day, however, you discover that all of your neighbors with ponds have noticed the same thing. In fact, half of the fish are dead throughout all waterways in the entire state. At this point, it is clear something deeper must be wrong. This is when you need to analyze the groundwater feeding these ponds. The fish are not at fault, and not even the local systems. Rather the underlying structures through which the fish seek life has failed. Imagine that instead of fish, we are discussing patients. —Paraphrase of Groundwater Approach Metaphor by Love and Hayes-Greene of The Racial Equity Institute.",success
36302590,False,"Journal Article;Review;Research Support, N.I.H., Extramural",,,,,,,,True,"Black women are disproportionately affected by cardiovascular disease with an excess burden of cardiovascular morbidity and mortality. In addition, the racialized structure of the United States shapes cardiovascular disease research and health care delivery for Black women. Given the indisputable evidence of the disparities in health care delivery, research, and cardiovascular outcomes, there is an urgent need to develop and implement effective and sustainable solutions to advance cardiovascular health equity for Black women while considering their ethnic diversity, regions of origin, and acculturation. Innovative and culturally tailored strategies that consider the differential impact of social determinants of health and the unique challenges that shape their health-seeking behaviors should be implemented. A patient-centered framework that involves collaboration among clinicians, health care systems, professional societies, and government agencies is required to improve cardiovascular outcomes for Black women. The time is ""now"" to achieve health equity for all Black women.",success
35341539,False,Journal Article;Review,,,,,,,,True,"Patients with heart failure (HF) are heterogeneous with various intrapersonal and interpersonal characteristics contributing to clinical outcomes. Bias, structural racism, and social determinants of health have been implicated in unequal treatment of patients with HF. Through several methodologies, artificial intelligence (AI) can provide models in HF prediction, prognostication, and provision of care, which may help prevent unequal outcomes. This review highlights AI as a strategy to address racial inequalities in HF; discusses key AI definitions within a health equity context; describes the current uses of AI in HF, strengths and harms in using AI; and offers recommendations for future directions.",success
37756500,True,"Randomized Controlled Trial;Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Recognition that cultural stereotypes can unintentionally perpetuate inequities throughout academic medicine has led to calls for ""implicit bias training"" without strong evidence to support these recommendations and some evidence of potential harm. The authors sought to determine the effectiveness of a single 3-hour workshop in helping department of medicine faculty overcome implicit stereotype-based bias and in improving the climate in the working environment. A multisite cluster randomized controlled study (October 2017 to April 2021) with clustering at the level of divisions within departments and participant-level analysis of survey responses involved 8,657 faculty in 204 divisions in 19 departments of medicine: 4,424 in the intervention group (1,526 attended a workshop) and 4,233 in the control group. Online surveys at baseline (3,764/8,657 = 43.48% response rate) and 3 months after the workshop (2,962/7,715 = 38.39% response rate) assessed bias awareness, bias-reducing intentional behavioral change, and perceptions of division climate. At 3 months, faculty in the intervention vs control divisions showed greater increases in awareness of personal bias vulnerability ( b = 0.190 [95% CI, 0.031 to 0.349], P = .02), bias reduction self-efficacy ( b = 0.097 [95% CI, 0.010 to 0.184], P = .03), and taking action to reduce bias ( b = 0.113 [95% CI, 0.007 to 0.219], P = .04). The workshop had no effect on climate or burnout, but slightly increased perceptions of respectful division meetings ( b = 0.072 [95% CI, 0.0003 to 0.143], P = .049). Results of this study should give confidence to those designing prodiversity interventions for faculty in academic medical centers that a single workshop which promotes awareness of stereotype-based implicit bias, explains and labels common bias concepts, and provides evidence-based strategies for participants to practice appears to have no harms and may have significant benefits in empowering faculty to break the bias habit.",success
29249837,False,Journal Article,,,,,,,,True,"Addressing the underrepresentation of women in science is a top priority for many institutions, but the majority of efforts to increase representation of women are neither evidence-based nor rigorously assessed. One exception is the gender bias habit-breaking intervention (Carnes et al., 2015), which, in a cluster-randomized trial involving all but two departmental clusters (<i>N</i> = 92) in the 6 STEMM focused schools/colleges at the University of Wisconsin - Madison, led to increases in gender bias awareness and self-efficacy to promote gender equity in academic science departments. Following this initial success, the present study compares, in a preregistered analysis, hiring rates of new female faculty pre- and post-manipulation. Whereas the proportion of women hired by control departments remained stable over time, the proportion of women hired by intervention departments increased by an estimated 18 percentage points (<i>OR</i> = 2.23, <i>d<sub>OR</sub></i> = 0.34). Though the preregistered analysis did not achieve conventional levels of statistical significance (<i>p</i> < 0.07), our study has a hard upper limit on statistical power, as the cluster-randomized trial has a maximum sample size of 92 departmental clusters. These patterns have undeniable practical significance for the advancement of women in science, and provide promising evidence that psychological interventions can facilitate gender equity and diversity.",success
26270005,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Indwelling arterial catheters (IACs) are used extensively in the ICU for hemodynamic monitoring and for blood gas analysis. IAC use also poses potentially serious risks, including bloodstream infections and vascular complications. The purpose of this study was to assess whether IAC use was associated with mortality in patients who are mechanically ventilated and do not require vasopressor support. This study used the Multiparameter Intelligent Monitoring in Intensive Care II database, consisting of > 24,000 patients admitted to the Beth Israel Deaconess Medical Center ICU between 2001 and 2008. Patients requiring mechanical ventilation who did not require vasopressors or have a diagnosis of sepsis were identified, and the primary outcome was 28-day mortality. A model based on patient demographics, comorbidities, vital signs, and laboratory results was developed to estimate the propensity for IAC placement. Patients were then propensity matched, and McNemar test was used to evaluate the association of IAC with 28-day mortality. We identified 1,776 patients who were mechanically ventilated who met inclusion criteria. There were no differences in the covariates included in the final propensity model between the IAC and non-IAC propensity-matched groups. For the matched cohort, there was no difference in 28-day mortality between the IAC group and the non-IAC group (14.7% vs 15.2%; OR, 0.96; 95% CI, 0.62-1.47). In hemodynamically stable patients who are mechanically ventilated, the presence of an IAC is not associated with a difference in 28-day mortality. Validation in other datasets, as well as further analyses in other subgroups, is warranted.",success
35789446,True,"Multicenter Study;Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The unprecedented global crisis brought about by the COVID-19 pandemic has sparked numerous efforts to create predictive models for the detection and prognostication of SARS-CoV-2 infections with the goal of helping health systems allocate resources. Machine learning models, in particular, hold promise for their ability to leverage patient clinical information and medical images for prediction. However, most of the published COVID-19 prediction models thus far have little clinical utility due to methodological flaws and lack of appropriate validation. In this paper, we describe our methodology to develop and validate multi-modal models for COVID-19 mortality prediction using multi-center patient data. The models for COVID-19 mortality prediction were developed using retrospective data from Madrid, Spain (N = 2547) and were externally validated in patient cohorts from a community hospital in New Jersey, USA (N = 242) and an academic center in Seoul, Republic of Korea (N = 336). The models we developed performed differently across various clinical settings, underscoring the need for a guided strategy when employing machine learning for clinical decision-making. We demonstrated that using features from both the structured electronic health records and chest X-ray imaging data resulted in better 30-day mortality prediction performance across all three datasets (areas under the receiver operating characteristic curves: 0.85 (95% confidence interval: 0.83-0.87), 0.76 (0.70-0.82), and 0.95 (0.92-0.98)). We discuss the rationale for the decisions made at every step in developing the models and have made our code available to the research community. We employed the best machine learning practices for clinical model development. Our goal is to create a toolkit that would assist investigators and organizations in building multi-modal models for prediction, classification, and/or optimization.",success
29447188,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"In secondary analysis of electronic health records, a crucial task consists in correctly identifying the patient cohort under investigation. In many cases, the most valuable and relevant information for an accurate classification of medical conditions exist only in clinical narratives. Therefore, it is necessary to use natural language processing (NLP) techniques to extract and evaluate these narratives. The most commonly used approach to this problem relies on extracting a number of clinician-defined medical concepts from text and using machine learning techniques to identify whether a particular patient has a certain condition. However, recent advances in deep learning and NLP enable models to learn a rich representation of (medical) language. Convolutional neural networks (CNN) for text classification can augment the existing techniques by leveraging the representation of language to learn which phrases in a text are relevant for a given medical condition. In this work, we compare concept extraction based methods with CNNs and other commonly used models in NLP in ten phenotyping tasks using 1,610 discharge summaries from the MIMIC-III database. We show that CNNs outperform concept extraction based methods in almost all of the tasks, with an improvement in F1-score of up to 26 and up to 7 percentage points in area under the ROC curve (AUC). We additionally assess the interpretability of both approaches by presenting and evaluating methods that calculate and extract the most salient phrases for a prediction. The results indicate that CNNs are a valid alternative to existing approaches in patient phenotyping and cohort identification, and should be further investigated. Moreover, the deep learning approach presented in this paper can be used to assist clinicians during chart review or support the extraction of billing codes from text by identifying and highlighting relevant phrases for various medical conditions.",success
30349085,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals<sup>1-3</sup>, but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients<sup>1,4-6</sup>. To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence (AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician's selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians' actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes.",success
27557678,False,Journal Article;Review,,,,,,,,True,"Electronic health records (EHRs) provide opportunities to enhance patient care, embed performance measures in clinical practice, and facilitate clinical research. Concerns have been raised about the increasing recruitment challenges in trials, burdensome and obtrusive data collection, and uncertain generalizability of the results. Leveraging electronic health records to counterbalance these trends is an area of intense interest. The initial applications of electronic health records, as the primary data source is envisioned for observational studies, embedded pragmatic or post-marketing registry-based randomized studies, or comparative effectiveness studies. Advancing this approach to randomized clinical trials, electronic health records may potentially be used to assess study feasibility, to facilitate patient recruitment, and streamline data collection at baseline and follow-up. Ensuring data security and privacy, overcoming the challenges associated with linking diverse systems and maintaining infrastructure for repeat use of high quality data, are some of the challenges associated with using electronic health records in clinical research. Collaboration between academia, industry, regulatory bodies, policy makers, patients, and electronic health record vendors is critical for the greater use of electronic health records in clinical research. This manuscript identifies the key steps required to advance the role of electronic health records in cardiovascular clinical research.",success
26994063,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we cannot conduct a randomized experiment, we analyze observational data. Causal inference from large observational databases (big data) can be viewed as an attempt to emulate a randomized experiment-the target experiment or target trial-that would answer the question of interest. When the goal is to guide decisions among several strategies, causal analyses of observational data need to be evaluated with respect to how well they emulate a particular target trial. We outline a framework for comparative effectiveness research using big data that makes the target trial explicit. This framework channels counterfactual theory for comparing the effects of sustained treatment strategies, organizes analytic approaches, provides a structured process for the criticism of observational studies, and helps avoid common methodologic pitfalls.",success
33426010,False,Journal Article;Review,,,,,,,,True,"In this current digital landscape, artificial intelligence (AI) has established itself as a powerful tool in the commercial industry and is an evolving technology in healthcare. Cutting-edge imaging modalities outputting multi-dimensional data are becoming increasingly complex. In this era of data explosion, the field of cardiovascular imaging is undergoing a paradigm shift toward machine learning (ML) driven platforms. These diverse algorithms can seamlessly analyze information and automate a range of tasks. In this review article, we explore the role of ML in the field of cardiovascular imaging.",success
32224753,False,Journal Article;Review,,,,,,,,True,"This article describes how imaging can be used by physicians in diagnosing, determining prognosis, and making appropriate treatment decisions in a timely manner in patients with acute stroke. Advances in acute stroke treatment, including the use of endovascular thrombectomy in patients with large vessel occlusion and, more recently, of IV thrombolysis in an extended time window, have resulted in a paradigm shift in how imaging is used in patients with acute stroke. This paradigm shift, combined with the understanding that ""time is brain,"" means that imaging must be fast, reliable, and available around the clock for physicians to make appropriate clinical decisions. CT has therefore become the primary imaging modality of choice. Recognition of a large vessel occlusion using CT angiography has become essential in identifying patients for endovascular thrombectomy, and techniques such as imaging collaterals on CT angiography or measuring blood flow to predict tissue fate using CT perfusion have become useful tools in selecting patients for acute stroke therapy. Understanding the use of these imaging modalities and techniques in dealing with an emergency such as acute stroke has therefore become more important than ever for physicians treating patients with acute stroke. Imaging the brain and the blood vessels supplying it using modern tools and techniques is a key step in understanding the pathophysiology of acute stroke and making appropriate and timely clinical decisions.",success
33211147,True,Journal Article;Multicenter Study;Consensus Development Conference,,,,,,,,True,"Machine learning offers great opportunities to streamline and improve clinical care from the perspective of cardiac imagers, patients, and the industry and is a very active scientific research field. In light of these advances, the European Society of Cardiovascular Radiology (ESCR), a non-profit medical society dedicated to advancing cardiovascular radiology, has assembled a position statement regarding the use of machine learning (ML) in cardiovascular imaging. The purpose of this statement is to provide guidance on requirements for successful development and implementation of ML applications in cardiovascular imaging. In particular, recommendations on how to adequately design ML studies and how to report and interpret their results are provided. Finally, we identify opportunities and challenges ahead. While the focus of this position statement is ML development in cardiovascular imaging, most considerations are relevant to ML in radiology in general. KEY POINTS: • Development and clinical implementation of machine learning in cardiovascular imaging is a multidisciplinary pursuit. • Based on existing study quality standard frameworks such as SPIRIT and STARD, we propose a list of quality criteria for ML studies in radiology. • The cardiovascular imaging research community should strive for the compilation of multicenter datasets for the development, evaluation, and benchmarking of ML algorithms.",success
30963270,False,Journal Article,,,,,,,,True,"The purpose of this study was to compare the image quality of coronary computed tomography angiography (CTA) subjected to deep learning-based image restoration (DLR) method with images subjected to hybrid iterative reconstruction (IR). We enrolled 30 patients (22 men, 8 women) who underwent coronary CTA on a 320-slice CT scanner. The images were reconstructed with hybrid IR and with DLR. The image noise in the ascending aorta, left atrium, and septal wall of the ventricle was measured on all images and the contrast-to-noise ratio (CNR) in the proximal coronary arteries was calculated. We also generated CT attenuation profiles across the proximal coronary arteries and measured the width of the edge rise distance (ERD) and the edge rise slope (ERS). Two observers visually evaluated the overall image quality using a 4-point scale (1 = poor, 4 = excellent). On DLR images, the mean image noise was lower than that on hybrid IR images (18.5 ± 2.8 HU vs. 23.0 ± 4.6 HU, p < 0.01) and the CNR was significantly higher (p < 0.01). The mean ERD was significantly shorter on DLR than on hybrid IR images, whereas the mean ERS was steeper on DLR than on hybrid IR images. The mean image quality score for hybrid IR and DLR images was 2.96 and 3.58, respectively (p < 0.01). DLR reduces the image noise and improves the image quality at coronary CTA. • Deep learning-based image restoration is a new technique that employs the deep convolutional neural network for image quality improvement. • Deep learning-based restoration reduces the image noise and improves image quality at coronary CT angiography. • This method may allow for a reduction in radiation exposure.",success
31122379,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"This paper reports the development, validation, and public availability of a new neural network-based system which attempts to identify the manufacturer and even the model group of a pacemaker or defibrillator from a chest radiograph. Medical staff often need to determine the model of a pacemaker or defibrillator (cardiac rhythm device) quickly and accurately. Current approaches involve comparing a device's radiographic appearance with a manual flow chart. In this study, radiographic images of 1,676 devices, comprising 45 models from 5 manufacturers were extracted. A convolutional neural network was developed to classify the images, using a training set of 1,451 images. The testing set contained an additional 225 images consisting of 5 examples of each model. The network's ability to identify the manufacturer of a device was compared with that of cardiologists, using a published flowchart. The neural network was 99.6% (95% confidence interval [CI]: 97.5% to 100.0%) accurate in identifying the manufacturer of a device from a radiograph and 96.4% (95% CI: 93.1% to 98.5%) accurate in identifying the model group. Among 5 cardiologists who used the flowchart, median identification of manufacturer accuracy was 72.0% (range 62.2% to 88.9%), and model group identification was not possible. The network's ability to identify the manufacturer of the devices was significantly superior to that of all the cardiologists (p < 0.0001 compared with the median human identification; p < 0.0001 compared with the best human identification). A neural network can accurately identify the manufacturer and even model group of a cardiac rhythm device from a radiograph and exceeds human performance. This system may speed up the diagnosis and treatment of patients with cardiac rhythm devices, and it is publicly accessible online.",success
32361380,False,Journal Article;Review,,,,,,,,True,"Research into the possibilities of AI in cardiac CT has been growing rapidly in the last decade. With the rise of publicly available databases and AI algorithms, many researchers and clinicians have started investigations into the use of AI in the clinical workflow. This review is a comprehensive overview on the types of tasks and applications in which AI can aid the clinician in cardiac CT, and can be used as a primer for medical researchers starting in the field of AI. The applications of AI algorithms are explained and recent examples in cardiac CT of these algorithms are further elaborated on. The critical factors for implementation in the future are discussed.",success
36006439,False,English Abstract;Journal Article;Review,,,,,,,,True,"Cardiac diseases are the leading cause of death. Many diseases can be specifically treated once a valid diagnosis is established. Cardiac magnetic resonance imaging (MRI) plays a central role in the workup of many cardiac pathologies. However, image acquisition as well as interpretation and related secondary image evaluation are time-consuming and complex. Cardiac MRI is becoming increasingly established in international guidelines for the evaluation of cardiac function and differential diagnosis of a wide variety of cardiac diseases. Cardiac MRI has limited reproducibility due to the acquisition technique and interpretation of findings with complex secondary measurements. Artificial intelligence techniques and radiomics offer the potential to improve the acquisition, interpretation, and reproducibility of cardiac MRI. Research suggests that artificial intelligence and radiomic analysis can improve cardiac MRI in terms of image acquisition and also diagnostic and prognostic value. Furthermore, the implementation of artificial intelligence and radiomics may result in the identification of new biomarkers. The implementation of artificial intelligence in cardiac MRI has great potential. However, the current level of evidence is still limited in some aspects; in particular there are too few prospective and large multicenter studies available. As a result, the algorithms developed are often not sufficiently validated scientifically and are not yet applied in clinical routine.",success
30621954,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Sudden cardiac arrest (SCA) is one of the largest causes of mortality globally, with an out-of-hospital survival below 10% despite intense research. This document outlines challenges in addressing the epidemic of SCA, along the framework of respond, understand and predict, and prevent. Response could be improved by technology-assisted orchestration of community responder systems, access to automated external defibrillators, and innovations to match resuscitation resources to victims in place and time. Efforts to understand and predict SCA may be enhanced by refining taxonomy along phenotypical and pathophysiological ""axes of risk,"" extending beyond cardiovascular pathology to identify less heterogeneous cohorts, facilitated by open-data platforms and analytics including machine learning to integrate discoveries across disciplines. Prevention of SCA must integrate these concepts, recognizing that all members of society are stakeholders. Ultimately, solutions to the public health challenge of SCA will require greater awareness, societal debate and focused public policy.",success
30847259,False,Journal Article,,,,,,,,True,"Ventricular tachycardia (VT), which can lead to sudden cardiac death, occurs frequently in patients with myocardial infarction. Catheter-based radiofrequency ablation of cardiac tissue has achieved only modest efficacy, owing to the inaccurate identification of ablation targets by current electrical mapping techniques, which can lead to extensive lesions and to a prolonged, poorly tolerated procedure. Here we show that personalized virtual-heart technology based on cardiac imaging and computational modelling can identify optimal infarct-related VT ablation targets in retrospective animal (5 swine) and human studies (21 patients) and in a prospective feasibility study (5 patients). We first assessed in retrospective studies (one of which included a proportion of clinical images with artifacts) the capability of the technology to determine the minimum-size ablation targets for eradicating all VTs. In the prospective study, VT sites predicted by the technology were targeted directly, without relying on prior electrical mapping. The approach could improve infarct-related VT ablation guidance, where accurate identification of patient-specific optimal targets could be achieved on a personalized virtual heart prior to the clinical procedure.",success
35613840,False,Journal Article;Review,,,,,,,,True,"In recent years, machine learning (ML) has had notable success in providing automated analyses of neuroimaging studies, and its role is likely to increase in the future. Thus, it is paramount for clinicians to understand these approaches, gain facility with interpreting ML results, and learn how to assess algorithm performance. To provide an overview of ML, present its role in acute stroke imaging, discuss methods to evaluate algorithms, and then provide an assessment of existing approaches. In this review, we give an overview of ML techniques commonly used in medical imaging analysis and methods to evaluate performance. We then review the literature for relevant publications. Searches were run in November 2021 in Ovid Medline and PubMed. Inclusion criteria included studies in English reporting use of artificial intelligence (AI), machine learning, or similar techniques in the setting of, and in applications for, acute ischemic stroke or mechanical thrombectomy. Articles that included image-level data with meaningful results and sound ML approaches were included in this discussion. Many publications on acute stroke imaging, including detection of large vessel occlusion, detection and quantification of intracranial hemorrhage and detection of infarct core, have been published using ML methods. Imaging inputs have included non-contrast head CT, CT angiograph and MRI, with a range of performances. We discuss and review several of the most relevant publications. ML in acute ischemic stroke imaging has already made tremendous headway. Additional applications and further integration with clinical care is inevitable. Thus, facility with these approaches is critical for the neurointerventional clinician.",success
30318264,False,"Journal Article;Research Support, Non-U.S. Gov't;Validation Study",,,,,,,,True,"Non-contrast head CT scan is the current standard for initial imaging of patients with head trauma or stroke symptoms. We aimed to develop and validate a set of deep learning algorithms for automated detection of the following key findings from these scans: intracranial haemorrhage and its types (ie, intraparenchymal, intraventricular, subdural, extradural, and subarachnoid); calvarial fractures; midline shift; and mass effect. We retrospectively collected a dataset containing 313 318 head CT scans together with their clinical reports from around 20 centres in India between Jan 1, 2011, and June 1, 2017. A randomly selected part of this dataset (Qure25k dataset) was used for validation and the rest was used to develop algorithms. An additional validation dataset (CQ500 dataset) was collected in two batches from centres that were different from those used for the development and Qure25k datasets. We excluded postoperative scans and scans of patients younger than 7 years. The original clinical radiology report and consensus of three independent radiologists were considered as gold standard for the Qure25k and CQ500 datasets, respectively. Areas under the receiver operating characteristic curves (AUCs) were primarily used to assess the algorithms. The Qure25k dataset contained 21 095 scans (mean age 43 years; 9030 [43%] female patients), and the CQ500 dataset consisted of 214 scans in the first batch (mean age 43 years; 94 [44%] female patients) and 277 scans in the second batch (mean age 52 years; 84 [30%] female patients). On the Qure25k dataset, the algorithms achieved an AUC of 0·92 (95% CI 0·91-0·93) for detecting intracranial haemorrhage (0·90 [0·89-0·91] for intraparenchymal, 0·96 [0·94-0·97] for intraventricular, 0·92 [0·90-0·93] for subdural, 0·93 [0·91-0·95] for extradural, and 0·90 [0·89-0·92] for subarachnoid). On the CQ500 dataset, AUC was 0·94 (0·92-0·97) for intracranial haemorrhage (0·95 [0·93-0·98], 0·93 [0·87-1·00], 0·95 [0·91-0·99], 0·97 [0·91-1·00], and 0·96 [0·92-0·99], respectively). AUCs on the Qure25k dataset were 0·92 (0·91-0·94) for calvarial fractures, 0·93 (0·91-0·94) for midline shift, and 0·86 (0·85-0·87) for mass effect, while AUCs on the CQ500 dataset were 0·96 (0·92-1·00), 0·97 (0·94-1·00), and 0·92 (0·89-0·95), respectively. Our results show that deep learning algorithms can accurately identify head CT scan abnormalities requiring urgent attention, opening up the possibility to use these algorithms to automate the triage process. Qure.ai.",success
33384294,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Artificial intelligence algorithms have the potential to become an important diagnostic tool to optimize stroke workflow. Viz LVO is a medical product leveraging a convolutional neural network designed to detect large-vessel occlusions on CTA scans and notify the treatment team within minutes via a dedicated mobile application. We aimed to evaluate the detection accuracy of the Viz LVO in real clinical practice at a comprehensive stroke center. Viz LVO was installed for this study in a comprehensive stroke center. All consecutive head and neck CTAs performed from January 2018 to March 2019 were scanned by the algorithm for detection of large-vessel occlusions. The system results were compared with the formal reports of senior neuroradiologists used as ground truth for the presence of a large-vessel occlusion. A total of 1167 CTAs were included in the study. Of these, 404 were stroke protocols. Seventy-five (6.4%) patients had a large-vessel occlusion as ground truth; 61 were detected by the system. Sensitivity was 0.81, negative predictive value was 0.99, and accuracy was 0.94. In the stroke protocol subgroup, 72 (17.8%) of 404 patients had a large-vessel occlusion, with 59 identified by the system, showing a sensitivity of 0.82, negative predictive value of 0.96, and accuracy of 0.89. Our experience evaluating Viz LVO shows that the system has the potential for early identification of patients with stroke with large-vessel occlusions, hopefully improving future management and stroke care.",success
31990267,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Background Identifying the presence and extent of infarcted brain tissue at baseline plays a crucial role in the treatment of patients with acute ischemic stroke (AIS). Patients with extensive infarction are unlikely to benefit from thrombolysis or thrombectomy procedures. Purpose To develop an automated approach to detect and quantitate infarction by using non-contrast-enhanced CT scans in patients with AIS. Materials and Methods Non-contrast-enhanced CT images in patients with AIS (<6 hours from symptom onset to CT) who also underwent diffusion-weighted (DW) MRI within 1 hour after AIS were obtained from May 2004 to July 2009 and were included in this retrospective study. Ischemic lesions manually contoured on DW MRI scans were used as the reference standard. An automatic segmentation approach involving machine learning (ML) was developed to detect infarction. Randomly selected nonenhanced CT images from 157 patients with the lesion labels manually contoured on DW MRI scans were used to train and validate the ML model; the remaining 100 patients independent of the derivation cohort were used for testing. The ML algorithm was quantitatively compared with the reference standard (DW MRI) by using Bland-Altman plots and Pearson correlation. Results In 100 patients in the testing data set (median age, 69 years; interquartile range [IQR]: 59-76 years; 59 men), baseline non-contrast-enhanced CT was performed within a median time of 48 minutes from symptom onset (IQR, 27-93 minutes); baseline MRI was performed a median of 38 minutes (IQR, 24-48 minutes) later. The algorithm-detected lesion volume correlated with the reference standard of expert-contoured lesion volume in acute DW MRI scans (<i>r</i> = 0.76, <i>P</i> < .001). The mean difference between the algorithm-segmented volume (median, 15 mL; IQR, 9-38 mL) and the DW MRI volume (median, 19 mL; IQR, 5-43 mL) was 11 mL (<i>P</i> = .89). Conclusion A machine learning approach for segmentation of infarction on non-contrast-enhanced CT images in patients with acute ischemic stroke showed good agreement with stroke volume on diffusion-weighted MRI scans. © RSNA, 2020 <i>Online supplemental material is available for this article.</i> See also the editorial by Nael in this issue.",success
34102758,False,Journal Article,,,,,,,,True,"Multiphase computed tomographic angiography (mCTA) provides time variant images of pial vasculature supplying brain in patients with acute ischemic stroke (AIS). To develop a machine learning (ML) technique to predict tissue perfusion and infarction from mCTA source images. 284 patients with AIS were included from the Precise and Rapid assessment of collaterals using multi-phase CTA in the triage of patients with acute ischemic stroke for Intra-artery Therapy (Prove-IT) study. All patients had non-contrast computed tomography, mCTA, and computed tomographic perfusion (CTP) at baseline and follow-up magnetic resonance imaging/non-contrast-enhanced computed tomography. Of the 284 patient images, 140 patient images were randomly selected to train and validate three ML models to predict a pre-defined Tmax thresholded perfusion abnormality, core and penumbra on CTP. The remaining 144 patient images were used to test the ML models. The predicted perfusion, core and penumbra lesions from ML models were compared to CTP perfusion lesion and to follow-up infarct using Bland-Altman plots, concordance correlation coefficient (CCC), intra-class correlation coefficient (ICC), and Dice similarity coefficient. Mean difference between the mCTA predicted perfusion volume and CTP perfusion volume was 4.6 mL (limit of agreement [LoA], -53 to 62.1 mL; P=0.56; CCC 0.63 [95% confidence interval [CI], 0.53 to 0.71; P<0.01], ICC 0.68 [95% CI, 0.58 to 0.78; P<0.001]). Mean difference between the mCTA predicted infarct and follow-up infarct in the 100 patients with acute reperfusion (modified thrombolysis in cerebral infarction [mTICI] 2b/2c/3) was 21.7 mL, while it was 3.4 mL in the 44 patients not achieving reperfusion (mTICI 0/1). Amongst reperfused subjects, CCC was 0.4 (95% CI, 0.15 to 0.55; P<0.01) and ICC was 0.42 (95% CI, 0.18 to 0.50; P<0.01); in non-reperfused subjects CCC was 0.52 (95% CI, 0.20 to 0.60; P<0.001) and ICC was 0.60 (95% CI, 0.37 to 0.76; P<0.001). No difference was observed between the mCTA and CTP predicted infarct volume in the test cohort (P=0.67). A ML based mCTA model is able to predict brain tissue perfusion abnormality and follow-up infarction, comparable to CTP.",success
26978244,False,Journal Article,,,,,,,,True,"There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.",success
32273514,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"The role of automatic electrocardiogram (ECG) analysis in clinical practice is limited by the accuracy of existing models. Deep Neural Networks (DNNs) are models composed of stacked transformations that learn tasks by examples. This technology has recently achieved striking success in a variety of task and there are great expectations on how it might improve clinical practice. Here we present a DNN model trained in a dataset with more than 2 million labeled exams analyzed by the Telehealth Network of Minas Gerais and collected under the scope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The DNN outperform cardiology resident medical doctors in recognizing 6 types of abnormalities in 12-lead ECG recordings, with F1 scores above 80% and specificity over 99%. These results indicate ECG analysis based on DNNs, previously studied in a single-lead setup, generalizes well to 12-lead exams, taking the technology closer to the standard clinical practice.",success
35029163,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Cirrhosis is associated with cardiac dysfunction and distinct electrocardiogram (ECG) abnormalities. This study aimed to develop a proof-of-concept deep learning-based artificial intelligence (AI) model that could detect cirrhosis-related signals on ECG and generate an AI-Cirrhosis-ECG (ACE) score that would correlate with disease severity. A review of Mayo Clinic's electronic health records identified 5,212 patients with advanced cirrhosis ≥18 years who underwent liver transplantation at the 3 Mayo Clinic transplant centers between 1988 and 2019. The patients were matched by age and sex in a 1:4 ratio to controls without liver disease and then divided into training, validation, and test sets using a 70%-10%-20% split. The primary outcome was the performance of the model in distinguishing patients with cirrhosis from controls using their ECGs. In addition, the association between the ACE score and the severity of patients' liver disease was assessed. The model's area under the curve in the test set was 0.908 with 84.9% sensitivity and 83.2% specificity, and this performance remained consistent after additional matching for medical comorbidities. Significant elevations in the ACE scores were seen with increasing model for end-stage liver disease-sodium score. Longitudinal trends in the ACE scores before and after liver transplantation mirrored the progression and resolution of liver disease. The ACE score, a deep learning model, can accurately discriminate ECGs from patients with and without cirrhosis. This novel relationship between AI-enabled ECG analysis and cirrhosis holds promise as the basis for future low-cost tools and applications in the care of patients with liver disease.",success
31378392,False,Journal Article;Validation Study,,,,,,,,True,"Atrial fibrillation is frequently asymptomatic and thus underdetected but is associated with stroke, heart failure, and death. Existing screening methods require prolonged monitoring and are limited by cost and low yield. We aimed to develop a rapid, inexpensive, point-of-care means of identifying patients with atrial fibrillation using machine learning. We developed an artificial intelligence (AI)-enabled electrocardiograph (ECG) using a convolutional neural network to detect the electrocardiographic signature of atrial fibrillation present during normal sinus rhythm using standard 10-second, 12-lead ECGs. We included all patients aged 18 years or older with at least one digital, normal sinus rhythm, standard 10-second, 12-lead ECG acquired in the supine position at the Mayo Clinic ECG laboratory between Dec 31, 1993, and July 21, 2017, with rhythm labels validated by trained personnel under cardiologist supervision. We classified patients with at least one ECG with a rhythm of atrial fibrillation or atrial flutter as positive for atrial fibrillation. We allocated ECGs to the training, internal validation, and testing datasets in a 7:1:2 ratio. We calculated the area under the curve (AUC) of the receiver operatoring characteristic curve for the internal validation dataset to select a probability threshold, which we applied to the testing dataset. We evaluated model performance on the testing dataset by calculating the AUC and the accuracy, sensitivity, specificity, and F1 score with two-sided 95% CIs. We included 180 922 patients with 649 931 normal sinus rhythm ECGs for analysis: 454 789 ECGs recorded from 126 526 patients in the training dataset, 64 340 ECGs from 18 116 patients in the internal validation dataset, and 130 802 ECGs from 36 280 patients in the testing dataset. 3051 (8·4%) patients in the testing dataset had verified atrial fibrillation before the normal sinus rhythm ECG tested by the model. A single AI-enabled ECG identified atrial fibrillation with an AUC of 0·87 (95% CI 0·86-0·88), sensitivity of 79·0% (77·5-80·4), specificity of 79·5% (79·0-79·9), F1 score of 39·2% (38·1-40·3), and overall accuracy of 79·4% (79·0-79·9). Including all ECGs acquired during the first month of each patient's window of interest (ie, the study start date or 31 days before the first recorded atrial fibrillation ECG) increased the AUC to 0·90 (0·90-0·91), sensitivity to 82·3% (80·9-83·6), specificity to 83·4% (83·0-83·8), F1 score to 45·4% (44·2-46·5), and overall accuracy to 83·3% (83·0-83·7). An AI-enabled ECG acquired during normal sinus rhythm permits identification at point of care of individuals with atrial fibrillation. None.",success
32769990,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Prompt identification of acute coronary syndrome is a challenge in clinical practice. The 12-lead electrocardiogram (ECG) is readily available during initial patient evaluation, but current rule-based interpretation approaches lack sufficient accuracy. Here we report machine learning-based methods for the prediction of underlying acute myocardial ischemia in patients with chest pain. Using 554 temporal-spatial features of the 12-lead ECG, we train and test multiple classifiers on two independent prospective patient cohorts (n = 1244). While maintaining higher negative predictive value, our final fusion model achieves 52% gain in sensitivity compared to commercial interpretation software and 37% gain in sensitivity compared to experienced clinicians. Such an ultra-early, ECG-based clinical decision support tool, when combined with the judgment of trained emergency personnel, would help to improve clinical outcomes and reduce unnecessary costs in patients with chest pain.",success
36253296,False,"Observational Study;Journal Article;Research Support, N.I.H., Extramural",NCT04237688,databank,NCT04237688,NCT04237688,NCT04237688,NCT04237688|databank,NCT04237688|databank,True,"Ischemic electrocardiogram (ECG) changes are subtle and transient in patients with suspected non-ST-segment elevation (NSTE)-acute coronary syndrome. However, the out-of-hospital ECG is not routinely used during subsequent evaluation at the emergency department. Therefore, we sought to compare the diagnostic performance of out-of-hospital and ED ECG and evaluate the incremental gain of artificial intelligence-augmented ECG analysis. This prospective observational cohort study recruited patients with out-of-hospital chest pain. We retrieved out-of-hospital-ECG obtained by paramedics in the field and the first ED ECG obtained by nurses during inhospital evaluation. Two independent and blinded reviewers interpreted ECG dyads in mixed order per practice recommendations. Using 179 morphological ECG features, we trained, cross-validated, and tested a random forest classifier to augment non ST-elevation acute coronary syndrome (NSTE-ACS) diagnosis. Our sample included 2,122 patients (age 59 [16]; 53% women; 44% Black, 13.5% confirmed acute coronary syndrome). The rate of diagnostic ST elevation and ST depression were 5.9% and 16.2% on out-of-hospital-ECG and 6.1% and 12.4% on ED ECG, with ∼40% of changes seen on out-of-hospital-ECG persisting and ∼60% resolving. Using expert interpretation of out-of-hospital-ECG alone gave poor baseline performance with area under the receiver operating characteristic (AUC), sensitivity, and negative predictive values of 0.69, 0.50, and 0.92. Using expert interpretation of serial ECG changes enhanced this performance (AUC 0.80, sensitivity 0.61, and specificity 0.93). Interestingly, augmenting the out-of-hospital-ECG alone with artificial intelligence algorithms boosted its performance (AUC 0.83, sensitivity 0.75, and specificity 0.95), yielding a net reclassification improvement of 29.5% against expert ECG interpretation. In this study, 60% of diagnostic ST changes resolved prior to hospital arrival, making the ED ECG suboptimal for the inhospital evaluation of NSTE-ACS. Using serial ECG changes or incorporating artificial intelligence-augmented analyses would allow correctly reclassifying one in 4 patients with suspected NSTE-ACS.",success
37386246,False,"Observational Study;Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Patients with occlusion myocardial infarction (OMI) and no ST-elevation on presenting electrocardiogram (ECG) are increasing in numbers. These patients have a poor prognosis and would benefit from immediate reperfusion therapy, but, currently, there are no accurate tools to identify them during initial triage. Here we report, to our knowledge, the first observational cohort study to develop machine learning models for the ECG diagnosis of OMI. Using 7,313 consecutive patients from multiple clinical sites, we derived and externally validated an intelligent model that outperformed practicing clinicians and other widely used commercial interpretation systems, substantially boosting both precision and sensitivity. Our derived OMI risk score provided enhanced rule-in and rule-out accuracy relevant to routine care, and, when combined with the clinical judgment of trained emergency personnel, it helped correctly reclassify one in three patients with chest pain. ECG features driving our models were validated by clinical experts, providing plausible mechanistic links to myocardial injury.",success
34854319,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Background Accurate detection of arrhythmic events in the intensive care units (ICU) is of paramount significance in providing timely care. However, traditional ICU monitors generate a high rate of false alarms causing alarm fatigue. In this work, we develop an algorithm to improve life threatening arrhythmia detection in the ICUs using a deep learning approach. Methods and Results This study involves a total of 953 independent life-threatening arrhythmia alarms generated from the ICU bedside monitors of 410 patients. Specifically, we used the ECG (4 channels), arterial blood pressure, and photoplethysmograph signals to accurately detect the onset and offset of various arrhythmias, without prior knowledge of the alarm type. We used a hybrid convolutional neural network based classifier that fuses traditional handcrafted features with features automatically learned using convolutional neural networks. Further, the proposed architecture remains flexible to be adapted to various arrhythmic conditions as well as multiple physiological signals. Our hybrid- convolutional neural network approach achieved superior performance compared with methods which only used convolutional neural network. We evaluated our algorithm using 5-fold cross-validation for 5 times and obtained an accuracy of 87.5%±0.5%, and a score of 81%±0.9%. Independent evaluation of our algorithm on the publicly available PhysioNet 2015 Challenge database resulted in overall classification accuracy and score of 93.9% and 84.3%, respectively, indicating its efficacy and generalizability. Conclusions Our method accurately detects multiple arrhythmic conditions. Suitable translation of our algorithm may significantly improve the quality of care in ICUs by reducing the burden of false alarms.",success
16321696,False,Journal Article,,,,,,,,True,"The aim of the study was to determine the frequency and nature of errors in computer electrocardiogram (ECG) reading. The ECGs were collected in the tertiary care VA Hospital from both inpatients and outpatients. They were read by the electrocardiograph built-in computer software, and then reread by two cardiologists. Statistical analysis was performed using sensitivity, specificity, positive and negative predicted value to analyze the data. An error index was formulated as indicator of diagnostic accuracy. Out of 2072 ECGs, 776 (37.5%) were normal, and 1296 (62.5%) were abnormal. In 9.9% of all ECGs and in 15.9% of abnormal ECGs there were significant disagreements between the computer and cardiologists. The errors in diagnosis of arrhythmia, conduction disorders and electronic pacemakers accounted for 178 cases, or 86.4% of all errors. The rest was represented by misdetection of chamber enlargement (7 cases, 3.4%), misdiagnosis of ischemia and acute myocardial infarction (16 cases, 7.8%), and lead misplacement (5 cases, 2.4%). The most frequent errors in computer ECG interpretation are related to arrhythmias, conduction disorders, and electronic pacemakers. Computer ECG diagnosis of life threatening conditions e.g. acute myocardial infarction or high degree AV blocks are frequently not accurate (40.7% and 75.0% errors, respectively). Improvement in the diagnostic algorithms should focus on these areas. Error index is a convenient and informative tool for evaluation of diagnostic accuracy.",success
30476648,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Cardiologs® has developed the first electrocardiogram (ECG) algorithm that uses a deep neural network (DNN) for full 12‑lead ECG analysis, including rhythm, QRS and ST-T-U waves. We compared the accuracy of the first version of Cardiologs® DNN algorithm to the Mortara/Veritas® conventional algorithm in emergency department (ED) ECGs. Individual ECG diagnoses were prospectively mapped to one of 16 pre-specified groups of ECG diagnoses, which were further classified as ""major"" ECG abnormality or not. Automated interpretations were compared to blinded experts'. The primary outcome was the performance of the algorithms in finding at least one ""major"" abnormality. The secondary outcome was the proportion of all ECGs for which all groups were identified, with no false negative or false positive groups (""accurate ECG interpretation""). Additionally, we measured sensitivity and positive predictive value (PPV) for any abnormal group. Cardiologs® vs. Veritas® accuracy for finding a major abnormality was 92.2% vs. 87.2% (p < 0.0001), with comparable sensitivity (88.7% vs. 92.0%, p = 0.086), improved specificity (94.0% vs. 84.7%, p < 0.0001) and improved positive predictive value (PPV 88.2% vs. 75.4%, p < 0.0001). Cardiologs® had accurate ECG interpretation for 72.0% (95% CI: 69.6-74.2) of ECGs vs. 59.8% (57.3-62.3) for Veritas® (P < 0.0001). Sensitivity for any abnormal group for Cardiologs® and Veritas®, respectively, was 69.6% (95CI 66.7-72.3) vs. 68.3% (95CI 65.3-71.1) (NS). Positive Predictive Value was 74.0% (71.1-76.7) for Cardiologs® vs. 56.5% (53.7-59.3) for Veritas® (P < 0.0001). Cardiologs' DNN was more accurate and specific in identifying ECGs with at least one major abnormal group. It had a significantly higher rate of accurate ECG interpretation, with similar sensitivity and higher PPV.",success
34993486,False,Journal Article,,,,,,,,True,"Cardiovascular disease is a major threat to maternal health, with cardiomyopathy being among the most common acquired cardiovascular diseases during pregnancy and the postpartum period. The aim of our study was to evaluate the effectiveness of an electrocardiogram (ECG)-based deep learning model in identifying cardiomyopathy during pregnancy and the postpartum period. We used an ECG-based deep learning model to detect cardiomyopathy in a cohort of women who were pregnant or in the postpartum period seen at Mayo Clinic. Model performance was evaluated using the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. We compared the diagnostic probabilities of the deep learning model with natriuretic peptides and a multivariable model consisting of demographic and clinical parameters. The study cohort included 1807 women; 7%, 10%, and 13% had left ventricular ejection fraction (LVEF) of 35% or less, <45%, and <50%, respectively. The ECG-based deep learning model identified cardiomyopathy with AUCs of 0.92 (LVEF ≤ 35%), 0.89 (LVEF < 45%), and 0.87 (LVEF < 50%). For LVEF of 35% or less, AUC was higher in Black (0.95) and Hispanic (0.98) women compared to White (0.91). Natriuretic peptides and the multivariable model had AUCs of 0.85 to 0.86 and 0.72, respectively. An ECG-based deep learning model effectively identifies cardiomyopathy during pregnancy and the postpartum period and outperforms natriuretic peptides and traditional clinical parameters with the potential to become a powerful initial screening tool for cardiomyopathy in the obstetric care setting.",success
33958795,True,Journal Article;Multicenter Study;Randomized Controlled Trial,NCT04000087,databank,NCT04000087,NCT04000087,NCT04000087,NCT04000087|databank,NCT04000087|databank,True,"We have conducted a pragmatic clinical trial aimed to assess whether an electrocardiogram (ECG)-based, artificial intelligence (AI)-powered clinical decision support tool enables early diagnosis of low ejection fraction (EF), a condition that is underdiagnosed but treatable. In this trial ( NCT04000087 ), 120 primary care teams from 45 clinics or hospitals were cluster-randomized to either the intervention arm (access to AI results; 181 clinicians) or the control arm (usual care; 177 clinicians). ECGs were obtained as part of routine care from a total of 22,641 adults (N = 11,573 intervention; N = 11,068 control) without prior heart failure. The primary outcome was a new diagnosis of low EF (≤50%) within 90 days of the ECG. The trial met the prespecified primary endpoint, demonstrating that the intervention increased the diagnosis of low EF in the overall cohort (1.6% in the control arm versus 2.1% in the intervention arm, odds ratio (OR) 1.32 (1.01-1.61), P = 0.007) and among those who were identified as having a high likelihood of low EF (that is, positive AI-ECG, 6% of the overall cohort) (14.5% in the control arm versus 19.5% in the intervention arm, OR 1.43 (1.08-1.91), P = 0.01). In the overall cohort, echocardiogram utilization was similar between the two arms (18.2% control versus 19.2% intervention, P = 0.17); for patients with positive AI-ECGs, more echocardiograms were obtained in the intervention compared to the control arm (38.1% control versus 49.6% intervention, P < 0.001). These results indicate that use of an AI algorithm based on ECGs can enable the early diagnosis of low EF in patients in the setting of routine primary care.",success
34871321,False,Journal Article,,,,,,,,True,"Left ventricular systolic dysfunction (LVSD) in Chagas disease (ChD) is relatively common and its treatment using low-cost drugs can improve symptoms and reduce mortality. Recently, an artificial intelligence (AI)-enabled ECG algorithm showed excellent accuracy to detect LVSD in a general population, but its accuracy in ChD has not been tested. To analyze the ability of AI to recognize LVSD in patients with ChD, defined as a left ventricular ejection fraction determined by the Echocardiogram ≤ 40%. This is a cross-sectional study of ECG obtained from a large cohort of patients with ChD named São Paulo-Minas Gerais Tropical Medicine Research Center (SaMi-Trop) Study. The digital ECGs of the participants were submitted to the analysis of the trained machine to detect LVSD. The diagnostic performance of the AI-enabled ECG to detect LVSD was tested using an echocardiogram as the gold standard to detect LVSD, defined as an ejection fraction <40%. The model was enriched with NT-proBNP plasma levels, male sex, and QRS ≥ 120ms. Among the 1,304 participants of this study, 67% were women, median age of 60; there were 93 (7.1%) individuals with LVSD. Most patients had major ECG abnormalities (59.5%). The AI algorithm identified LVSD among ChD patients with an odds ratio of 63.3 (95% CI 32.3-128.9), a sensitivity of 73%, a specificity of 83%, an overall accuracy of 83%, and a negative predictive value of 97%; the AUC was 0.839. The model adjusted for the male sex and QRS ≥ 120ms improved the AUC to 0.859. The model adjusted for the male sex and elevated NT-proBNP had a higher accuracy of 0.89 and an AUC of 0.874. The AI analysis of the ECG of Chagas disease patients can be transformed into a powerful tool for the recognition of LVSD.",success
36247412,False,Journal Article,,,,,,,,True,"Some artificial intelligence models applied in medical practice require ongoing retraining, introduce unintended racial bias, or have variable performance among different subgroups of patients. We assessed the real-world performance of the artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction with respect to multiple patient and electrocardiogram variables to determine the algorithm's long-term efficacy and potential bias in the absence of retraining. Electrocardiograms acquired in 2019 at Mayo Clinic in Minnesota, Arizona, and Florida with an echocardiogram performed within 14 days were analyzed (<i>n</i> = 44 986 unique patients). The area under the curve (AUC) was calculated to evaluate performance of the algorithm among age groups, racial and ethnic groups, patient encounter location, electrocardiogram features, and over time. The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction had an AUC of 0.903 for the total cohort. Time series analysis of the model validated its temporal stability. Areas under the curve were similar for all racial and ethnic groups (0.90-0.92) with minimal performance difference between sexes. Patients with a 'normal sinus rhythm' electrocardiogram (<i>n</i> = 37 047) exhibited an AUC of 0.91. All other electrocardiogram features had areas under the curve between 0.79 and 0.91, with the lowest performance occurring in the left bundle branch block group (0.79). The artificial intelligence-enhanced electrocardiogram to detect left ventricular systolic dysfunction is stable over time in the absence of retraining and robust with respect to multiple variables including time, patient race, and electrocardiogram features.",success
34998740,True,"Journal Article;Multicenter Study;Observational Study;Research Support, Non-U.S. Gov't",NCT04601415,databank,NCT04601415,NCT04601415,NCT04601415,NCT04601415|databank,NCT04601415|databank,True,"Most patients who have heart failure with a reduced ejection fraction, when left ventricular ejection fraction (LVEF) is 40% or lower, are diagnosed in hospital. This is despite previous presentations to primary care with symptoms. We aimed to test an artificial intelligence (AI) algorithm applied to a single-lead ECG, recorded during ECG-enabled stethoscope examination, to validate a potential point-of-care screening tool for LVEF of 40% or lower. We conducted an observational, prospective, multicentre study of a convolutional neural network (known as AI-ECG) that was previously validated for the detection of reduced LVEF using 12-lead ECG as input. We used AI-ECG retrained to interpret single-lead ECG input alone. Patients (aged ≥18 years) attending for transthoracic echocardiogram in London (UK) were recruited. All participants had 15 s of supine, single-lead ECG recorded at the four standard anatomical positions for cardiac auscultation, plus one handheld position, using an ECG-enabled stethoscope. Transthoracic echocardiogram-derived percentage LVEF was used as ground truth. The primary outcome was performance of AI-ECG at classifying reduced LVEF (LVEF ≤40%), measured using metrics including the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity, with two-sided 95% CIs. The primary outcome was reported for each position individually and with an optimal combination of AI-ECG outputs (interval range 0-1) from two positions using a rule-based approach and several classification models. This study is registered with ClinicalTrials.gov, NCT04601415. Between Feb 6 and May 27, 2021, we recruited 1050 patients (mean age 62 years [SD 17·4], 535 [51%] male, 432 [41%] non-White). 945 (90%) had an ejection fraction of at least 40%, and 105 (10%) had an ejection fraction of 40% or lower. Across all positions, ECGs were most frequently of adequate quality for AI-ECG interpretation at the pulmonary position (979 [93·3%] of 1050). Quality was lowest for the aortic position (846 [80·6%]). AI-ECG performed best at the pulmonary valve position (p=0·02), with an AUROC of 0·85 (95% CI 0·81-0·89), sensitivity of 84·8% (76·2-91·3), and specificity of 69·5% (66·4-72·6). Diagnostic odds ratios did not differ by age, sex, or non-White ethnicity. Taking the optimal combination of two positions (pulmonary and handheld positions), the rule-based approach resulted in an AUROC of 0·85 (0·81-0·89), sensitivity of 82·7% (72·7-90·2), and specificity of 79·9% (77·0-82·6). Using AI-ECG outputs from these two positions, a weighted logistic regression with l2 regularisation resulted in an AUROC of 0·91 (0·88-0·95), sensitivity of 91·9% (78·1-98·3), and specificity of 80·2% (75·5-84·3). A deep learning system applied to single-lead ECGs acquired during a routine examination with an ECG-enabled stethoscope can detect LVEF of 40% or lower. These findings highlight the potential for inexpensive, non-invasive, workflow-adapted, point-of-care screening, for earlier diagnosis and prognostically beneficial treatment. NHS Accelerated Access Collaborative, NHSX, and the National Institute for Health Research.",success
34419527,False,Journal Article,,,,,,,,True,"There is no established screening approach for hypertrophic cardiomyopathy (HCM). We recently developed an artificial intelligence (AI) model for the detection of HCM based on the 12‑lead electrocardiogram (AI-ECG) in adults. Here, we aimed to validate this approach of ECG-based HCM detection in pediatric patients (age ≤ 18 years). We identified a cohort of 300 children and adolescents with HCM (mean age 12.5 ± 4.6 years, male 68%) who had an ECG and echocardiogram at our institution. Patients were age- and sex-matched to 18,439 non-HCM controls. Diagnostic performance of the AI-ECG model for the detection of HCM was estimated using the previously identified optimal diagnostic threshold of 11% (the probability output derived by the model above which an ECG is considered to belong to an HCM patient). Mean AI-ECG probabilities of HCM were 92% and 5% in the case and control groups, respectively. The area under the receiver operating characteristic curve (AUC) of the AI-ECG model for HCM detection was 0.98 (95% CI 0.98-0.99) with corresponding sensitivity 92% and specificity 95%. The positive and negative predictive values were 22% and 99%, respectively. The model performed similarly in males and females and in genotype-positive and genotype-negative HCM patients. Performance tended to be superior with increasing age. In the age subgroup <5 years, the test's AUC was 0.93. In comparison, the AUC was 0.99 in the age subgroup 15-18 years. A deep-learning, AI model can detect pediatric HCM with high accuracy from the standard 12‑lead ECG.",success
34218880,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"To develop an artificial intelligence (AI)-based tool to detect cardiac amyloidosis (CA) from a standard 12-lead electrocardiogram (ECG). We collected 12-lead ECG data from 2541 patients with light chain or transthyretin CA seen at Mayo Clinic between 2000 and 2019. Cases were nearest neighbor matched for age and sex, with 2454 controls. A subset of 2997 (60%) cases and controls were used to train a deep neural network to predict the presence of CA with an internal validation set (n=999; 20%) and a randomly selected holdout testing set (n=999; 20%). We performed experiments using single-lead and 6-lead ECG subsets. The area under the receiver operating characteristic curve (AUC) was 0.91 (CI, 0.90 to 0.93), with a positive predictive value for detecting either type of CA of 0.86. By use of a cutoff probability of 0.485 determined by the Youden index, 426 (84%) of the holdout patients with CA were detected by the model. Of the patients with CA and prediagnosis electrocardiographic studies, the AI model successfully predicted the presence of CA more than 6 months before the clinical diagnosis in 59%. The best single-lead model was V5 with an AUC of 0.86 and a precision of 0.78, with other single leads performing similarly. The 6-lead (bipolar leads) model had an AUC of 0.90 and a precision of 0.85. An AI-driven ECG model effectively detects CA and may promote early diagnosis of this life-threatening disease.",success
33748852,False,Journal Article,,,,,,,,True,"Early detection of aortic stenosis (AS) is becoming increasingly important with a better outcome after aortic valve replacement in asymptomatic severe AS patients and a poor outcome in moderate AS. We aimed to develop artificial intelligence-enabled electrocardiogram (AI-ECG) using a convolutional neural network to identify patients with moderate to severe AS. Between 1989 and 2019, 258 607 adults [mean age 63 ± 16.3 years; women 122 790 (48%)] with an echocardiography and an ECG performed within 180 days were identified from the Mayo Clinic database. Moderate to severe AS by echocardiography was present in 9723 (3.7%) patients. Artificial intelligence training was performed in 129 788 (50%), validation in 25 893 (10%), and testing in 102 926 (40%) randomly selected subjects. In the test group, the AI-ECG labelled 3833 (3.7%) patients as positive with the area under the curve (AUC) of 0.85. The sensitivity, specificity, and accuracy were 78%, 74%, and 74%, respectively. The sensitivity increased and the specificity decreased as age increased. Women had lower sensitivity but higher specificity compared with men at any age groups. The model performance increased when age and sex were added to the model (AUC 0.87), which further increased to 0.90 in patients without hypertension. Patients with false-positive AI-ECGs had twice the risk for developing moderate or severe AS in 15 years compared with true negative AI-ECGs (hazard ratio 2.18, 95% confidence interval 1.90-2.50). An AI-ECG can identify patients with moderate or severe AS and may serve as a powerful screening tool for AS in the community.",success
35272798,False,"Letter;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"AI analysis of HCM ECGs correlates with longitudinal hemodynamic, cardiac structural and laboratory markers in obstructive HCM patients.",success
33185118,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"An artificial intelligence (AI) algorithm applied to electrocardiography during sinus rhythm has recently been shown to detect concurrent episodic atrial fibrillation (AF). We sought to characterize the value of AI-enabled electrocardiography (AI-ECG) as a predictor of future AF and assess its performance compared with the CHARGE-AF score (Cohorts for Aging and Research in Genomic Epidemiology-AF) in a population-based sample. We calculated the probability of AF using AI-ECG, among participants in the population-based Mayo Clinic Study of Aging who had no history of AF at the time of the baseline study visit. Cox proportional hazards models were fit to assess the independent prognostic value and interaction between AI-ECG AF model output and CHARGE-AF score. C statistics were calculated for AI-ECG AF model output, CHARGE-AF score, and combined AI-ECG and CHARGE-AF score. A total of 1936 participants with median age 75.8 (interquartile range, 70.4-81.8) years and median CHARGE-AF score 14.0 (IQR, 13.2-14.7) were included in the analysis. Participants with AI-ECG AF model output of >0.5 at the baseline visit had cumulative incidence of AF 21.5% at 2 years and 52.2% at 10 years. When included in the same model, both AI-ECG AF model output (hazard ratio, 1.76 per SD after logit transformation [95% CI, 1.51-2.04]) and CHARGE-AF score (hazard ratio, 1.90 per SD [95% CI, 1.58-2.28]) independently predicted future AF without significant interaction (<i>P</i>=0.54). C statistics were 0.69 (95% CI, 0.66-0.72) for AI-ECG AF model output, 0.69 (95% CI, 0.66-0.71) for CHARGE-AF, and 0.72 (95% CI, 0.69-0.75) for combined AI-ECG and CHARGE-AF score. In the present study, both the AI-ECG AF model output and CHARGE-AF score independently predicted incident AF. The AI-ECG may offer a means to assess risk with a single test and without requiring manual or automated clinical data abstraction.",success
34743566,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Artificial intelligence (AI)-enabled analysis of 12-lead ECGs may facilitate efficient estimation of incident atrial fibrillation (AF) risk. However, it remains unclear whether AI provides meaningful and generalizable improvement in predictive accuracy beyond clinical risk factors for AF. We trained a convolutional neural network (ECG-AI) to infer 5-year incident AF risk using 12-lead ECGs in patients receiving longitudinal primary care at Massachusetts General Hospital (MGH). We then fit 3 Cox proportional hazards models, composed of ECG-AI 5-year AF probability, CHARGE-AF clinical risk score (Cohorts for Heart and Aging in Genomic Epidemiology-Atrial Fibrillation), and terms for both ECG-AI and CHARGE-AF (CH-AI), respectively. We assessed model performance by calculating discrimination (area under the receiver operating characteristic curve) and calibration in an internal test set and 2 external test sets (Brigham and Women's Hospital [BWH] and UK Biobank). Models were recalibrated to estimate 2-year AF risk in the UK Biobank given limited available follow-up. We used saliency mapping to identify ECG features most influential on ECG-AI risk predictions and assessed correlation between ECG-AI and CHARGE-AF linear predictors. The training set comprised 45 770 individuals (age 55±17 years, 53% women, 2171 AF events) and the test sets comprised 83 162 individuals (age 59±13 years, 56% women, 2424 AF events). Area under the receiver operating characteristic curve was comparable using CHARGE-AF (MGH, 0.802 [95% CI, 0.767-0.836]; BWH, 0.752 [95% CI, 0.741-0.763]; UK Biobank, 0.732 [95% CI, 0.704-0.759]) and ECG-AI (MGH, 0.823 [95% CI, 0.790-0.856]; BWH, 0.747 [95% CI, 0.736-0.759]; UK Biobank, 0.705 [95% CI, 0.673-0.737]). Area under the receiver operating characteristic curve was highest using CH-AI (MGH, 0.838 [95% CI, 0.807 to 0.869]; BWH, 0.777 [95% CI, 0.766 to 0.788]; UK Biobank, 0.746 [95% CI, 0.716 to 0.776]). Calibration error was low using ECG-AI (MGH, 0.0212; BWH, 0.0129; UK Biobank, 0.0035) and CH-AI (MGH, 0.012; BWH, 0.0108; UK Biobank, 0.0001). In saliency analyses, the ECG P-wave had the greatest influence on AI model predictions. ECG-AI and CHARGE-AF linear predictors were correlated (Pearson <i>r</i>: MGH, 0.61; BWH, 0.66; UK Biobank, 0.41). AI-based analysis of 12-lead ECGs has similar predictive usefulness to a clinical risk factor model for incident AF and the approaches are complementary. ECG-AI may enable efficient quantification of future AF risk.",success
32274894,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"P-wave duration (P<sub>DURATION</sub> ) and P-wave area (P<sub>AREA</sub> ) have been linked to risk of atrial fibrillation (AF), but they do not improve the efficacy of Framingham AF risk score. We suggest the incorporation of both variables in one index, the P-wave area/P-wave duration (P<sub>AREA</sub><sub>/</sub><sub>DURATION</sub> ) index, which may be considered an expression of the average amplitude of the P wave that reflects aspects of P-wave morphology. To assess the prognostic value of P-wave area/P-wave duration index (P<sub>AREA/DURATION</sub> index) in lead II together with other P-wave indices (PWIs) in incidence of AF in the Copenhagen Holter Study. The study included 632 men and women, between 55 and 75 years with no apparent heart disease or AF. Baseline standard 12-lead Electrocardiography (ECGs) were analyzed manually. The median follow-up time was 14.7 (14.5;14.9) years. A total of 68 cases of AF and 233 cases of death were recorded. The restricted cubic spline method showed a U-shaped association between P<sub>AREA/DURATION</sub> and rate of AF. The lowest quintile of P<sub>AREA/DURATION</sub> index in lead II was associated with increased rate of AF, HR 2.80 (1.64-4.79). The addition of the new index to the Framingham model for AF improved the model in this population. The P<sub>AREA</sub> in lead II in its lowest quintile was also associated with increased rate of AF, HR 2.16 (1.25-3.75), but did not improve the Framingham model. P<sub>DURATION</sub> and P-wave terminal force (PTF) were not significantly associated with AF. A flat P wave as expressed by a small P<sub>AREA/DURATION</sub> index in lead II is associated with increased rate of incident AF beyond known AF risk factors.",success
31450977,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Sex and age have long been known to affect the ECG. Several biologic variables and anatomic factors may contribute to sex and age-related differences on the ECG. We hypothesized that a convolutional neural network (CNN) could be trained through a process called deep learning to predict a person's age and self-reported sex using only 12-lead ECG signals. We further hypothesized that discrepancies between CNN-predicted age and chronological age may serve as a physiological measure of health. We trained CNNs using 10-second samples of 12-lead ECG signals from 499 727 patients to predict sex and age. The networks were tested on a separate cohort of 275 056 patients. Subsequently, 100 randomly selected patients with multiple ECGs over the course of decades were identified to assess within-individual accuracy of CNN age estimation. Of 275 056 patients tested, 52% were males and mean age was 58.6±16.2 years. For sex classification, the model obtained 90.4% classification accuracy with an area under the curve of 0.97 in the independent test data. Age was estimated as a continuous variable with an average error of 6.9±5.6 years (R-squared =0.7). Among 100 patients with multiple ECGs over the course of at least 2 decades of life, most patients (51%) had an average error between real age and CNN-predicted age of <7 years. Major factors seen among patients with a CNN-predicted age that exceeded chronologic age by >7 years included: low ejection fraction, hypertension, and coronary disease (P<0.01). In the 27% of patients where correlation was >0.8 between CNN-predicted and chronologic age, no incident events occurred over follow-up (33±12 years). Applying artificial intelligence to the ECG allows prediction of patient sex and estimation of age. The ability of an artificial intelligence algorithm to determine physiological age, with further validation, may serve as a measure of overall health.",success
34397262,False,Letter,,,,,,,,False,,success
36713011,False,Journal Article,,,,,,,,True,"Developing functional machine learning (ML)-based models to address unmet clinical needs requires unique considerations for optimal clinical utility. Recent debates about the rigours, transparency, explainability, and reproducibility of ML models, terms which are defined in this article, have raised concerns about their clinical utility and suitability for integration in current evidence-based practice paradigms. This featured article focuses on increasing the literacy of ML among clinicians by providing them with the knowledge and tools needed to understand and critically appraise clinical studies focused on ML. A checklist is provided for evaluating the rigour and reproducibility of the four ML building blocks: data curation, feature engineering, model development, and clinical deployment. Checklists like this are important for quality assurance and to ensure that ML studies are rigourously and confidently reviewed by clinicians and are guided by domain knowledge of the setting in which the findings will be applied. Bridging the gap between clinicians, healthcare scientists, and ML engineers can address many shortcomings and pitfalls of ML-based solutions and their potential deployment at the bedside.",success
32067584,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Review",,,,,,,,False,,success
34604757,False,Journal Article;Review,,,,,,,,True,"The aim of this review was to assess the evidence for deep learning (DL) analysis of resting electrocardiograms (ECGs) to predict structural cardiac pathologies such as left ventricular (LV) systolic dysfunction, myocardial hypertrophy, and ischaemic heart disease. A systematic literature search was conducted to identify published original articles on end-to-end DL analysis of resting ECG signals for the detection of structural cardiac pathologies. Studies were excluded if the ECG was acquired by ambulatory, stress, intracardiac, or implantable devices, and if the pathology of interest was arrhythmic in nature. After duplicate reviewers screened search results, 12 articles met the inclusion criteria and were included. Three articles used DL to detect LV systolic dysfunction, achieving an area under the curve (AUC) of 0.89-0.93 and an accuracy of 98%. One study used DL to detect LV hypertrophy, achieving an AUC of 0.87 and an accuracy of 87%. Six articles used DL to detect acute myocardial infarction, achieving an AUC of 0.88-1.00 and an accuracy of 83-99.9%. Two articles used DL to detect stable ischaemic heart disease, achieving an accuracy of 95-99.9%. Deep learning models, particularly those that used convolutional neural networks, outperformed rules-based models and other machine learning models. Deep learning is a promising technique to analyse resting ECG signals for the detection of structural cardiac pathologies, which has clinical applicability for more effective screening of asymptomatic populations and expedited diagnostic work-up of symptomatic patients at risk for cardiovascular disease.",success
29292271,False,Journal Article;Systematic Review,,,,,,,,True,"Alarm fatigue threatens patient safety by delaying or reducing clinician response to alarms, which can lead to missed critical events. Interventions to reduce alarms without jeopardizing patient safety target either inaccurate or clinically irrelevant alarms, so assessment of alarm accuracy and clinical relevance may enhance the rigor of alarm intervention studies done in clinical units. To (1) examine approaches used to measure accuracy and/or clinical relevance of physiological monitor alarms in intensive care units and (2) compare the proportions of inaccurate and clinically irrelevant alarms. An integrative review was used to systematically search the literature and synthesize resulting articles. Twelve studies explicitly measuring alarm accuracy and/or clinical relevance on a clinical unit were identified. In the most rigorous studies, alarms were annotated retrospectively by obtaining alarm data and parameter waveforms rather than being annotated in real time. More than half of arrhythmia alarms in recent studies were inaccurate. However, contextual data were needed to determine alarms' clinical relevance. Proportions of clinically irrelevant alarms were high, but definitions of clinically irrelevant alarms often included inaccurate alarms. Future studies testing interventions on clinical units should include alarm accuracy and/or clinical relevance as outcome measures. Arrhythmia alarm accuracy should improve with advances in technology. Clinical interventions should focus on reducing clinically irrelevant alarms, with careful consideration of how clinical relevance is defined and measured.",success
31508497,False,Journal Article,,,,,,,,True,"This work attempts to reduce the number of false alarms generated by bedside monitors in the intensive care unit (ICU), as a majority of current alarms are false. In this study, we applied methods that can be categorized into three stages: signal processing, feature extraction, and optimized machine learning. At the stage of signal processing, we ensured that the heartbeats were properly annotated. During feature extraction, besides extracting features that are relevant to the arrhythmic alarms, we also extracted a set of signal quality indices (SQIs), which we used to distinguish noise/artifact from normal physiological signals. When applying a machine learning algorithm (Random Forest), we performed feature selection in order to reduce the complexity of the models and improve the efficiency of the algorithm. The dataset used is from Reducing False Arrhythmia Alarms in the ICU: the PhysioNet/Computing in Cardiology Challenge 2015. Using the performance metric ""score"" from the Challenge, we achieved a score of 83.08 in the real-time category on the hidden test set, which is the highest in all published work.",success
30417258,False,Journal Article;Review,,,,,,,,True,"The use of machine learning (ML) in healthcare has enormous potential for improving disease detection, clinical decision support, and workflow efficiencies. In this commentary, we review published and potential applications for the use of ML for monitoring within the hospital environment. We present use cases as well as several questions regarding the application of ML to the analysis of the vast amount of complex data that clinicians must interpret in the realm of continuous physiological monitoring. ML, especially employed in bidirectional conjunction with electronic health record data, has the potential to extract much more useful information out of this currently under-analyzed data source from a population level. As a data driven entity, ML is dependent on copious, high quality input data so that error can be introduced by low quality data sources. At present, while ML is being studied in hybrid formulations along with static expert systems for monitoring applications, it is not yet actively incorporated in the formal artificial learning sense of an algorithm constantly learning and updating its rules without external intervention. Finally, innovations in monitoring, including those supported by ML, will pose regulatory and medico-legal challenges, as well as questions regarding precisely how to incorporate these features into clinical care and medical education. Rigorous evaluation of ML techniques compared to traditional methods or other AI methods will be required to validate the algorithms developed with consideration of database limitations and potential learning errors. Demonstration of value on processes and outcomes will be necessary to support the use of ML as a feature in monitoring system development: Future research is needed to evaluate all AI based programs before clinical implementation in non-research settings.",success
25055312,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"We consider an integrated patient monitoring system, combining electronic patient records with high-rate acquisition of patient physiological data. There remain many challenges in increasing the robustness of ""e-health"" applications to a level at which they are clinically useful, particularly in the use of automated algorithms used to detect and cope with artifact in data contained within the electronic patient record, and in analyzing and communicating the resultant data for reporting to clinicians. There is a consequential ""plague of pilots,"" in which engineering prototype systems do not enter into clinical use. This paper describes an approach in which, for the first time, the Emergency Department (ED) of a major research hospital has adopted such systems for use during a large clinical trial. We describe the disadvantages of existing evaluation metrics when applied to such large trials, and propose a solution suitable for large-scale validation. We demonstrate that machine learning technologies embedded within healthcare information systems can provide clinical benefit, with the potential to improve patient outcomes in the busy environment of a major ED and other high-dependence areas of patient care.",success
28033032,False,Journal Article,,,,,,,,True,"Cardiorespiratory insufficiency (CRI) is a term applied to the manifestations of loss of normal cardiorespiratory reserve and portends a bad outcome. CRI occurs commonly in hospitalized patients, but its risk escalation patterns are unexplored. To describe the dynamic and personal character of CRI risk evolution observed through continuous vital sign monitoring of individual step-down unit patients. Using a machine learning model, we estimated risk trends for CRI (defined as exceedance of vital sign stability thresholds) for each of 1,971 admissions (1,880 unique patients) to a 24-bed adult surgical trauma step-down unit at an urban teaching hospital in Pittsburgh, Pennsylvania using continuously recorded vital signs from standard bedside monitors. We compared and contrasted risk trends during initial 4-hour periods after step-down unit admission, and again during the 4 hours immediately before the CRI event, between cases (ever had a CRI) and control subjects (never had a CRI). We further explored heterogeneity of risk escalation patterns during the 4 hours before CRI among cases, comparing personalized to nonpersonalized risk. Estimated risk was significantly higher for cases (918) than control subjects (1,053; P ≤ 0.001) during the initial 4-hour stable periods. Among cases, the aggregated nonpersonalized risk trend increased 2 hours before the CRI, whereas the personalized risk trend became significantly different from control subjects 90 minutes ahead. We further discovered several unique phenotypes of risk escalation patterns among cases for nonpersonalized (14.6% persistently high risk, 18.6% early onset, 66.8% late onset) and personalized risk (7.7% persistently high risk, 8.9% early onset, 83.4% late onset). Insights from this proof-of-concept analysis may guide design of dynamic and personalized monitoring systems that predict CRI, taking into account the triage and real-time monitoring utility of vital signs. These monitoring systems may prove useful in the dynamic allocation of technological and clinical personnel resources in acute care hospitals.",success
35214310,False,Journal Article,,,,,,,,True,"Early recognition of pathologic cardiorespiratory stress and forecasting cardiorespiratory decompensation in the critically ill is difficult even in highly monitored patients in the Intensive Care Unit (ICU). Instability can be intuitively defined as the overt manifestation of the failure of the host to adequately respond to cardiorespiratory stress. The enormous volume of patient data available in ICU environments, both of high-frequency numeric and waveform data accessible from bedside monitors, plus Electronic Health Record (EHR) data, presents a platform ripe for Artificial Intelligence (AI) approaches for the detection and forecasting of instability, and data-driven intelligent clinical decision support (CDS). Building unbiased, reliable, and usable AI-based systems across health care sites is rapidly becoming a high priority, specifically as these systems relate to diagnostics, forecasting, and bedside clinical decision support. The ICU environment is particularly well-positioned to demonstrate the value of AI in saving lives. The goal is to create AI models embedded in a real-time CDS for forecasting and mitigation of critical instability in ICU patients of sufficient readiness to be deployed at the bedside. Such a system must leverage multi-source patient data, machine learning, systems engineering, and human action expertise, the latter being key to successful CDS implementation in the clinical workflow and evaluation of bias. We present one approach to create an operationally relevant AI-based forecasting CDS system.",success
33538696,False,Journal Article;Scoping Review,,,,,,,,True,"Timely identification of patients at a high risk of clinical deterioration is key to prioritizing care, allocating resources effectively, and preventing adverse outcomes. Vital signs-based, aggregate-weighted early warning systems are commonly used to predict the risk of outcomes related to cardiorespiratory instability and sepsis, which are strong predictors of poor outcomes and mortality. Machine learning models, which can incorporate trends and capture relationships among parameters that aggregate-weighted models cannot, have recently been showing promising results. This study aimed to identify, summarize, and evaluate the available research, current state of utility, and challenges with machine learning-based early warning systems using vital signs to predict the risk of physiological deterioration in acutely ill patients, across acute and ambulatory care settings. PubMed, CINAHL, Cochrane Library, Web of Science, Embase, and Google Scholar were searched for peer-reviewed, original studies with keywords related to ""vital signs,"" ""clinical deterioration,"" and ""machine learning."" Included studies used patient vital signs along with demographics and described a machine learning model for predicting an outcome in acute and ambulatory care settings. Data were extracted following PRISMA, TRIPOD, and Cochrane Collaboration guidelines. We identified 24 peer-reviewed studies from 417 articles for inclusion; 23 studies were retrospective, while 1 was prospective in nature. Care settings included general wards, intensive care units, emergency departments, step-down units, medical assessment units, postanesthetic wards, and home care. Machine learning models including logistic regression, tree-based methods, kernel-based methods, and neural networks were most commonly used to predict the risk of deterioration. The area under the curve for models ranged from 0.57 to 0.97. In studies that compared performance, reported results suggest that machine learning-based early warning systems can achieve greater accuracy than aggregate-weighted early warning systems but several areas for further research were identified. While these models have the potential to provide clinical decision support, there is a need for standardized outcome measures to allow for rigorous evaluation of performance across models. Further research needs to address the interpretability of model outputs by clinicians, clinical efficacy of these systems through prospective study design, and their potential impact in different clinical settings.",success
32647214,False,"Journal Article;Observational Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"To assist in the early warning of deterioration in hospitalised children we studied the feasibility of collecting continuous wireless physiological data using Lifetouch (ECG-derived heart and respiratory rate) and WristOx2 (pulse-oximetry and derived pulse rate) sensors. We compared our bedside paediatric early warning (PEW) score and a machine learning automated approach: a Real-time Adaptive Predictive Indicator of Deterioration (RAPID) to identify children experiencing significant clinical deterioration. 982 patients contributed 7,073,486 min during 1,263 monitoring sessions. The proportion of intended monitoring time was 93% for Lifetouch and 55% for WristOx2. Valid clinical data was 63% of intended monitoring time for Lifetouch and 50% WristOx2. 29 patients experienced 36 clinically significant deteriorations. The RAPID Index detected significant deterioration more frequently (77% to 97%) and earlier than the PEW score ≥ 9/26. High sensitivity and negative predictive value for the RAPID Index was associated with low specificity and low positive predictive value. We conclude that it is feasible to collect clinically valid physiological data wirelessly for 50% of intended monitoring time. The RAPID Index identified more deterioration, before the PEW score, but has a low specificity. By using the RAPID Index with a PEW system some life-threatening events may be averted.",success
30992534,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Septic shock is a life-threatening condition in which timely treatment substantially reduces mortality. Reliable identification of patients with sepsis who are at elevated risk of developing septic shock therefore has the potential to save lives by opening an early window of intervention. We hypothesize the existence of a novel clinical state of sepsis referred to as the ""pre-shock"" state, and that patients with sepsis who enter this state are highly likely to develop septic shock at some future time. We apply three different machine learning techniques to the electronic health record data of 15,930 patients in the MIMIC-III database to test this hypothesis. This novel paradigm yields improved performance in identifying patients with sepsis who will progress to septic shock, as defined by Sepsis- 3 criteria, with the best method achieving a 0.93 area under the receiver operating curve, 88% sensitivity, 84% specificity, and median early warning time of 7 hours. Additionally, we introduce the notion of patient-specific positive predictive value, assigning confidence to individual predictions, and achieving values as high as 91%. This study demonstrates that early prediction of impending septic shock, and thus early intervention, is possible many hours in advance.",success
31641162,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Sepsis is a major health concern with global estimates of 31.5 million cases per year. Case fatality rates are still unacceptably high, and early detection and treatment is vital since it significantly reduces mortality rates for this condition. Appropriately designed automated detection tools have the potential to reduce the morbidity and mortality of sepsis by providing early and accurate identification of patients who are at risk of developing sepsis. In this paper, we present ""LiSep LSTM""; a Long Short-Term Memory neural network designed for early identification of septic shock. LSTM networks are typically well-suited for detecting long-term dependencies in time series data. LiSep LSTM was developed using the machine learning framework Keras with a Google TensorFlow back end. The model was trained with data from the Medical Information Mart for Intensive Care database which contains vital signs, laboratory data, and journal entries from approximately 59,000 ICU patients. We show that LiSep LSTM can outperform a less complex model, using the same features and targets, with an AUROC 0.8306 (95% confidence interval: 0.8236, 0.8376) and median offsets between prediction and septic shock onset up to 40 hours (interquartile range, 20 to 135 hours). Moreover, we discuss how our classifier performs at specific offsets before septic shock onset, and compare it with five state-of-the-art machine learning algorithms for early detection of sepsis.",success
20973998,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"In the intensive care unit (ICU), clinical staff must stay vigilant to promptly detect and treat hypotensive episodes (HEs). Given the stressful context of busy ICUs, an automated hypotensive risk stratifier can help ICU clinicians focus care and resources by prospectively identifying patients at increased risk of impending HEs. The objective of this study was to investigate the possible existence of discriminatory patterns in hemodynamic data that can be indicative of future hypotensive risk. Given the complexity and heterogeneity of ICU data, a machine learning approach was used in this study. Time series of minute-by-minute measures of mean arterial blood pressure, heart rate, pulse pressure, and relative cardiac output from 1,311 records from the MIMIC II Database were used. An HE was defined as a 30-minute period during which the mean arterial pressure was below 60 mmHg for at least 90% of the time. Features extracted from the hemodynamic data during an observation period of either 30 or 60 minutes were analyzed to predict the occurrence of HEs 1 or 2 hours into the future. Artificial neural networks (ANNs) were trained for binary classification (normotensive vs. hypotensive) and regression (estimation of future mean blood pressure). The ANNs were successfully trained to discriminate patterns in the multidimensional hemodynamic data that were predictive of future HEs. The best overall binary classification performance resulted in a mean area under ROC curve of 0.918, a sensitivity of 0.826, and a specificity of 0.859. Predicting further into the future resulted in poorer performance, whereas observation duration minimally affected performance. The low prevalence of HEs led to poor positive predictive values. In regression, the best mean absolute error was 9.67%. The promising pattern recognition performance demonstrates the existence of discriminatory patterns in hemodynamic data that can indicate impending hypotension. The poor PPVs discourage a direct HE predictor, but a hypotensive risk stratifier based on the pattern recognition algorithms of this study would be of significant clinical value in busy ICU environments.",success
35854120,False,Journal Article,,,,,,,,True,"There is a large body of evidence showing that delayed initiation of sepsis bundle is associated with adverse clinical outcomes in patients with sepsis. However, it is controversial whether electronic automated alerts can help improve clinical outcomes of sepsis. Electronic databases are searched from inception to December 2021 for comparative effectiveness studies comparing automated alerts versus usual care for the management of sepsis. A total of 36 studies are eligible for analysis, including 6 randomized controlled trials and 30 non-randomized studies. There is significant heterogeneity in these studies concerning the study setting, design, and alerting methods. The Bayesian meta-analysis by using pooled effects of non-randomized studies as priors shows a beneficial effect of the alerting system (relative risk [RR]: 0.71; 95% credible interval: 0.62 to 0.81) in reducing mortality. The automated alerting system shows less beneficial effects in the intensive care unit (RR: 0.90; 95% CI: 0.73-1.11) than that in the emergency department (RR: 0.68; 95% CI: 0.51-0.90) and ward (RR: 0.71; 95% CI: 0.61-0.82). Furthermore, machine learning-based prediction methods can reduce mortality by a larger magnitude (RR: 0.56; 95% CI: 0.39-0.80) than rule-based methods (RR: 0.73; 95% CI: 0.63-0.85). The study shows a statistically significant beneficial effect of using the automated alerting system in the management of sepsis. Interestingly, machine learning monitoring systems coupled with better early interventions show promise, especially for patients outside of the intensive care unit.",success
33981592,False,Journal Article,,,,,,,,True,"Modern Intensive Care Units (ICUs) provide continuous monitoring of critically ill patients susceptible to many complications affecting morbidity and mortality. ICU settings require a high staff-to-patient ratio and generates a sheer volume of data. For clinicians, the real-time interpretation of data and decision-making is a challenging task. Machine Learning (ML) techniques in ICUs are making headway in the early detection of high-risk events due to increased processing power and freely available datasets such as the Medical Information Mart for Intensive Care (MIMIC). We conducted a systematic literature review to evaluate the effectiveness of applying ML in the ICU settings using the MIMIC dataset. A total of 322 articles were reviewed and a quantitative descriptive analysis was performed on 61 qualified articles that applied ML techniques in ICU settings using MIMIC data. We assembled the qualified articles to provide insights into the areas of application, clinical variables used, and treatment outcomes that can pave the way for further adoption of this promising technology and possible use in routine clinical decision-making. The lessons learned from our review can provide guidance to researchers on application of ML techniques to increase their rate of adoption in healthcare.",success
34883382,False,Journal Article,,,,,,,,True,"Cardiac arrest (CA) is the most serious death-related event in critically ill patients and the early detection of CA is beneficial to reduce mortality according to clinical research. This study aims to develop and verify a real-time, interpretable machine learning model, namely cardiac arrest prediction index (CAPI), to predict CA of critically ill patients based on bedside vital signs monitoring. A total of 1,860 patients were analyzed retrospectively from the Medical Information Mart for Intensive Care III (MIMIC-III) database. Based on vital signs, we extracted a total of 43 features for building machine learning model. Extreme Gradient Boosting (XGBoost) was used to develop a real-time prediction model. Three-fold cross validation determined the consistency of model accuracy. SHAP value was used to capture the overall and real-time interpretability of the model. On the test set, CAPI predicted 95% of CA events, 80% of which were identified more than 25 min in advance, resulting in an area under the receiver operating characteristic curve (AUROC) of 0.94. The sensitivity, specificity, area under the precision-recall curve (AUPRC) and F1-score were 0.86, 0.85, 0.12 and 0.05, respectively. CAPI can help predict patients with CA in the vital signs monitoring at bedside. Compared with previous studies, CAPI can give more timely notifications to doctors for CA events. However, current performance was at the cost of alarm fatigue. Future research is still needed to achieve better clinical application.",success
30264218,False,Journal Article,,,,,,,,True,"A cardiac arrest is a life-threatening event, often fatal. Whilst clinicians classify some of the cardiac arrests as potentially predictable, the majority are difficult to identify even in a post-incident analysis. Changes in some patients' physiology when analysed in detail can however be predictive of acute deterioration leading to cardiac or respiratory arrests. This paper seeks to exploit the causally-related changing patterns in signals such as heart rate, respiration rate, systolic blood pressure and peripheral cutaneous oxygen saturation to evaluate the predictability of cardiac arrests in critically ill paediatric patients in intensive care. In this paper we report the results of a framework constituting feature space embedding and time series forecasting methods to build an automated prediction system. The results were compared with clinical assessment of predictability. A sensitivity of 71% and specificity of 69% was obtained when the maximum value of Anomaly Index (12) in the 50 min (starting one hour and ending 10 min) before the arrest was considered for the case patients and a random 50 min of data was considered for the control set patients. A positive predictive value of 11% and negative predictive value of 98% was obtained with a prevalence of 5% by our method of prediction. While clinicians predicted 4 out of the 69 cardiac arrests (6%), the prediction system predicted 63 (91%) cardiac arrests. Prospective validation of the automated system remains.",success
31416562,False,Journal Article,,,,,,,,True,"Sepsis-associated cardiac arrest is a common issue with the low survival rate. Early prediction of cardiac arrest can provide the time required for intervening and preventing its onset in order to reduce mortality. Several studies have been conducted to predict cardiac arrest using machine learning. However, no previous research has used machine learning for predicting cardiac arrest in adult sepsis patients. Moreover, the potential of some techniques, including ensemble algorithms, has not yet been addressed in improving the prediction outcomes. It is required to find methods for generating high-performance predictions with sufficient time lapse before the arrest. In this regard, various variables and parameters should also been examined. The aim was to use machine learning in order to propose a cardiac arrest prediction model for adult patients with sepsis. It is required to predict the arrest several hours before the incidence with high efficiency. The other goal was to investigate the effect of the time series dynamics of vital signs on the prediction of cardiac arrest. 30 h clinical data of every sepsis patients were extracted from Mimic III database (79 cases, 4532 controls). Three datasets (multivariate, time series and combined) were created. Various machine learning models for six time groups were trained on these datasets. The models included classical techniques (SVM, decision tree, logistic regression, KNN, GaussianNB) and ensemble methods (gradient Boosting, XGBoost, random forest, balanced bagging classifier and stacking). Proper solutions were proposed to address the challenges of missing values, imbalanced classes of data and irregularity of time series. The best results were obtained using a stacking algorithm and multivariate dataset (accuracy = 0.76, precision = 0.19, sensitivity = 0.77, f1-score = 0.31, AUC= 0.82). The proposed model predicts the arrest incidence of up to six hours earlier with the accuracy and sensitivity over 70%. We illustrated that machine learning techniques, especially ensemble algorithms have high potentials to be used in prognostic systems for sepsis patients. The proposed model, in comparison with the exiting warning systems including APACHE II and MEWS, significantly improved the evaluation criteria. According to the results, the time series dynamics of vital signs are of great importance in the prediction of cardiac arrest incidence in sepsis patients.",success
32686705,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"In this paper, we propose a novel method for predicting acute clinical deterioration triggered by hypotension, ventricular fibrillation, and an undiagnosed multiple disease condition using biological signals, such as heart rate, RR interval, and blood pressure. Efforts trying to predict such acute clinical deterioration events have received much attention from researchers lately, but most of them are targeted to a single symptom. The distinctive feature of the proposed method is that the occurrence of the event is manifested as a probability by applying a recurrent probabilistic neural network, which is embedded with a hidden Markov model and a Gaussian mixture model. Additionally, its machine learning scheme allows it to learn from the sample data and apply it to a wide range of symptoms. The performance of the proposed method was tested using a dataset provided by Physionet and the University of Tokyo Hospital. The results show that the proposed method has a prediction accuracy of 92.5% for patients with acute hypotension and can predict the occurrence of ventricular fibrillation 5 min before it occurs with an accuracy of 82.5%. In addition, a multiple disease condition can be predicted 7 min before they occur, with an accuracy of over 90%.",success
30767136,False,Journal Article,,,,,,,,True,"Tachycardia is a strong though non-specific marker of cardiovascular stress that proceeds hemodynamic instability. We designed a predictive model of tachycardia using multi-granular intensive care unit (ICU) data by creating a risk score and dynamic trajectory. A subset of clinical and numerical signals were extracted from the Multiparameter Intelligent Monitoring in Intensive Care II database. A tachycardia episode was defined as heart rate ≥ 130/min lasting for ≥ 5 min, with ≥ 10% density. Regularized logistic regression (LR) and random forest (RF) classifiers were trained to create a risk score for upcoming tachycardia. Three different risk score models were compared for tachycardia and control (non-tachycardia) groups. Risk trajectory was generated from time windows moving away at 1 min increments from the tachycardia episode. Trajectories were computed over 3 hours leading up to the episode for three different models. From 2809 subjects, 787 tachycardia episodes and 707 control periods were identified. Patients with tachycardia had increased vasopressor support, longer ICU stay, and increased ICU mortality than controls. In model evaluation, RF was slightly superior to LR, which accuracy ranged from 0.847 to 0.782, with area under the curve from 0.921 to 0.842. Risk trajectory analysis showed average risks for tachycardia group evolved to 0.78 prior to the tachycardia episodes, while control group risks remained < 0.3. Among the three models, the internal control model demonstrated evolving trajectory approximately 75 min before tachycardia episode. Clinically relevant tachycardia episodes can be predicted from vital sign time series using machine learning algorithms.",success
34215511,False,Journal Article,,,,,,,,True,"Using the Medical Information Mart for Intensive Care III (MIMIC-III) database, we compared the performance of machine learning (ML) to the to the established gold standard scoring tool (POAF Score) in predicting postoperative atrial fibrillation (POAF) during intensive care unit (ICU) admission after cardiac surgery. Random forest classifier (RF), decision tree classifier (DT), logistic regression (LR), K neighbours classifier (KNN), support vector machine (SVM), and gradient boosted machine (GBM) were compared to the POAF Score. Cross-validation was used to assess the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity of ML models. POAF Score performance confidence intervals were generated using 1,000 bootstraps. Risk profiles for GBM were generated using Shapley additive values. A total of 6,349 ICU admissions encompassing 6,040 patients were included. POAF occurred in 1,364 of the 6,349 admissions (21.5%). For predicting POAF during ICU admission after cardiac surgery, GBM, LR, RF, KNN, SVM and DT achieved an AUC of 0.74 (0.71-0.77), 0.73 (0.71-0.75), 0.72 (0.69-0.75), 0.68 (0.67-0.69), 0.67 (0.66-0.68) and 0.59 (0.55-0.63) respectively. The POAF Score AUC was 0.63 (0.62-0.64). Shapley additive values analysis of GBM generated patient level explanations for each prediction. Machine learning models based on readily available preoperative data can outperform clinical scoring tools for predicting POAF during ICU admission after cardiac surgery. Explanatory models are shown to have potential in personalising POAF risk profiles for patients by illustrating probabilistic input variable contributions. Future research is required to evaluate the clinical utility and safety of implementing ML-driven tools for POAF prediction.",success
34556682,False,Journal Article,,,,,,,,True,"Critically ill patients affected by atrial fibrillation are at high risk of adverse events: however, the actual risk stratification models for haemorrhagic and thrombotic events are not validated in a critical care setting. With this paper we aimed to identify, adopting topological data analysis, the risk factors for therapeutic failure (in-hospital death or intensive care unit transfer), the in-hospital occurrence of stroke/TIA and major bleeding in a cohort of critically ill patients with pre-existing atrial fibrillation admitted to a stepdown unit; to engineer newer prediction models based on machine learning in the same cohort. We selected all medical patients admitted for critical illness and a history of pre-existing atrial fibrillation in the timeframe 01/01/2002-03/08/2007. All data regarding patients' medical history, comorbidities, drugs adopted, vital parameters and outcomes (therapeutic failure, stroke/TIA and major bleeding) were acquired from electronic medical records. Risk factors for each outcome were analyzed adopting topological data analysis. Machine learning was used to generate three different predictive models. We were able to identify specific risk factors and to engineer dedicated clinical prediction models for therapeutic failure (AUC: 0.974, 95%CI: 0.934-0.975), stroke/TIA (AUC: 0.931, 95%CI: 0.896-0.940; Brier score: 0.13) and major bleeding (AUC: 0.930:0.911-0.939; Brier score: 0.09) in critically-ill patients, which were able to predict accurately their respective clinical outcomes. Topological data analysis and machine learning techniques represent a concrete viewpoint for the physician to predict the risk at the patients' level, aiding the selection of the best therapeutic strategy in critically ill patients affected by pre-existing atrial fibrillation.",success
29664888,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Validation Study",,,,,,,,True,"WHAT THIS ARTICLE TELLS US THAT IS NEW: BACKGROUND:: The authors tested the hypothesis that deep neural networks trained on intraoperative features can predict postoperative in-hospital mortality. The data used to train and validate the algorithm consists of 59,985 patients with 87 features extracted at the end of surgery. Feed-forward networks with a logistic output were trained using stochastic gradient descent with momentum. The deep neural networks were trained on 80% of the data, with 20% reserved for testing. The authors assessed improvement of the deep neural network by adding American Society of Anesthesiologists (ASA) Physical Status Classification and robustness of the deep neural network to a reduced feature set. The networks were then compared to ASA Physical Status, logistic regression, and other published clinical scores including the Surgical Apgar, Preoperative Score to Predict Postoperative Mortality, Risk Quantification Index, and the Risk Stratification Index. In-hospital mortality in the training and test sets were 0.81% and 0.73%. The deep neural network with a reduced feature set and ASA Physical Status classification had the highest area under the receiver operating characteristics curve, 0.91 (95% CI, 0.88 to 0.93). The highest logistic regression area under the curve was found with a reduced feature set and ASA Physical Status (0.90, 95% CI, 0.87 to 0.93). The Risk Stratification Index had the highest area under the receiver operating characteristics curve, at 0.97 (95% CI, 0.94 to 0.99). Deep neural networks can predict in-hospital mortality based on automatically extractable intraoperative data, but are not (yet) superior to existing methods.",success
34303963,False,Journal Article,,,,,,,,True,"Embolic strokes of unknown source (ESUS) are common and often suspected to be caused by unrecognized paroxysmal atrial fibrillation (AF). An AI-enabled ECG (AI-ECG) during sinus rhythm has been shown to identify patients with unrecognized AF. We pursued this study to determine if the AI-ECG model differentiates between patients with ESUS and those with known causes of stroke, and to evaluate whether the AF prediction by AI-ECG among patients with ESUS was associated with the results of prolonged ambulatory cardiac rhythm monitoring. We reviewed consecutive patients admitted with acute ischemic stroke to a comprehensive stroke center between January 2018 and August 2019 and employed the TOAST classification to categorize the mechanisms of ischemia. Use and results of ambulatory cardiac rhythm monitoring after discharge were gathered. We ran the AI-ECG model to obtain AF probabilities from all ECGs acquired during the hospitalization and compared those probabilities in patients with ESUS versus those with known stroke causes (apart from AF), and between patients with and without AF detected by ambulatory cardiac rhythm monitoring. The study cohort had 930 patients, including 263 patients (28.3%) with known AF or AF diagnosed during the index hospitalization and 265 cases (28.5%) categorized as ESUS. Ambulatory cardiac rhythm monitoring was performed in 226 (85.3%) patients with ESUS. AF probability by AI-ECG was not associated with ESUS. However, among patients with ESUS, the probability of AF by AI-ECG was associated with a higher likelihood of AF detection by ambulatory monitoring (P = 0.004). A probability of AF by AI-ECG greater than 0.20 was associated with AF detection by ambulatory cardiac rhythm monitoring with an OR of 5.47 (95% CI 1.51-22.51). AI-ECG may help guide the use of prolonged ambulatory cardiac rhythm monitoring in patients with ESUS to identify those who might benefit from anticoagulation.",success
29297274,False,"Journal Article;Research Support, U.S. Gov't, Non-P.H.S.;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"Blockage of some ion channels and in particular, the hERG (human Ether-a'-go-go-Related Gene) cardiac potassium channel delays cardiac repolarization and can induce arrhythmia. In some cases it leads to a potentially life-threatening arrhythmia known as Torsade de Pointes (TdP). Therefore recognizing drugs with TdP risk is essential. Candidate drugs that are determined not to cause cardiac ion channel blockage are more likely to pass successfully through clinical phases II and III trials (and preclinical work) and not be withdrawn even later from the marketplace due to cardiotoxic effects. The objective of the present study is to develop an SAR (Structure-Activity Relationship) model that can be used as an early screen for torsadogenic (causing TdP arrhythmias) potential in drug candidates. The method is performed using descriptors comprised of atomic NMR chemical shifts (<sup>13</sup>C and <sup>15</sup>N NMR) and corresponding interatomic distances which are combined into a 3D abstract space matrix. The method is called 3D-SDAR (3-dimensional spectral data-activity relationship) and can be interrogated to identify molecular features responsible for the activity, which can in turn yield simplified hERG toxicophores. A dataset of 55 hERG potassium channel inhibitors collected from Kramer et al. consisting of 32 drugs with TdP risk and 23 with no TdP risk was used for training the 3D-SDAR model. An artificial neural network (ANN) with multilayer perceptron was used to define collinearities among the independent 3D-SDAR features. A composite model from 200 random iterations with 25% of the molecules in each case yielded the following figures of merit: training, 99.2%; internal test sets, 66.7%; external (blind validation) test set, 68.4%. In the external test set, 70.3% of positive TdP drugs were correctly predicted. Moreover, toxicophores were generated from TdP drugs. A 3D-SDAR was successfully used to build a predictive model for drug-induced torsadogenic and non-torsadogenic drugs based on 55 compounds. The model was tested in 38 external drugs.",success
32091972,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Drug-induced proarrhythmia is so tightly associated with prolongation of the QT interval that QT prolongation is an accepted surrogate marker for arrhythmia. But QT interval is too sensitive a marker and not selective, resulting in many useful drugs eliminated in drug discovery. To predict the impact of a drug from the drug chemistry on the cardiac rhythm. In a new linkage, we connected atomistic scale information to protein, cell, and tissue scales by predicting drug-binding affinities and rates from simulation of ion channel and drug structure interactions and then used these values to model drug effects on the hERG channel. Model components were integrated into predictive models at the cell and tissue scales to expose fundamental arrhythmia vulnerability mechanisms and complex interactions underlying emergent behaviors. Human clinical data were used for model framework validation and showed excellent agreement, demonstrating feasibility of a new approach for cardiotoxicity prediction. We present a multiscale model framework to predict electrotoxicity in the heart from the atom to the rhythm. Novel mechanistic insights emerged at all scales of the system, from the specific nature of proarrhythmic drug interaction with the hERG channel, to the fundamental cellular and tissue-level arrhythmia mechanisms. Applications of machine learning indicate necessary and sufficient parameters that predict arrhythmia vulnerability. We expect that the model framework may be expanded to make an impact in drug discovery, drug safety screening for a variety of compounds and targets, and in a variety of regulatory processes.",success
32863454,False,Journal Article,,,,,,,,True,"Prolonged QT intervals are a major risk factor for ventricular arrhythmias and a leading cause of sudden cardiac death. Various drugs are known to trigger QT interval prolongation and increase the proarrhythmic potential. Yet, how precisely the action of drugs on the cellular level translates into QT interval prolongation on the whole organ level remains insufficiently understood. Here we use machine learning techniques to systematically characterize the effect of 30 common drugs on the QT interval. We combine information from high fidelity three-dimensional human heart simulations with low fidelity one-dimensional cable simulations to build a surrogate model for the QT interval using multi-fidelity Gaussian process regression. Once trained and cross-validated, we apply our surrogate model to perform sensitivity analysis and uncertainty quantification. Our sensitivity analysis suggests that compounds that block the rapid delayed rectifier potassium current <i>I</i> <sub>Kr</sub> have the greatest prolonging effect of the QT interval, and that blocking the L-type calcium current <i>I</i> <sub>CaL</sub> and late sodium current <i>I</i> <sub>NaL</sub> shortens the QT interval. Our uncertainty quantification allows us to propagate the experimental variability from individual block-concentration measurements into the QT interval and reveals that QT interval uncertainty is mainly driven by the variability in <i>I</i> <sub>Kr</sub> block. In a final validation study, we demonstrate an excellent agreement between our predicted QT interval changes and the changes observed in a randomized clinical trial for the drugs dofetilide, quinidine, ranolazine, and verapamil. We anticipate that both the machine learning methods and the results of this study will have great potential in the efficient development of safer drugs.",success
34863396,False,Journal Article,,,,,,,,True,"To assess whether an electrocardiography-based artificial intelligence (AI) algorithm developed to detect severe ventricular dysfunction (left ventricular ejection fraction [LVEF] of 35% or below) independently predicts long-term mortality after cardiac surgery among patients without severe ventricular dysfunction (LVEF>35%). Patients who underwent valve or coronary bypass surgery at Mayo Clinic (1993-2019) and had documented LVEF above 35% on baseline electrocardiography were included. We compared patients with an abnormal vs a normal AI-enhanced electrocardiogram (AI-ECG) screen for LVEF of 35% or below on preoperative electrocardiography. The primary end point was all-cause mortality. A total of 20,627 patients were included, of whom 17,125 (83.0%) had a normal AI-ECG screen and 3502 (17.0%) had an abnormal AI-ECG screen. Patients with an abnormal AI-ECG screen were older and had more comorbidities. Probability of survival at 5 and 10 years was 86.2% and 68.2% in patients with a normal AI-ECG screen vs 71.4% and 45.1% in those with an abnormal screen (log-rank, P<.01). In the multivariate Cox survival analysis, the abnormal AI-ECG screen was independently associated with a higher all-cause mortality overall (hazard ratio [HR], 1.31; 95% CI, 1.24 to 1.37) and in subgroups of isolated valve surgery (HR, 1.30; 95% CI, 1.18 to 1.42), isolated coronary artery bypass grafting (HR, 1.29; 95% CI, 1.20 to 1.39), and combined coronary artery bypass grafting and valve surgery (HR, 1.19; 95% CI, 1.08 to 1.32). In a subgroup analysis, the association between abnormal AI-ECG screen and mortality was consistent in patients with LVEF of 35% to 55% and among those with LVEF above 55%. A novel electrocardiography-based AI algorithm that predicts severe ventricular dysfunction can predict long-term mortality among patients with LVEF above 35% undergoing valve and/or coronary bypass surgery.",success
33956800,False,Journal Article,,,,,,,,True,"In current anesthesiology practice, anesthesiologists infer the state of unconsciousness without directly monitoring the brain. Drug- and patient-specific electroencephalographic (EEG) signatures of anesthesia-induced unconsciousness have been identified previously. We applied machine learning approaches to construct classification models for real-time tracking of unconscious state during anesthesia-induced unconsciousness. We used cross-validation to select and train the best performing models using 33,159 2s segments of EEG data recorded from 7 healthy volunteers who received increasing infusions of propofol while responding to stimuli to directly assess unconsciousness. Cross-validated models of unconsciousness performed very well when tested on 13,929 2s EEG segments from 3 left-out volunteers collected under the same conditions (median volunteer AUCs 0.99-0.99). Models showed strong generalization when tested on a cohort of 27 surgical patients receiving solely propofol collected in a separate clinical dataset under different circumstances and using different hardware (median patient AUCs 0.95-0.98), with model predictions corresponding with actions taken by the anesthesiologist during the cases. Performance was also strong for 17 patients receiving sevoflurane (alone or in addition to propofol) (median AUCs 0.88-0.92). These results indicate that EEG spectral features can predict unconsciousness, even when tested on a different anesthetic that acts with a similar neural mechanism. With high performance predictions of unconsciousness, we can accurately monitor anesthetic state, and this approach may be used to engineer infusion pumps to intelligibly respond to patients' neural activity.",success
34880365,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Recently, research has been conducted to automatically control anesthesia using machine learning, with the aim of alleviating the shortage of anesthesiologists. In this study, we address the problem of predicting decisions made by anesthesiologists during surgery using machine learning; specifically, we formulate a decision making problem by increasing the flow rate at each time point in the continuous administration of analgesic remifentanil as a supervised binary classification problem. The experiments were conducted to evaluate the prediction performance using six machine learning models: logistic regression, support vector machine, random forest, LightGBM, artificial neural network, and long short-term memory (LSTM), using 210 case data collected during actual surgeries. The results demonstrated that when predicting the future increase in flow rate of remifentanil after 1 min, the model using LSTM was able to predict with scores of 0.659 for sensitivity, 0.732 for specificity, and 0.753 for ROC-AUC; this demonstrates the potential to predict the decisions made by anesthesiologists using machine learning. Furthermore, we examined the importance and contribution of the features of each model using Shapley additive explanations-a method for interpreting predictions made by machine learning models. The trends indicated by the results were partially consistent with known clinical findings.",success
34998516,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Anesthesiologists simultaneously manage several aspects of patient care during general anesthesia. Automating administration of hypnotic agents could enable more precise control of a patient's level of unconsciousness and enable anesthesiologists to focus on the most critical aspects of patient care. Reinforcement learning (RL) algorithms can be used to fit a mapping from patient state to a medication regimen. These algorithms can learn complex control policies that, when paired with modern techniques for promoting model interpretability, offer a promising approach for developing a clinically viable system for automated anesthestic drug delivery. We expand on our prior work applying deep RL to automated anesthetic dosing by now using a continuous-action model based on the actor-critic RL paradigm. The proposed RL agent is composed of a policy network that maps observed anesthetic states to a continuous probability density over propofol-infusion rates and a value network that estimates the favorability of observed states. We train and test three versions of the RL agent using varied reward functions. The agent is trained using simulated pharmacokinetic/pharmacodynamic models with randomized parameters to ensure robustness to patient variability. The model is tested on simulations and retrospectively on nine general anesthesia cases collected in the operating room. We utilize Shapley additive explanations to gain an understanding of the factors with the greatest influence over the agent's decision-making. The deep RL agent significantly outperformed a proportional-integral-derivative controller (median episode median absolute performance error 1.9% ± 1.8 and 3.1% ± 1.1). The model that was rewarded for minimizing total doses performed the best across simulated patient demographics (median episode median performance error 1.1% ± 0.5). When run on real-world clinical datasets, the agent recommended doses that were consistent with those administered by the anesthesiologist. The proposed approach marks the first fully continuous deep RL algorithm for automating anesthestic drug dosing. The reward function used by the RL training algorithm can be flexibly designed for desirable practices (e.g. use less anesthetic) and bolstered performances. Through careful analysis of the learned policies, techniques for interpreting dosing decisions, and testing on clinical data, we confirm that the agent's anesthetic dosing is consistent with our understanding of best-practices in anesthesia care.",success
33176085,False,"Comparative Study;Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Hospitalized adults whose condition deteriorates while they are in wards (outside the intensive care unit [ICU]) have considerable morbidity and mortality. Early identification of patients at risk for clinical deterioration has relied on manually calculated scores. Outcomes after an automated detection of impending clinical deterioration have not been widely reported. On the basis of a validated model that uses information from electronic medical records to identify hospitalized patients at high risk for clinical deterioration (which permits automated, real-time risk-score calculation), we developed an intervention program involving remote monitoring by nurses who reviewed records of patients who had been identified as being at high risk; results of this monitoring were then communicated to rapid-response teams at hospitals. We compared outcomes (including the primary outcome, mortality within 30 days after an alert) among hospitalized patients (excluding those in the ICU) whose condition reached the alert threshold at hospitals where the system was operational (intervention sites, where alerts led to a clinical response) with outcomes among patients at hospitals where the system had not yet been deployed (comparison sites, where a patient's condition would have triggered a clinical response after an alert had the system been operational). Multivariate analyses adjusted for demographic characteristics, severity of illness, and burden of coexisting conditions. The program was deployed in a staggered fashion at 19 hospitals between August 1, 2016, and February 28, 2019. We identified 548,838 non-ICU hospitalizations involving 326,816 patients. A total of 43,949 hospitalizations (involving 35,669 patients) involved a patient whose condition reached the alert threshold; 15,487 hospitalizations were included in the intervention cohort, and 28,462 hospitalizations in the comparison cohort. Mortality within 30 days after an alert was lower in the intervention cohort than in the comparison cohort (adjusted relative risk, 0.84, 95% confidence interval, 0.78 to 0.90; P<0.001). The use of an automated predictive model to identify high-risk patients for whom interventions by rapid-response teams could be implemented was associated with decreased mortality. (Funded by the Gordon and Betty Moore Foundation and others.).",success
35864251,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Machine learning-based clinical decision support tools for sepsis create opportunities to identify at-risk patients and initiate treatments at early time points, which is critical for improving sepsis outcomes. In view of the increasing use of such systems, better understanding of how they are adopted and used by healthcare providers is needed. Here, we analyzed provider interactions with a sepsis early detection tool (Targeted Real-time Early Warning System), which was deployed at five hospitals over a 2-year period. Among 9,805 retrospectively identified sepsis cases, the early detection tool achieved high sensitivity (82% of sepsis cases were identified) and a high rate of adoption: 89% of all alerts by the system were evaluated by a physician or advanced practice provider and 38% of evaluated alerts were confirmed by a provider. Adjusting for patient presentation and severity, patients with sepsis whose alert was confirmed by a provider within 3 h had a 1.85-h (95% CI 1.66-2.00) reduction in median time to first antibiotic order compared to patients with sepsis whose alert was either dismissed, confirmed more than 3 h after the alert or never addressed in the system. Finally, we found that emergency department providers and providers who had previous interactions with an alert were more likely to interact with alerts, as well as to confirm alerts on retrospectively identified patients with sepsis. Beyond efforts to improve the performance of early warning systems, efforts to improve adoption are essential to their clinical impact and should focus on understanding providers' knowledge of, experience with and attitudes toward such systems.",success
29938790,False,"Journal Article;Research Support, Non-U.S. Gov't;Systematic Review",,,,,,,,True,"Sepsis is a life-threatening condition that is usually diagnosed when a patient has a suspected or documented infection, and meets two or more criteria for systemic inflammatory response syndrome (SIRS). The incidence of sepsis is higher among people admitted to critical care settings such as the intensive care unit (ICU) than among people in other settings. If left untreated sepsis can quickly worsen; severe sepsis has a mortality rate of 40% or higher, depending on definition. Recognition of sepsis can be challenging as it usually requires patient data to be combined from multiple unconnected sources, and interpreted correctly, which can be complex and time consuming to do. Electronic systems that are designed to connect information sources together, and automatically collate, analyse, and continuously monitor the information, as well as alerting healthcare staff when pre-determined diagnostic thresholds are met, may offer benefits by facilitating earlier recognition of sepsis and faster initiation of treatment, such as antimicrobial therapy, fluid resuscitation, inotropes, and vasopressors if appropriate. However, there is the possibility that electronic, automated systems do not offer benefits, or even cause harm. This might happen if the systems are unable to correctly detect sepsis (meaning that treatment is not started when it should be, or it is started when it shouldn't be), or healthcare staff may not respond to alerts quickly enough, or get 'alarm fatigue' especially if the alarms go off frequently or give too many false alarms. To evaluate whether automated systems for the early detection of sepsis can reduce the time to appropriate treatment (such as initiation of antibiotics, fluids, inotropes, and vasopressors) and improve clinical outcomes in critically ill patients in the ICU. We searched CENTRAL; MEDLINE; Embase; CINAHL; ISI Web of science; and LILACS, clinicaltrials.gov, and the World Health Organization trials portal. We searched all databases from their date of inception to 18 September 2017, with no restriction on country or language of publication. We included randomized controlled trials (RCTs) that compared automated sepsis-monitoring systems to standard care (such as paper-based systems) in participants of any age admitted to intensive or critical care units for critical illness. We defined an automated system as any process capable of screening patient records or data (one or more systems) automatically at intervals for markers or characteristics that are indicative of sepsis. We defined critical illness as including, but not limited to postsurgery, trauma, stroke, myocardial infarction, arrhythmia, burns, and hypovolaemic or haemorrhagic shock. We excluded non-randomized studies, quasi-randomized studies, and cross-over studies . We also excluded studies including people already diagnosed with sepsis. We used the standard methodological procedures expected by Cochrane. Our primary outcomes were: time to initiation of antimicrobial therapy; time to initiation of fluid resuscitation; and 30-day mortality. Secondary outcomes included: length of stay in ICU; failed detection of sepsis; and quality of life. We used GRADE to assess the quality of evidence for each outcome. We included three RCTs in this review. It was unclear if the RCTs were three separate studies involving 1199 participants in total, or if they were reports from the same study involving fewer participants. We decided to treat the studies separately, as we were unable to make contact with the study authors to clarify.All three RCTs are of very low study quality because of issues with unclear randomization methods, allocation concealment and uncertainty of effect size. Some of the studies were reported as abstracts only and contained limited data, which prevented meaningful analysis and assessment of potential biases.The studies included participants who all received automated electronic monitoring during their hospital stay. Participants were randomized to an intervention group (automated alerts sent from the system) or to usual care (no automated alerts sent from the system).Evidence from all three studies reported 'Time to initiation of antimicrobial therapy'. We were unable to pool the data, but the largest study involving 680 participants reported median time to initiation of antimicrobial therapy in the intervention group of 5.6 hours (interquartile range (IQR) 2.3 to 19.7) in the intervention group (n = not stated) and 7.8 hours (IQR 2.5 to 33.1) in the control group (n = not stated).No studies reported 'Time to initiation of fluid resuscitation' or the adverse event 'Mortality at 30 days'. However very low-quality evidence was available where mortality was reported at other time points. One study involving 77 participants reported 14-day mortality of 20% in the intervention group and 21% in the control group (numerator and denominator not stated). One study involving 442 participants reported mortality at 28 days, or discharge was 14% in the intervention group and 10% in the control group (numerator and denominator not reported). Sample sizes were not reported adequately for these outcomes and so we could not estimate confidence intervals.Very low-quality evidence from one study involving 442 participants reported 'Length of stay in ICU'. Median length of stay was 3.0 days in the intervention group (IQR = 2.0 to 5.0), and 3.0 days (IQR 2.0 to 4.0 in the control).Very low-quality evidence from one study involving at least 442 participants reported the adverse effect 'Failed detection of sepsis'. Data were only reported for failed detection of sepsis in two participants and it wasn't clear which group(s) this outcome occurred in.No studies reported 'Quality of life'. It is unclear what effect automated systems for monitoring sepsis have on any of the outcomes included in this review. Very low-quality evidence is only available on automated alerts, which is only one component of automated monitoring systems. It is uncertain whether such systems can replace regular, careful review of the patient's condition by experienced healthcare staff.",success
29435343,False,Journal Article,NCT03015454,databank,NCT03015454,NCT03015454,NCT03015454,NCT03015454|databank,NCT03015454|databank,True,"Several methods have been developed to electronically monitor patients for severe sepsis, but few provide predictive capabilities to enable early intervention; furthermore, no severe sepsis prediction systems have been previously validated in a randomised study. We tested the use of a machine learning-based severe sepsis prediction system for reductions in average length of stay and in-hospital mortality rate. We conducted a randomised controlled clinical trial at two medical-surgical intensive care units at the University of California, San Francisco Medical Center, evaluating the primary outcome of average length of stay, and secondary outcome of in-hospital mortality rate from December 2016 to February 2017. Adult patients (18+) admitted to participating units were eligible for this factorial, open-label study. Enrolled patients were assigned to a trial arm by a random allocation sequence. In the control group, only the current severe sepsis detector was used; in the experimental group, the machine learning algorithm (MLA) was also used. On receiving an alert, the care team evaluated the patient and initiated the severe sepsis bundle, if appropriate. Although participants were randomly assigned to a trial arm, group assignments were automatically revealed for any patients who received MLA alerts. Outcomes from 75 patients in the control and 67 patients in the experimental group were analysed. Average length of stay decreased from 13.0 days in the control to 10.3 days in the experimental group (p=0.042). In-hospital mortality decreased by 12.4 percentage points when using the MLA (p=0.018), a relative reduction of 58.0%. No adverse events were reported during this trial. The MLA was associated with improved patient outcomes. This is the first randomised controlled trial of a sepsis surveillance system to demonstrate statistically significant differences in length of stay and in-hospital mortality. NCT03015454.",success
35168623,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Coronary heart disease (CHD) has become the leading cause of death and one of the most serious epidemic diseases worldwide. CHD is characterized by urgency, danger and severity, and dynamic treatment strategies for CHD patients are needed. We aimed to build and validate an AI model for dynamic treatment recommendations for CHD patients with the goal of improving patient outcomes and learning best practices from clinicians to help clinical decision support for treating CHD patients. We formed the treatment strategy as a sequential decision problem, and applied an AI supervised reinforcement learning-long short-term memory (SRL-LSTM) framework that combined supervised learning (SL) and reinforcement learning (RL) with an LSTM network to track patients' states to learn a recommendation model that took a patient's diagnosis and evolving health status as input and provided a treatment recommendation in the form of whether to take specific drugs. The experiments were conducted by leveraging a real-world intensive care unit (ICU) database with 13,762 admitted patients diagnosed with CHD. We compared the performance of the applied SRL-LSTM model and several state-of-the-art SL and RL models in reducing the estimated in-hospital mortality and the Jaccard similarity with clinicians' decisions. We used a random forest algorithm to calculate the feature importance of both the clinician policy and the AI policy to illustrate the interpretability of the AI model. Our experimental study demonstrated that the AI model could help reduce the estimated in-hospital mortality through its RL function and learn the best practice from clinicians through its SL function. The similarity between the clinician policy and the AI policy regarding the surviving patients was high, while for the expired patients, it was much lower. The dynamic treatment strategies made by the AI model were clinically interpretable and relied on sensible clinical features extracted according to monitoring indexes and risk factors for CHD patients. We proposed a pipeline for constructing an AI model to learn dynamic treatment strategies for CHD patients that could improve patient outcomes and mimic the best practices of clinicians. And a lot of further studies and efforts are needed to make it practical.",success
33357088,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"<b>Rationale:</b> The Epic Deterioration Index (EDI) is a proprietary prediction model implemented in over 100 U.S. hospitals that was widely used to support medical decision-making during the coronavirus disease (COVID-19) pandemic. The EDI has not been independently evaluated, and other proprietary models have been shown to be biased against vulnerable populations. <b>Objectives:</b> To independently evaluate the EDI in hospitalized patients with COVID-19 overall and in disproportionately affected subgroups. <b>Methods:</b> We studied adult patients admitted with COVID-19 to units other than the intensive care unit at a large academic medical center from March 9 through May 20, 2020. We used the EDI, calculated at 15-minute intervals, to predict a composite outcome of intensive care unit-level care, mechanical ventilation, or in-hospital death. In a subset of patients hospitalized for at least 48 hours, we also evaluated the ability of the EDI to identify patients at low risk of experiencing this composite outcome during their remaining hospitalization. <b>Results:</b> Among 392 COVID-19 hospitalizations meeting inclusion criteria, 103 (26%) met the composite outcome. The median age of the cohort was 64 (interquartile range, 53-75) with 168 (43%) Black patients and 169 (43%) women. The area under the receiver-operating characteristic curve of the EDI was 0.79 (95% confidence interval, 0.74-0.84). EDI predictions did not differ by race or sex. When exploring clinically relevant thresholds of the EDI, we found patients who met or exceeded an EDI of 68.8 made up 14% of the study cohort and had a 74% probability of experiencing the composite outcome during their hospitalization with a sensitivity of 39% and a median lead time of 24 hours from when this threshold was first exceeded. Among the 286 patients hospitalized for at least 48 hours who had not experienced the composite outcome, 14 (13%) never exceeded an EDI of 37.9, with a negative predictive value of 90% and a sensitivity above this threshold of 91%. <b>Conclusions:</b> We found the EDI identifies small subsets of high-risk and low-risk patients with COVID-19 with good discrimination, although its clinical use as an early warning system is limited by low sensitivity. These findings highlight the importance of independent evaluation of proprietary models before widespread operational use among patients with COVID-19.",success
34152373,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"The Epic Sepsis Model (ESM), a proprietary sepsis prediction model, is implemented at hundreds of US hospitals. The ESM's ability to identify patients with sepsis has not been adequately evaluated despite widespread use. To externally validate the ESM in the prediction of sepsis and evaluate its potential clinical value compared with usual care. This retrospective cohort study was conducted among 27 697 patients aged 18 years or older admitted to Michigan Medicine, the academic health system of the University of Michigan, Ann Arbor, with 38 455 hospitalizations between December 6, 2018, and October 20, 2019. The ESM score, calculated every 15 minutes. Sepsis, as defined by a composite of (1) the Centers for Disease Control and Prevention surveillance criteria and (2) International Statistical Classification of Diseases and Related Health Problems, Tenth Revision diagnostic codes accompanied by 2 systemic inflammatory response syndrome criteria and 1 organ dysfunction criterion within 6 hours of one another. Model discrimination was assessed using the area under the receiver operating characteristic curve at the hospitalization level and with prediction horizons of 4, 8, 12, and 24 hours. Model calibration was evaluated with calibration plots. The potential clinical benefit associated with the ESM was assessed by evaluating the added benefit of the ESM score compared with contemporary clinical practice (based on timely administration of antibiotics). Alert fatigue was evaluated by comparing the clinical value of different alerting strategies. We identified 27 697 patients who had 38 455 hospitalizations (21 904 women [57%]; median age, 56 years [interquartile range, 35-69 years]) meeting inclusion criteria, of whom sepsis occurred in 2552 (7%). The ESM had a hospitalization-level area under the receiver operating characteristic curve of 0.63 (95% CI, 0.62-0.64). The ESM identified 183 of 2552 patients with sepsis (7%) who did not receive timely administration of antibiotics, highlighting the low sensitivity of the ESM in comparison with contemporary clinical practice. The ESM also did not identify 1709 patients with sepsis (67%) despite generating alerts for an ESM score of 6 or higher for 6971 of all 38 455 hospitalized patients (18%), thus creating a large burden of alert fatigue. This external validation cohort study suggests that the ESM has poor discrimination and calibration in predicting the onset of sepsis. The widespread adoption of the ESM despite its poor performance raises fundamental concerns about sepsis management on a national level.",success
34973608,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Artificial Intelligence (AI) is increasingly used to support bedside clinical decisions, but information must be presented in usable ways within workflow. Graphical User Interfaces (GUI) are front-facing presentations for communicating AI outputs, but clinicians are not routinely invited to participate in their design, hindering AI solution potential. To inform early user-engaged design of a GUI prototype aimed at predicting future Cardiorespiratory Insufficiency (CRI) by exploring clinician methods for identifying at-risk patients, previous experience with implementing new technologies into clinical workflow, and user perspectives on GUI screen changes. We conducted a qualitative focus group study to elicit iterative design feedback from clinical end-users on an early GUI prototype display. Five online focus group sessions were held, each moderated by an expert focus group methodologist. Iterative design changes were made sequentially, and the updated GUI display was presented to the next group of participants. 23 clinicians were recruited (14 nurses, 4 nurse practitioners, 5 physicians; median participant age ∼35 years; 60% female; median clinical experience 8 years). Five themes emerged from thematic content analysis: trend evolution, context (risk evolution relative to vital signs and interventions), evaluation/interpretation/explanation (sub theme: continuity of evaluation), clinician intuition, and clinical operations. Based on these themes, GUI display changes were made. For example, color and scale adjustments, integration of clinical information, and threshold personalization. Early user-engaged design was useful in adjusting GUI presentation of AI output. Next steps involve clinical testing and further design modification of the AI output to optimally facilitate clinician surveillance and decisions. Clinicians should be involved early and often in clinical decision support design to optimize efficacy of AI tools.",success
27784047,True,"Comparative Study;Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"Several attempts have been made at developing models to predict 30-day readmissions in patients with heart failure, but none have sufficient discriminatory capacity for clinical use. Machine-learning (ML) algorithms represent a novel approach and may have potential advantages over traditional statistical modeling. To develop models using a ML approach to predict all-cause readmissions 30 days after discharge from a heart failure hospitalization and to compare ML model performance with models developed using ""conventional"" statistically based methods. Models were developed using ML algorithms, specifically, a tree-augmented naive Bayesian network, a random forest algorithm, and a gradient-boosted model and compared with traditional statistical methods using 2 independently derived logistic regression models (a de novo model and an a priori model developed using electronic health records) and a least absolute shrinkage and selection operator method. The study sample was randomly divided into training (70%) and validation (30%) sets to develop and test model performance. This was a registry-based study, and the study sample was obtained by linking patients from the Get With the Guidelines Heart Failure registry with Medicare data. After applying appropriate inclusion and exclusion criteria, 56 477 patients were included in our analysis. The study was conducted between January 4, 2005, and December 1, 2010, and analysis of the data was conducted between November 25, 2014, and June 30, 2016. C statistics were used for comparison of discriminatory capacity across models in the validation sample. The overall 30-day rehospitalization rate was 21.2% (11 959 of 56 477 patients). For the tree-augmented naive Bayesian network, random forest, gradient-boosted, logistic regression, and least absolute shrinkage and selection operator models, C statistics for the validation sets were similar: 0.618, 0.607, 0.614, 0.624, and 0.618, respectively. Applying the previously validated electronic health records model to our study sample yielded a C statistic of 0.589 for the validation set. Use of a number of ML algorithms did not improve prediction of 30-day heart failure readmissions compared with more traditional prediction models. Although there will likely be further applications of ML approaches in prognostic modeling, our study fits within the literature of limited predictive ability for heart failure readmissions.",success
26391638,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Congestive Heart Failure (CHF) is a serious cardiac condition that brings high risks of urgent hospitalization and death. Remote monitoring systems are well-suited to managing patients suffering from CHF, and can reduce deaths and re-hospitalizations, as shown by the literature, including multiple systematic reviews. The monitoring system proposed in this paper aims at helping CHF stakeholders make appropriate decisions in managing the disease and preventing cardiac events, such as decompensation, which can lead to hospitalization or death. Monitoring activities are stratified into three layers: scheduled visits to a hospital following up on a cardiac event, home monitoring visits by nurses, and patient's self-monitoring performed at home using specialized equipment. Appropriate hardware, desktop and mobile software applications were developed to enable a patient's monitoring by all stakeholders. For the first two layers, we designed and implemented a Decision Support System (DSS) using machine learning (Random Forest algorithm) to predict the number of decompensations per year and to assess the heart failure severity based on a variety of clinical data. For the third layer, custom-designed sensors (the Blue Scale system) for electrocardiogram (EKG), pulse transit times, bio-impedance and weight allowed frequent collection of CHF-related data in the comfort of the patient's home. We also performed a short-term Heart Rate Variability (HRV) analysis on electrocardiograms self-acquired by 15 healthy volunteers and compared the obtained parameters with those of 15 CHF patients from PhysioNet's PhysioBank archives. We report numerical performances of the DSS, calculated as multiclass accuracy, sensitivity and specificity in a 10-fold cross-validation. The obtained average accuracies are: 71.9% in predicting the number of decompensations and 81.3% in severity assessment. The most serious class in severity assessment is detected with good sensitivity and specificity (0.87 / 0.95), while, in predicting decompensation, high specificity combined with good sensitivity prevents false alarms. The HRV parameters extracted from the self-measured EKG using the Blue Scale system of sensors are comparable with those reported in the literature about healthy people. The performance of DSSs trained with new patients confirmed the results of previous work, and emphasizes the strong correlation between some CHF markers, such as brain natriuretic peptide (BNP) and ejection fraction (EF), with the outputs of interest. Comparing HRV parameters from healthy volunteers with HRV parameters obtained from PhysioBank archives, we confirm the literature that considers the HRV a promising method for distinguishing healthy from CHF patients.",success
29551433,False,"Evaluation Study;Journal Article;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"The impact of telehealth and remote patient monitoring has not been well established in palliative care populations in rural communities. The objectives of this study were to 1) describe a telehealth palliative care program using the TapCloud remote patient monitoring application and videoconferencing; 2) evaluate the feasibility, usability, and acceptability of a telehealth system in palliative care; and 3) use a quality data assessment collection tool in addition to TapCloud ratings of symptom burden and hospice transitions. A mixed-methods approach was used to assess feasibility, usability, and acceptability. Quantitative assessments included patient symptom burden and improvement, hospice transitions, and advanced directives. Qualitative semistructured interviews on a subpopulation of telehealth patients, caregivers, and providers were performed to learn about their experiences using TapCloud. One-hundred one palliative care patients in rural Western North Carolina were enrolled in the program. The mean age of patients enrolled was 72 years, with a majority (60%) being female and a pulmonary diagnosis accounting for the largest percentage of patients (23%). Remote patient monitoring using TapCloud resulted in improved symptom management, and patients in the model had a hospice transition rate of 35%. Patients, caregivers, and providers reported overwhelmingly positive experiences with telehealth with three main advantages: 1) access to clinicians, 2) quick responses, and 3) improved efficiency and quality of care. This is one of the first articles to describe a telehealth palliative care program and to demonstrate acceptability, feasibility, and usability as well as describe symptom outcomes and hospice transitions.",success
31605093,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Artificial intelligence (AI) holds promise for cardiovascular medicine but is limited by a lack of large, heterogeneous and granular data sets. Blockchain provides secure interoperability between siloed stakeholders and centralized data sources. We discuss integration of blockchain with AI for data-centric analysis and information flow, its current limitations and potential cardiovascular applications.",success
32047863,False,Journal Article,,,,,,,,True,"As wearable technologies are being increasingly used for clinical research and healthcare, it is critical to understand their accuracy and determine how measurement errors may affect research conclusions and impact healthcare decision-making. Accuracy of wearable technologies has been a hotly debated topic in both the research and popular science literature. Currently, wearable technology companies are responsible for assessing and reporting the accuracy of their products, but little information about the evaluation method is made publicly available. Heart rate measurements from wearables are derived from photoplethysmography (PPG), an optical method for measuring changes in blood volume under the skin. Potential inaccuracies in PPG stem from three major areas, includes (1) diverse skin types, (2) motion artifacts, and (3) signal crossover. To date, no study has systematically explored the accuracy of wearables across the full range of skin tones. Here, we explored heart rate and PPG data from consumer- and research-grade wearables under multiple circumstances to test whether and to what extent these inaccuracies exist. We saw no statistically significant difference in accuracy across skin tones, but we saw significant differences between devices, and between activity types, notably, that absolute error during activity was, on average, 30% higher than during rest. Our conclusions indicate that different wearables are all reasonably accurate at resting and prolonged elevated heart rate, but that differences exist between devices in responding to changes in activity. This has implications for researchers, clinicians, and consumers in drawing study conclusions, combining study results, and making health-related decisions using these devices.",success
23899591,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Correct detection and classification of ventricular fibrillation (VF) and rapid ventricular tachycardia (VT) is of pivotal importance for an automatic external defibrillator and patient monitoring. In this paper, a VF/VT classification algorithm using a machine learning method, a support vector machine, is proposed. A total of 14 metrics were extracted from a specific window length of the electrocardiogram (ECG). A genetic algorithm was then used to select the optimal variable combinations. Three annotated public domain ECG databases (the American Heart Association Database, the Creighton University Ventricular Tachyarrhythmia Database, and the MIT-BIH Malignant Ventricular Arrhythmia Database) were used as training, test, and validation datasets. Different window sizes, varying from 1 to 10 s were tested. An accuracy (Ac) of 98.1%, sensitivity (Se) of 98.4%, and specificity (Sp) of 98.0% were obtained on the in-sample training data with 5 s-window size and two selected metrics. On the out-of-sample validation data, an Ac of 96.3% ± 3.4%, Se of 96.2% ± 2.7%, and Sp of 96.2% ± 4.6% were obtained by fivefold cross validation. The results surpass those of current reported methods.",success
29562087,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Atrial fibrillation (AF) affects 34 million people worldwide and is a leading cause of stroke. A readily accessible means to continuously monitor for AF could prevent large numbers of strokes and death. To develop and validate a deep neural network to detect AF using smartwatch data. In this multinational cardiovascular remote cohort study coordinated at the University of California, San Francisco, smartwatches were used to obtain heart rate and step count data for algorithm development. A total of 9750 participants enrolled in the Health eHeart Study and 51 patients undergoing cardioversion at the University of California, San Francisco, were enrolled between February 2016 and March 2017. A deep neural network was trained using a method called heuristic pretraining in which the network approximated representations of the R-R interval (ie, time between heartbeats) without manual labeling of training data. Validation was performed against the reference standard 12-lead electrocardiography (ECG) in a separate cohort of patients undergoing cardioversion. A second exploratory validation was performed using smartwatch data from ambulatory individuals against the reference standard of self-reported history of persistent AF. Data were analyzed from March 2017 to September 2017. The sensitivity, specificity, and receiver operating characteristic C statistic for the algorithm to detect AF were generated based on the reference standard of 12-lead ECG-diagnosed AF. Of the 9750 participants enrolled in the remote cohort, including 347 participants with AF, 6143 (63.0%) were male, and the mean (SD) age was 42 (12) years. There were more than 139 million heart rate measurements on which the deep neural network was trained. The deep neural network exhibited a C statistic of 0.97 (95% CI, 0.94-1.00; P < .001) to detect AF against the reference standard 12-lead ECG-diagnosed AF in the external validation cohort of 51 patients undergoing cardioversion; sensitivity was 98.0% and specificity was 90.2%. In an exploratory analysis relying on self-report of persistent AF in ambulatory participants, the C statistic was 0.72 (95% CI, 0.64-0.78); sensitivity was 67.7% and specificity was 67.6%. This proof-of-concept study found that smartwatch photoplethysmography coupled with a deep neural network can passively detect AF but with some loss of sensitivity and specificity against a criterion-standard ECG. Further studies will help identify the optimal role for smartwatch-guided rhythm assessment.",success
31722151,True,"Journal Article;Pragmatic Clinical Trial;Research Support, Non-U.S. Gov't",NCT03335800,databank,NCT03335800,NCT03335800,NCT03335800,NCT03335800|databank,NCT03335800|databank,True,"Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatch-based irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed - which had been applied, on average, 13 days after notification - atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.).",success
30617320,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow<sup>1</sup>. Widely available digital ECG data and the algorithmic paradigm of deep learning<sup>2</sup> present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F<sub>1</sub> score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions.",success
28391210,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"We present a smartphone-only solution for the detection of atrial fibrillation (AFib), which utilizes the built-in accelerometer and gyroscope sensors [inertial measurement unit, (IMU)] in the detection. Depending on the patient's situation, it is possible to use the developed smartphone application either regularly or occasionally for making a measurement of the subject. The smartphone is placed on the chest of the patient who is adviced to lay down and perform a noninvasive recording, while no external sensors are needed. After that, the application determines whether the patient suffers from AFib or not. The presented method has high potential to detect paroxysmal (""silent"") AFib from large masses. In this paper, we present the preprocessing, feature extraction, feature analysis, and classification results of the envisioned AFib detection system based on clinical data acquired with a standard mobile phone equipped with Google Android OS. Test data was gathered from 16 AFib patients (validated against ECG), as well as a control group of 23 healthy individuals with no diagnosed heart diseases. We obtained an accuracy of 97.4% in AFib versus healthy classification (a sensitivity of 93.8% and a specificity of 100%). Due to the wide availability of smart devices/sensors with embedded IMU, the proposed methods could potentially also scale to other domains such as embedded body-sensor networks.",success
30784691,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",NCT02956343,databank,NCT02956343,NCT02956343,NCT02956343,NCT02956343|databank,NCT02956343|databank,True,"The WATCH AF (SmartWATCHes for Detection of Atrial Fibrillation) trial compared the diagnostic accuracy to detect atrial fibrillation (AF) by a smartwatch-based algorithm using photoplethysmographic (PPG) signals with cardiologists' diagnosis by electrocardiography (ECG). Timely detection of AF is crucial for stroke prevention. In this prospective, 2-center, case-control trial, a PPG pulse wave recording using a commercially available smartwatch was obtained along with Internet-enabled mobile ECG in 672 hospitalized subjects. PPG recordings were analyzed by a novel automated algorithm. Cardiologists' diagnoses were available for 650 subjects, although 142 (21.8%) datasets were not suitable for PPG analysis, among them 101 (15.1%) that were also not interpretable by the automated Internet-enabled mobile ECG algorithm, resulting in a sample size of 508 subjects (mean age 76.4 years, 225 women, 237 with AF) for the main analyses. For the PPG algorithm, we found a sensitivity of 93.7% (95% confidence interval [CI]: 89.8% to 96.4%), a specificity of 98.2% (95% CI: 95.8% to 99.4%), and 96.1% accuracy (95% CI: 94.0% to 97.5%) to detect AF. The results of the WATCH AF trial suggest that detection of AF using a commercially available smartwatch is in principle feasible, with very high diagnostic accuracy. Applicability of the tested algorithm is currently limited by a high dropout rate as a result of insufficient signal quality. Thus, achieving sufficient signal quality remains challenging, but real-time signal quality checks are expected to improve signal quality. Whether smartwatches may be useful complementary tools for convenient long-term AF screening in selected at-risk patients must be evaluated in larger population-based samples. (SmartWATCHes for Detection of Atrial Fibrillation [WATCH AF]:; NCT02956343).",success
32093506,True,"Journal Article;Multicenter Study;Observational Study;Research Support, U.S. Gov't, Non-P.H.S.",NCT03037710,databank,NCT03037710,NCT03037710,NCT03037710,NCT03037710|databank,NCT03037710|databank,True,"Implantable cardiac sensors have shown promise in reducing rehospitalization for heart failure (HF), but the efficacy of noninvasive approaches has not been determined. The objective of this study was to determine the accuracy of noninvasive remote monitoring in predicting HF rehospitalization. The LINK-HF study (Multisensor Non-invasive Remote Monitoring for Prediction of Heart Failure Exacerbation) examined the performance of a personalized analytical platform using continuous data streams to predict rehospitalization after HF admission. Study subjects were monitored for up to 3 months using a disposable multisensor patch placed on the chest that recorded physiological data. Data were uploaded continuously via smartphone to a cloud analytics platform. Machine learning was used to design a prognostic algorithm to detect HF exacerbation. Clinical events were formally adjudicated. One hundred subjects aged 68.4±10.2 years (98% male) were enrolled. After discharge, the analytical platform derived a personalized baseline model of expected physiological values. Differences between baseline model estimated vital signs and actual monitored values were used to trigger a clinical alert. There were 35 unplanned nontrauma hospitalization events, including 24 worsening HF events. The platform was able to detect precursors of hospitalization for HF exacerbation with 76% to 88% sensitivity and 85% specificity. Median time between initial alert and readmission was 6.5 (4.2-13.7) days. Multivariate physiological telemetry from a wearable sensor can provide accurate early detection of impending rehospitalization with a predictive accuracy comparable to implanted devices. The clinical efficacy and generalizability of this low-cost noninvasive approach to rehospitalization mitigation should be further tested. Registration: URL: https://www.clinicaltrials.gov. Unique Identifier: NCT03037710.",success
32163936,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Heart failure (HF) can be difficult to diagnose by physical examination alone. We examined whether wristband technologies may facilitate more accurate bedside testing. We studied on a cohort of 97 monitored in-patients and performed a cross-sectional analysis to predict HF with data from the wearable and other clinically available data. We recorded photoplethysmography (PPG) and accelerometry data using the wearable at 128 samples per second for 5 min. HF diagnosis was ascertained via chart review. We extracted four features of beat-to-beat variability and signal quality, and used them as inputs to a machine learning classification algorithm. The median [interquartile] age was 60 [51 68] years, 65% were men, and 54% had heart failure; in addition, 30% had acutely decompensated HF. The best 10-fold cross-validated testing performance for the diagnosis of HF was achieved using a support vector machine. The waveform-based features alone achieved a pooled test area under the curve (AUC) of 0.80; when a high-sensitivity cut-point (90%) was chosen, the specificity was 50%. When adding demographics, medical history, and vital signs, the AUC improved to 0.87, and specificity improved to 72% (90% sensitivity). In a cohort of monitored in-patients, we were able to build an HF classifier from data gathered on a wristband wearable. To our knowledge, this is the first study to demonstrate an algorithm using wristband technology to classify HF patients. This supports the use of such a device as an adjunct tool in bedside diagnostic evaluation and risk stratification.",success
30759094,False,Journal Article,,,,,,,,True,"This study trained long short-term memory (LSTM) recurrent neural networks (RNNs) incorporating an attention mechanism to predict daily sepsis, myocardial infarction (MI), and vancomycin antibiotic administration over two week patient ICU courses in the MIMIC-III dataset. These models achieved next-day predictive AUC of 0.876 for sepsis, 0.823 for MI, and 0.833 for vancomycin administration. Attention maps built from these models highlighted those times when input variables most influenced predictions and could provide a degree of interpretability to clinicians. These models appeared to attend to variables that were proxies for clinician decision-making, demonstrating a challenge of using flexible deep learning approaches trained with EHR data to build clinical decision support. While continued development and refinement is needed, we believe that such models could one day prove useful in reducing information overload for ICU physicians by providing needed clinical decision support for a variety of clinically important tasks.",success
36711170,False,Journal Article,,,,,,,,True,"This collaborative statement from the International Society for Holter and Noninvasive Electrocardiology / Heart Rhythm Society / European Heart Rhythm Association / Asia Pacific Heart Rhythm Society describes the current status of mobile health (""mHealth"") technologies in arrhythmia management. The range of digital medical tools and heart rhythm disorders that they may be applied to and clinical decisions that may be enabled are discussed. The facilitation of comorbidity and lifestyle management (increasingly recognized to play a role in heart rhythm disorders) and patient self-management are novel aspects of mHealth. The promises of predictive analytics but also operational challenges in embedding mHealth into routine clinical care are explored.",success
31077802,False,Journal Article,,,,,,,,False,,success
22163626,False,Journal Article;Review,,,,,,,,True,"Characteristics of physical activity are indicative of one's mobility level, latent chronic diseases and aging process. Accelerometers have been widely accepted as useful and practical sensors for wearable devices to measure and assess physical activity. This paper reviews the development of wearable accelerometry-based motion detectors. The principle of accelerometry measurement, sensor properties and sensor placements are first introduced. Various research using accelerometry-based wearable motion detectors for physical activity monitoring and assessment, including posture and movement classification, estimation of energy expenditure, fall detection and balance control evaluation, are also reviewed. Finally this paper reviews and compares existing commercial products to provide a comprehensive outlook of current development status and possible emerging technologies.",success
31974209,False,Editorial;Comment,,,,,,,,False,,success
32979046,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"The study sought to characterize the evaluation of patients who present following detection of an abnormal pulse using Apple Watch. We conducted a retrospective review of patients evaluated for abnormal pulse detected using Apple Watch over a 4-month period. Among 264 included patients, clinical documentation for 41 (15.5%) explicitly noted an abnormal pulse alert. Preexisting atrial fibrillation was noted in 58 (22.0%). Most commonly performed testing included 12-lead echocardiography (n = 158; 59.8%), Holter monitor (n = 77; 29.2%), and chest x-ray (n = 64; 24.2%). A clinically actionable cardiovascular diagnosis of interest was established in only 30 (11.4%) patients, including 6 of 41 (15%) patients who received an explicit alert. False positive screening results may lead to overutilization of healthcare resources. The Food and Drug Administration and Apple should consider the unintended consequences of widespread screening for asymptomatic (""silent"") atrial fibrillation and use of the Apple Watch abnormal pulse detection functionality by populations in whom the device has not been adequately studied.",success
31304399,False,Journal Article,,,,,,,,True,"Wearable biometric monitoring devices (BMDs) and artificial intelligence (AI) enable the remote measurement and analysis of patient data in real time. These technologies have generated a lot of ""hype,"" but their real-world effectiveness will depend on patients' uptake. Our objective was to describe patients' perceptions of the use of BMDs and AI in healthcare. We recruited adult patients with chronic conditions in France from the ""Community of Patients for Research"" (ComPaRe). Participants (1) answered quantitative and open-ended questions about the potential benefits and dangers of using of these new technologies and (2) participated in a case-vignette experiment to assess their readiness for using BMDs and AI in healthcare. Vignettes covered the use of AI to screen for skin cancer, remote monitoring of chronic conditions to predict exacerbations, smart clothes to guide physical therapy, and AI chatbots to answer emergency calls. A total of 1183 patients (51% response rate) were enrolled between May and June 2018. Overall, 20% considered that the benefits of technology (e.g., improving the reactivity in care and reducing the burden of treatment) greatly outweighed the dangers. Only 3% of participants felt that negative aspects (inadequate replacement of human intelligence, risks of hacking and misuse of private patient data) greatly outweighed potential benefits. We found that 35% of patients would refuse to integrate at least one existing or soon-to-be available intervention using BMDs and AI-based tools in their care. Accounting for patients' perspectives will help make the most of technology without impairing the human aspects of care, generating a burden or intruding on patients' lives.",success
34822263,False,News,,,,,,,,False,,success
32736792,False,"Journal Article;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Most neuropsychiatric disorders are highly polygenic, implicating hundreds to thousands of causal genetic variants that span much of the genome. This widespread polygenicity complicates biological understanding because no single variant can explain disease etiology. A strategy to advance biological insight is to seek convergent functions among the large set of variants and map them to a smaller set of disease-relevant genes and pathways. Accordingly, functional genomic resources that provide data on intermediate molecular phenotypes, such as gene-expression and methylation status, can be leveraged to functionally annotate variants and map them to genes. Such molecular quantitative trait locus mappings can be integrated with genome-wide association studies to make sense of the polygenic signal that underlies complex disease. Other resources that provide data on the 3-dimensional structure of chromatin and functional importance of specific genomic regions can be integrated similarly. In addition, mapped genes can then be tested for convergence in biological function, tissue, cell type, or developmental stage. In this review, we provide an overview of functional genomic resources and methods that can be used to interpret results from genome-wide association studies, and we discuss current challenges for biological understanding and future requirements to overcome them.",success
29448949,False,"Letter;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The accurate description of ancestry is essential to interpret, access, and integrate human genomics data, and to ensure that these benefit individuals from all ancestral backgrounds. However, there are no established guidelines for the representation of ancestry information. Here we describe a framework for the accurate and standardized description of sample ancestry, and validate it by application to the NHGRI-EBI GWAS Catalog. We confirm known biases and gaps in diversity, and find that African and Hispanic or Latin American ancestry populations contribute a disproportionately high number of associations. It is our hope that widespread adoption of this framework will lead to improved analysis, interpretation, and integration of human genomics data.",success
34140649,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",,,NTR6831,,,,,True,"Array technology to genotype single-nucleotide variants (SNVs) is widely used in genome-wide association studies (GWAS), clinical diagnostics, and linkage studies. Arrays have undergone a tremendous growth in both number and content over recent years making a comprehensive comparison all the more important. We have compared 28 genotyping arrays on their overall content, genome-wide coverage, imputation quality, presence of known GWAS loci, mtDNA variants and clinically relevant genes (i.e., American College of Medical Genetics (ACMG) actionable genes, pharmacogenetic genes, human leukocyte antigen (HLA) genes and SNV density). Our comparison shows that genome-wide coverage is highly correlated with the number of SNVs on the array but does not correlate with imputation quality, which is the main determinant of GWAS usability. Average imputation quality for all tested arrays was similar for European and African populations, indicating that this is not a good criterion for choosing a genotyping array. Rather, the additional content on the array, such as pharmacogenetics or HLA variants, should be the deciding factor. As the research question of a study will in large part determine which class of genes are of interest, there is not just one perfect array for all different research questions. This study can thus help as a guideline to determine which array best suits a study's requirements.",success
30445434,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The GWAS Catalog delivers a high-quality curated collection of all published genome-wide association studies enabling investigations to identify causal variants, understand disease mechanisms, and establish targets for novel therapies. The scope of the Catalog has also expanded to targeted and exome arrays with 1000 new associations added for these technologies. As of September 2018, the Catalog contains 5687 GWAS comprising 71673 variant-trait associations from 3567 publications. New content includes 284 full P-value summary statistics datasets for genome-wide and new targeted array studies, representing 6 × 109 individual variant-trait statistics. In the last 12 months, the Catalog's user interface was accessed by ∼90000 unique users who viewed >1 million pages. We have improved data access with the release of a new RESTful API to support high-throughput programmatic access, an improved web interface and a new summary statistics database. Summary statistics provision is supported by a new format proposed as a community standard for summary statistics data representation. This format was derived from our experience in standardizing heterogeneous submissions, mapping formats and in harmonizing content. Availability: https://www.ebi.ac.uk/gwas/.",success
19249008,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Previous genome-wide association (GWA) studies typically focus on single-locus analysis, which may not have the power to detect the majority of genuinely associated loci. Here, we applied pathway analysis using Affymetrix SNP genotype data from the Wellcome Trust Case Control Consortium (WTCCC) and uncovered significant association between Crohn Disease (CD) and the IL12/IL23 pathway, harboring 20 genes (p = 8 x 10(-5)). Interestingly, the pathway contains multiple genes (IL12B and JAK2) or homologs of genes (STAT3 and CCR6) that were recently identified as genuine susceptibility genes only through meta-analysis of several GWA studies. In addition, the pathway contains other susceptibility genes for CD, including IL18R1, JUN, IL12RB1, and TYK2, which do not reach genome-wide significance by single-marker association tests. The observed pathway-specific association signal was subsequently replicated in three additional GWA studies of European and African American ancestry generated on the Illumina HumanHap550 platform. Our study suggests that examination beyond individual SNP hits, by focusing on genetic networks and pathways, is important to unleashing the true power of GWA studies.",success
30104762,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"A key public health need is to identify individuals at high risk for a given disease to enable enhanced screening or preventive therapies. Because most common diseases have a genetic component, one important approach is to stratify individuals based on inherited DNA variation<sup>1</sup>. Proposed clinical applications have largely focused on finding carriers of rare monogenic mutations at several-fold increased risk. Although most disease risk is polygenic in nature<sup>2-5</sup>, it has not yet been possible to use polygenic predictors to identify individuals at risk comparable to monogenic mutations. Here, we develop and validate genome-wide polygenic scores for five common diseases. The approach identifies 8.0, 6.1, 3.5, 3.2, and 1.5% of the population at greater than threefold increased risk for coronary artery disease, atrial fibrillation, type 2 diabetes, inflammatory bowel disease, and breast cancer, respectively. For coronary artery disease, this prevalence is 20-fold higher than the carrier frequency of rare monogenic mutations conferring comparable risk<sup>6</sup>. We propose that it is time to contemplate the inclusion of polygenic risk prediction in clinical care, and discuss relevant issues.",success
35183061,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Deep learning is a promising tool that uses nonlinear transformations to extract features from high-dimensional data. Deep learning is challenging in genome-wide association studies (GWAS) with high-dimensional genomic data. Here we propose a novel three-step approach (SWAT-CNN) for identification of genetic variants using deep learning to identify phenotype-related single nucleotide polymorphisms (SNPs) that can be applied to develop accurate disease classification models. In the first step, we divided the whole genome into nonoverlapping fragments of an optimal size and then ran convolutional neural network (CNN) on each fragment to select phenotype-associated fragments. In the second step, using a Sliding Window Association Test (SWAT), we ran CNN on the selected fragments to calculate phenotype influence scores (PIS) and identify phenotype-associated SNPs based on PIS. In the third step, we ran CNN on all identified SNPs to develop a classification model. We tested our approach using GWAS data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) including (N = 981; cognitively normal older adults (CN) = 650 and AD = 331). Our approach identified the well-known APOE region as the most significant genetic locus for AD. Our classification model achieved an area under the curve (AUC) of 0.82, which was compatible with traditional machine learning approaches, random forest and XGBoost. SWAT-CNN, a novel deep learning-based genome-wide approach, identified AD-associated SNPs and a classification model for AD and may hold promise for a range of biomedical applications.",success
35995843,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,10.6084/M9.FIGSHARE.20304135.V1;10.6084/M9.FIGSHARE.20301423.V1,,,,,True,"Polygenic risk scores (PRS) are commonly used to quantify the inherited susceptibility for a trait, yet they fail to account for non-linear and interaction effects between single nucleotide polymorphisms (SNPs). We address this via a machine learning approach, validated in nine complex phenotypes in a multi-ancestry population. We use an ensemble method of SNP selection followed by gradient boosted trees (XGBoost) to allow for non-linearities and interaction effects. We compare our results to the standard, linear PRS model developed using PRSice, LDpred2, and lassosum2. Combining a PRS as a feature in an XGBoost model results in a relative increase in the percentage variance explained compared to the standard linear PRS model by 22% for height, 27% for HDL cholesterol, 43% for body mass index, 50% for sleep duration, 58% for systolic blood pressure, 64% for total cholesterol, 66% for triglycerides, 77% for LDL cholesterol, and 100% for diastolic blood pressure. Multi-ancestry trained models perform similarly to specific racial/ethnic group trained models and are consistently superior to the standard linear PRS models. This work demonstrates an effective method to account for non-linearities and interaction effects in genetics-based prediction models.",success
35297530,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Population stratification analyses targeting genetically closely related East Asians have revealed that distinguishable differentiation exists between Han Chinese, Korean, and Japanese individuals, as well as between southern (S-) and northern (N-) Han Chinese. Previous studies offer a number of choices for ancestry informative single nucleotide polymorphisms (AISNPs) to discriminate East-Asian populations. In this study, we collected and examined the efficiency of 1185 AISNPs using frequency and genotype data from various publicly available databases. With the aim to perform fine-scale classification of S-Han, N-Han, Korean, and Japanese subjects, machine-learning methods (Softmax and Random Forest) were used to screen a panel of highly informative AISNPs and to develop a superior classification model. Stepwise classification was implemented to increase and balance the discrimination in the process of AISNP selection, first discriminating Han, Korean, and Japanese individuals, and then characterizing stratification between S-Han and N-Han. The final 272-AISNP panel is an alternative optimization of various previous works, which promises reliable and >90% accuracy in classification of the four East-Asian groups. This AISNP panel and the machine-learning model could be a useful and superior choice in medical genome-wide association studies and in forensic investigations for unknown suspect identity.",success
30617323,False,Journal Article,,,,,,,,True,"Syndromic genetic conditions, in aggregate, affect 8% of the population<sup>1</sup>. Many syndromes have recognizable facial features<sup>2</sup> that are highly informative to clinical geneticists<sup>3-5</sup>. Recent studies show that facial analysis technologies measured up to the capabilities of expert clinicians in syndrome identification<sup>6-9</sup>. However, these technologies identified only a few disease phenotypes, limiting their role in clinical settings, where hundreds of diagnoses must be considered. Here we present a facial image analysis framework, DeepGestalt, using computer vision and deep-learning algorithms, that quantifies similarities to hundreds of syndromes. DeepGestalt outperformed clinicians in three initial experiments, two with the goal of distinguishing subjects with a target syndrome from other syndromes, and one of separating different genetic subtypes in Noonan syndrome. On the final experiment reflecting a real clinical setting problem, DeepGestalt achieved 91% top-10 accuracy in identifying the correct syndrome on 502 different images. The model was trained on a dataset of over 17,000 images representing more than 200 syndromes, curated through a community-driven phenotyping platform. DeepGestalt potentially adds considerable value to phenotypic evaluations in clinical genetics, genetic testing, research and precision medicine.",success
33566059,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Long QT syndrome (LQTS) is characterized by prolongation of the QT interval and is associated with an increased risk of sudden cardiac death. However, although QT interval prolongation is the hallmark feature of LQTS, approximately 40% of patients with genetically confirmed LQTS have a normal corrected QT (QTc) at rest. Distinguishing patients with LQTS from those with a normal QTc is important to correctly diagnose disease, implement simple LQTS preventive measures, and initiate prophylactic therapy if necessary. To determine whether artificial intelligence (AI) using deep neural networks is better than the QTc alone in distinguishing patients with concealed LQTS from those with a normal QTc using a 12-lead electrocardiogram (ECG). A diagnostic case-control study was performed using all available 12-lead ECGs from 2059 patients presenting to a specialized genetic heart rhythm clinic. Patients were included if they had a definitive clinical and/or genetic diagnosis of type 1, 2, or 3 LQTS (LQT1, 2, or 3) or were seen because of an initial suspicion for LQTS but were discharged without this diagnosis. A multilayer convolutional neural network was used to classify patients based on a 10-second, 12-lead ECG, AI-enhanced ECG (AI-ECG). The convolutional neural network was trained using 60% of the patients, validated in 10% of the patients, and tested on the remaining patients (30%). The study was conducted from January 1, 1999, to December 31, 2018. The goal of the study was to test the ability of the convolutional neural network to distinguish patients with LQTS from those who were evaluated for LQTS but discharged without this diagnosis, especially among patients with genetically confirmed LQTS but a normal QTc value at rest (referred to as genotype positive/phenotype negative LQTS, normal QT interval LQTS, or concealed LQTS). Of the 2059 patients included, 1180 were men (57%); mean (SD) age at first ECG was 21.6 (15.6) years. All 12-lead ECGs from 967 patients with LQTS and 1092 who were evaluated for LQTS but discharged without this diagnosis were included for AI-ECG analysis. Based on the ECG-derived QTc alone, patients were classified with an area under the curve (AUC) value of 0.824 (95% CI, 0.79-0.858); using AI-ECG, the AUC was 0.900 (95% CI, 0.876-0.925). Furthermore, in the subset of patients who had a normal resting QTc (<450 milliseconds), the QTc alone distinguished those with LQTS from those without LQTS with an AUC of 0.741 (95% CI, 0.689-0.794), whereas the AI-ECG increased this discrimination to an AUC of 0.863 (95% CI, 0.824-0.903). In addition, the AI-ECG was able to distinguish the 3 main genotypic subgroups (LQT1, LQT2, and LQT3) with an AUC of 0.921 (95% CI, 0.890-0.951) for LQT1 compared with LQT2 and 3, 0.944 (95% CI, 0.918-0.970) for LQT2 compared with LQT1 and 3, and 0.863 (95% CI, 0.792-0.934) for LQT3 compared with LQT1 and 2. In this study, the AI-ECG was found to distinguish patients with electrocardiographically concealed LQTS from those discharged without a diagnosis of LQTS and provide a nearly 80% accurate pregenetic test anticipation of LQTS genotype status. This model may aid in the detection of LQTS in patients presenting to an arrhythmia clinic and, with validation, may be the stepping stone to similar tools to be developed for use in the general population.",success
24487276,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Current methods for annotating and interpreting human genetic variation tend to exploit a single information type (for example, conservation) and/or are restricted in scope (for example, to missense changes). Here we describe Combined Annotation-Dependent Depletion (CADD), a method for objectively integrating many diverse annotations into a single measure (C score) for each variant. We implement CADD as a support vector machine trained to differentiate 14.7 million high-frequency human-derived alleles from 14.7 million simulated variants. We precompute C scores for all 8.6 billion possible human single-nucleotide variants and enable scoring of short insertions-deletions. C scores correlate with allelic diversity, annotations of functionality, pathogenicity, disease severity, experimentally measured regulatory effects and complex trait associations, and they highly rank known pathogenic variants within individual genomes. The ability of CADD to prioritize functional, deleterious and pathogenic variants across many functional categories, effect sizes and genetic architectures is unmatched by any current single-annotation method.",success
30038395,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"Millions of human genomes and exomes have been sequenced, but their clinical applications remain limited due to the difficulty of distinguishing disease-causing mutations from benign genetic variation. Here we demonstrate that common missense variants in other primate species are largely clinically benign in human, enabling pathogenic mutations to be systematically identified by the process of elimination. Using hundreds of thousands of common variants from population sequencing of six non-human primate species, we train a deep neural network that identifies pathogenic mutations in rare disease patients with 88% accuracy and enables the discovery of 14 new candidate genes in intellectual disability at genome-wide significance. Cataloging common variation from additional primate species would improve interpretation for millions of variants of uncertain significance, further advancing the clinical utility of human genome sequencing.",success
34645491,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Clinical interpretation of genetic variants in the context of the patient's phenotype is becoming the largest component of cost and time expenditure for genome-based diagnosis of rare genetic diseases. Artificial intelligence (AI) holds promise to greatly simplify and speed genome interpretation by integrating predictive methods with the growing knowledge of genetic disease. Here we assess the diagnostic performance of Fabric GEM, a new, AI-based, clinical decision support tool for expediting genome interpretation. We benchmarked GEM in a retrospective cohort of 119 probands, mostly NICU infants, diagnosed with rare genetic diseases, who received whole-genome or whole-exome sequencing (WGS, WES). We replicated our analyses in a separate cohort of 60 cases collected from five academic medical centers. For comparison, we also analyzed these cases with current state-of-the-art variant prioritization tools. Included in the comparisons were trio, duo, and singleton cases. Variants underpinning diagnoses spanned diverse modes of inheritance and types, including structural variants (SVs). Patient phenotypes were extracted from clinical notes by two means: manually and using an automated clinical natural language processing (CNLP) tool. Finally, 14 previously unsolved cases were reanalyzed. GEM ranked over 90% of the causal genes among the top or second candidate and prioritized for review a median of 3 candidate genes per case, using either manually curated or CNLP-derived phenotype descriptions. Ranking of trios and duos was unchanged when analyzed as singletons. In 17 of 20 cases with diagnostic SVs, GEM identified the causal SVs as the top candidate and in 19/20 within the top five, irrespective of whether SV calls were provided or inferred ab initio by GEM using its own internal SV detection algorithm. GEM showed similar performance in absence of parental genotypes. Analysis of 14 previously unsolved cases resulted in a novel finding for one case, candidates ultimately not advanced upon manual review for 3 cases, and no new findings for 10 cases. GEM enabled diagnostic interpretation inclusive of all variant types through automated nomination of a very short list of candidate genes and disorders for final review and reporting. In combination with deep phenotyping by CNLP, GEM enables substantial automation of genetic disease diagnosis, potentially decreasing cost and expediting case review.",success
25466337,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Improved mortality prediction for patients in intensive care units is a big challenge. Many severity scores have been proposed, but findings of validation studies have shown that they are not adequately calibrated. The Super ICU Learner Algorithm (SICULA), an ensemble machine learning technique that uses multiple learning algorithms to obtain better prediction performance, does at least as well as the best member of its library. We aimed to assess whether the Super Learner could provide a new mortality prediction algorithm for patients in intensive care units, and to assess its performance compared with other scoring systems. From January, 2001, to December, 2008, we used the Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II) database (version 26) including all patients admitted to an intensive care unit at the Beth Israel Deaconess Medical Centre, Boston, MA, USA. We assessed the calibration, discrimination, and risk classification of predicted hospital mortality based on Super Learner compared with SAPS-II, APACHE-II, and SOFA. We calculated performance measures with cross-validation to avoid making biased assessments. Our proposed score was then externally validated on a dataset of 200 randomly selected patients admitted at the intensive care unit of Hôpital Européen Georges-Pompidou, Paris, France, between Sept 1, 2013, and June, 30, 2014. The primary outcome was hospital mortality. The explanatory variables were the same as those included in the SAPS II score. 24,508 patients were included, with median SAPS-II of 38 (IQR 27-51) and median SOFA of 5 (IQR 2-8). 3002 of 24,508 (12%) patients died in the Beth Israel Deaconess Medical Centre. We produced two sets of predictions based on the Super Learner; the first based on the 17 variables as they appear in the SAPS-II score (SL1), and the second, on the original, untransformed variables (SL2). The two versions yielded average predicted probabilities of death of 0·12 (IQR 0·02-0·16) and 0·13 (0·01-0·19), whereas the corresponding value for SOFA was 0·12 (0·05-0·15) and for SAPS-II 0·30 (0·08-0·48). The cross-validated area under the receiver operating characteristic curve (AUROC) for SAPS-II was 0·78 (95% CI 0·77-0·78) and 0·71 (0·70-0·72) for SOFA. Super Learner had an AUROC of 0·85 (0·84-0·85) when the explanatory variables were categorised as in SAPS-II, and of 0·88 (0·87-0·89) when the same explanatory variables were included without any transformation. Additionally, Super Learner showed better calibration properties than previous score systems. On the external validation dataset, the AUROC was 0·94 (0·90-0·98) and calibration properties were good. Compared with conventional severity scores, Super Learner offers improved performance for predicting hospital mortality in patients in intensive care units. A user-friendly implementation is available online and should be useful for clinicians seeking to validate our score. Fulbright Foundation, Assistance Publique-Hôpitaux de Paris, Doris Duke Clinical Scientist Development Award, and the NIH.",success
31428430,False,Journal Article,,,,,,,,True,"We investigated if early intensive care unit (ICU) scoring with the Simplified Acute Physiology Score (SAPS 3) could be improved using artificial neural networks (ANNs). All first-time adult intensive care admissions in Sweden during 2009-2017 were included. A test set was set aside for validation. We trained ANNs with two hidden layers with random hyper-parameters and retained the best ANN, determined using cross-validation. The ANNs were constructed using the same parameters as in the SAPS 3 model. The performance was assessed with the area under the receiver operating characteristic curve (AUC) and Brier score. A total of 217,289 admissions were included. The developed ANN (AUC 0.89 and Brier score 0.096) was found to be superior (<i>p</i> <10<sup>-15</sup> for AUC and <i>p</i> <10<sup>-5</sup> for Brier score) in early prediction of 30-day mortality for intensive care patients when compared with SAPS 3 (AUC 0.85 and Brier score 0.109). In addition, a simple, eight-parameter ANN model was found to perform just as well as SAPS 3, but with better calibration (AUC 0.85 and and Brier score 0.106, <i>p</i> <10<sup>-5</sup>). Furthermore, the ANN model was superior in correcting mortality for age. ANNs can outperform the SAPS 3 model for early prediction of 30-day mortality for intensive care patients.",success
34620086,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Currently, the risk stratification of critically ill patient with chest pain is a challenge. We aimed to use machine learning approach to predict the critical care outcomes in patients with chest pain, and simultaneously compare its performance with HEART, GRACE, and TIMI scores. This was a retrospective, case-control study in patients with acute non-traumatic chest pain who presented to the emergency department (ED) between January 2017 and December 2019. The outcomes included cardiac arrest, transfer to ICU, and death during treatment in ED. In the randomly sampled training set (70%), a LASSO regression model was developed, and presented with nomogram. The performance was measured in both training set (70% participants) and testing set (30% participants), and findings were compared with the three widely used scores. We proposed a LASSO regression model incorporating mode of arrival, reperfusion therapy, Killip class, systolic BP, serum creatinine, creatine kinase-MB, and brain natriuretic peptide as independent predictors of critical care outcomes in patients with chest pain. Our model significantly outperformed the HEART, GRACE, TIMI score with AUC of 0.953 (95%CI: 0.922-0.984), 0.754 (95%CI: 0.675-0.832), 0.747 (95%CI: 0.664-0.829), 0.735 (95%CI: 0.655-0.815), respectively. Consistently, our model demonstrated better outcomes regarding the metrics of accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and F1 score. Similarly, the decision curve analysis elucidated a greater net benefit of our model over the full ranges of clinical thresholds. We present an accurate model for predicting the critical care outcomes in patients with chest pain, and provide substantial support to its application as a decision-making tool in ED.",success
29382633,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"As a high-prevalence health condition, hypertension is clinically costly, difficult to manage, and often leads to severe and life-threatening diseases such as cardiovascular disease (CVD) and stroke. The aim of this study was to develop and validate prospectively a risk prediction model of incident essential hypertension within the following year. Data from individual patient electronic health records (EHRs) were extracted from the Maine Health Information Exchange network. Retrospective (N=823,627, calendar year 2013) and prospective (N=680,810, calendar year 2014) cohorts were formed. A machine learning algorithm, XGBoost, was adopted in the process of feature selection and model building. It generated an ensemble of classification trees and assigned a final predictive risk score to each individual. The 1-year incident hypertension risk model attained areas under the curve (AUCs) of 0.917 and 0.870 in the retrospective and prospective cohorts, respectively. Risk scores were calculated and stratified into five risk categories, with 4526 out of 381,544 patients (1.19%) in the lowest risk category (score 0-0.05) and 21,050 out of 41,329 patients (50.93%) in the highest risk category (score 0.4-1) receiving a diagnosis of incident hypertension in the following 1 year. Type 2 diabetes, lipid disorders, CVDs, mental illness, clinical utilization indicators, and socioeconomic determinants were recognized as driving or associated features of incident essential hypertension. The very high risk population mainly comprised elderly (age>50 years) individuals with multiple chronic conditions, especially those receiving medications for mental disorders. Disparities were also found in social determinants, including some community-level factors associated with higher risk and others that were protective against hypertension. With statewide EHR datasets, our study prospectively validated an accurate 1-year risk prediction model for incident essential hypertension. Our real-time predictive analytic model has been deployed in the state of Maine, providing implications in interventions for hypertension and related diseases and hopefully enhancing hypertension care.",success
33297865,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Oral anticoagulation is generally indicated for cardioembolic strokes, but not for other stroke causes. Consequently, subtype classification of ischemic stroke is important for risk stratification and secondary prevention. Because manual classification of ischemic stroke is time-intensive, we assessed the accuracy of automated algorithms for performing cardioembolic stroke subtyping using an electronic health record (EHR) database. We adapted TOAST (Trial of ORG 10172 in Acute Stroke Treatment) features associated with cardioembolic stroke for derivation in the EHR. Using administrative codes and echocardiographic reports within Mass General Brigham Biobank (N=13 079), we iteratively developed EHR-based algorithms to define the TOAST cardioembolic stroke features, revising regular expression algorithms until achieving positive predictive value ≥80%. We compared several machine learning-based statistical algorithms for discriminating cardioembolic stroke using the feature algorithms applied to EHR data from 1598 patients with acute ischemic strokes from the Massachusetts General Hospital Ischemic Stroke Registry (2002-2010) with previously adjudicated TOAST and Causative Classification of Stroke subtypes. Regular expression-based feature extraction algorithms achieved a mean positive predictive value of 95% (range, 88%-100%) across 11 echocardiographic features. Among 1598 patients from the Massachusetts General Hospital Ischemic Stroke Registry, 1068 had any cardioembolic stroke feature within predefined time windows in proximity to the stroke event. Cardioembolic stroke tended to occur at an older age, with more TOAST-based comorbidities, and with atrial fibrillation (82.3%). The best model was a random forest with 92.2% accuracy and area under the receiver operating characteristic curve of 91.1% (95% CI, 87.5%-93.9%). Atrial fibrillation, age, dilated cardiomyopathy, congestive heart failure, patent foramen ovale, mitral annulus calcification, and recent myocardial infarction were the most discriminatory features. Machine learning-based identification of cardioembolic stroke using EHR data is feasible. Future work is needed to improve the accuracy of automated cardioembolic stroke identification and assess generalizability of electronic phenotyping algorithms across clinical settings.",success
36773349,False,Systematic Review;Meta-Analysis;Journal Article,,,,,,,,True,"Ventricular arrhythmia (VA) precipitating sudden cardiac arrest (SCD) is among the most frequent causes of death and pose a high burden on public health systems worldwide. The increasing availability of electrophysiological signals collected through conventional methods (e.g. electrocardiography (ECG)) and digital health technologies (e.g. wearable devices) in combination with novel predictive analytics using machine learning (ML) and deep learning (DL) hold potential for personalised predictions of arrhythmic events. This systematic review and exploratory meta-analysis assesses the state-of-the-art of ML/DL models of electrophysiological signals for personalised prediction of malignant VA or SCD, and studies potential causes of bias (PROSPERO, reference: CRD42021283464). Five electronic databases were searched to identify eligible studies. Pooled estimates of the diagnostic odds ratio (DOR) and summary area under the curve (AUROC) were calculated. Meta-analyses were performed separately for studies using publicly available, ad-hoc datasets, versus targeted clinical data acquisition. Studies were scored on risk of bias by the PROBAST tool. 2194 studies were identified of which 46 were included in the systematic review and 32 in the meta-analysis. Pooling of individual models demonstrated a summary AUROC of 0.856 (95% CI 0.755-0.909) for short-term (time-to-event up to 72 h) prediction and AUROC of 0.876 (95% CI 0.642-0.980) for long-term prediction (time-to-event up to years). While models developed on ad-hoc sets had higher pooled performance (AUROC 0.919, 95% CI 0.867-0.952), they had a high risk of bias related to the re-use and overlap of small ad-hoc datasets, choices of ML tool and a lack of external model validation. ML and DL models appear to accurately predict malignant VA and SCD. However, wide heterogeneity between studies, in part due to small ad-hoc datasets and choice of ML model, may reduce the ability to generalise and should be addressed in future studies. This publication is part of the project DEEP RISK ICD (with project number 452019308) of the research programme Rubicon which is (partly) financed by the Dutch Research Council (NWO). This research is partly funded by the Amsterdam Cardiovascular Sciences (personal grant F.V.Y.T).",success
35089057,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Current ablation therapy for atrial fibrillation is suboptimal, and long-term response is challenging to predict. Clinical trials identify bedside properties that provide only modest prediction of long-term response in populations, while patient-specific models in small cohorts primarily explain acute response to ablation. We aimed to predict long-term atrial fibrillation recurrence after ablation in large cohorts, by using machine learning to complement biophysical simulations by encoding more interindividual variability. Patient-specific models were constructed for 100 atrial fibrillation patients (43 paroxysmal, 41 persistent, and 16 long-standing persistent), undergoing first ablation. Patients were followed for 1 year using ambulatory ECG monitoring. Each patient-specific biophysical model combined differing fibrosis patterns, fiber orientation maps, electrical properties, and ablation patterns to capture uncertainty in atrial properties and to test the ability of the tissue to sustain fibrillation. These simulation stress tests of different model variants were postprocessed to calculate atrial fibrillation simulation metrics. Machine learning classifiers were trained to predict atrial fibrillation recurrence using features from the patient history, imaging, and atrial fibrillation simulation metrics. We performed 1100 atrial fibrillation ablation simulations across 100 patient-specific models. Models based on simulation stress tests alone showed a maximum accuracy of 0.63 for predicting long-term fibrillation recurrence. Classifiers trained to history, imaging, and simulation stress tests (average 10-fold cross-validation area under the curve, 0.85±0.09; recall, 0.80±0.13; precision, 0.74±0.13) outperformed those trained to history and imaging (area under the curve, 0.66±0.17) or history alone (area under the curve, 0.61±0.14). A novel computational pipeline accurately predicted long-term atrial fibrillation recurrence in individual patients by combining outcome data with patient-specific acute simulation response. This technique could help to personalize selection for atrial fibrillation ablation.",success
35867397,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Machine learning is a promising approach to personalize atrial fibrillation management strategies for patients after catheter ablation. Prior atrial fibrillation ablation outcome prediction studies applied classical machine learning methods to hand-crafted clinical scores, and none have leveraged intracardiac electrograms or 12-lead surface electrocardiograms for outcome prediction. We hypothesized that (1) machine learning models trained on electrograms or electrocardiogram (ECG) signals can perform better at predicting patient outcomes after atrial fibrillation ablation than existing clinical scores and (2) multimodal fusion of electrogram, ECG, and clinical features can further improve the prediction of patient outcomes. Consecutive patients who underwent catheter ablation between 2015 and 2017 with panoramic left atrial electrogram before ablation and clinical follow-up for at least 1 year following ablation were included. Convolutional neural network and a novel multimodal fusion framework were developed for predicting 1-year atrial fibrillation recurrence after catheter ablation from electrogram, ECG signals, and clinical features. The models were trained and validated using 10-fold cross-validation on patient-level splits. One hundred fifty-six patients (64.5±10.5 years, 74% male, 42% paroxysmal) were analyzed. Using electrogram signals alone, the convolutional neural network achieved an area under the receiver operating characteristics curve (AUROC) of 0.731, outperforming the existing APPLE scores (AUROC=0.644) and CHA2DS2-VASc scores (AUROC=0.650). Similarly using 12-lead ECG alone, the convolutional neural network achieved an AUROC of 0.767. Combining electrogram, ECG, and clinical features, the fusion model achieved an AUROC of 0.859, outperforming single and dual modality models. Deep neural networks trained on electrogram or ECG signals improved the prediction of catheter ablation outcome compared with existing clinical scores, and fusion of electrogram, ECG, and clinical features further improved the prediction. This suggests the promise of using machine learning to help treatment planning for patients after catheter ablation.",success
31223556,False,Journal Article,,,,,,,,True,"Use of the electronic health record (EHR) for CVD surveillance is increasingly common. However, these data can introduce systematic error that influences the internal and external validity of study findings. We reviewed recent literature on EHR-based studies of CVD risk to summarize the most common types of bias that arise. Subsequently, we recommend strategies informed by work from others as well as our own to reduce the impact of these biases in future research. Systematic error, or bias, is a concern in all observational research including EHR-based studies of CVD risk surveillance. Patients captured in an EHR system may not be representative of the general population, due to issues such as informed presence bias, perceptions about the healthcare system that influence entry, and access to health services. Further, the EHR may contain inaccurate information or be missing key data points of interest due to loss to follow-up or over-diagnosis bias. Several strategies, including implementation of unique patient identifiers, adoption of standardized rules for inclusion/exclusion criteria, statistical procedures for data harmonization and analysis, and incorporation of patient-reported data have been used to reduce the impact of these biases. EHR data provide an opportunity to monitor and characterize CVD risk in populations. However, understanding the biases that arise from EHR datasets is instrumental in planning epidemiological studies and interpreting study findings. Strategies to reduce the impact of bias in the context of EHR data can increase the quality and utility of these data.",success
34670780,False,"Journal Article;Research Support, Non-U.S. Gov't;Systematic Review",,,,,,,,True,"To assess the methodological quality of studies on prediction models developed using machine learning techniques across all medical specialties. Systematic review. PubMed from 1 January 2018 to 31 December 2019. Articles reporting on the development, with or without external validation, of a multivariable prediction model (diagnostic or prognostic) developed using supervised machine learning for individualised predictions. No restrictions applied for study design, data source, or predicted patient related health outcomes. Methodological quality of the studies was determined and risk of bias evaluated using the prediction risk of bias assessment tool (PROBAST). This tool contains 21 signalling questions tailored to identify potential biases in four domains. Risk of bias was measured for each domain (participants, predictors, outcome, and analysis) and each study (overall). 152 studies were included: 58 (38%) included a diagnostic prediction model and 94 (62%) a prognostic prediction model. PROBAST was applied to 152 developed models and 19 external validations. Of these 171 analyses, 148 (87%, 95% confidence interval 81% to 91%) were rated at high risk of bias. The analysis domain was most frequently rated at high risk of bias. Of the 152 models, 85 (56%, 48% to 64%) were developed with an inadequate number of events per candidate predictor, 62 handled missing data inadequately (41%, 33% to 49%), and 59 assessed overfitting improperly (39%, 31% to 47%). Most models used appropriate data sources to develop (73%, 66% to 79%) and externally validate the machine learning based prediction models (74%, 51% to 88%). Information about blinding of outcome and blinding of predictors was, however, absent in 60 (40%, 32% to 47%) and 79 (52%, 44% to 60%) of the developed models, respectively. Most studies on machine learning based prediction models show poor methodological quality and are at high risk of bias. Factors contributing to risk of bias include small study size, poor handling of missing data, and failure to deal with overfitting. Efforts to improve the design, conduct, reporting, and validation of such studies are necessary to boost the application of machine learning based prediction models in clinical practice. PROSPERO CRD42019161764.",success
32864600,False,Journal Article;Review,,,,,,,,True,An emphasis on overly broad notions of generalisability as it pertains to applications of machine learning in health care can overlook situations in which machine learning might provide clinical utility. We believe that this narrow focus on generalisability should be replaced with wider considerations for the ultimate goal of building machine learning systems that are useful at the bedside.,success
32658648,False,Journal Article;Review,,,,,,,,True,"Causal directed acyclic graphs (cDAGs) have become popular tools for researchers to better examine biases related to causal questions. DAGs comprise a series of arrows connecting nodes that represent variables and in doing so can demonstrate the causal relation between different variables. cDAGs can provide researchers with a blueprint of the exposure and outcome relation and the other variables that play a role in that causal question. cDAGs can be helpful in the design and interpretation of observational studies in pulmonary, critical care, sleep, and cardiovascular medicine. They can also help clinicians and researchers to better identify the structure of different biases that can affect the validity of observational studies. Most of the available literature on cDAGs and their function use language that might be unfamiliar to clinicians. This article explains cDAG terminology and the principles behind how they work. We use cDAGs and clinical examples that are mostly focused in the area of pulmonary medicine to describe the structure of confounding, selection bias, overadjustment bias, and detection bias. These principles are then applied to a more complex published case study on the use of statins and COPD mortality. We also introduce readers to other resources for a more in-depth discussion of causal inference principles.",success
36981544,False,Journal Article;Review,,,,,,,,True,"ChatGPT is an artificial intelligence (AI)-based conversational large language model (LLM). The potential applications of LLMs in health care education, research, and practice could be promising if the associated valid concerns are proactively examined and addressed. The current systematic review aimed to investigate the utility of ChatGPT in health care education, research, and practice and to highlight its potential limitations. Using the PRIMSA guidelines, a systematic search was conducted to retrieve English records in PubMed/MEDLINE and Google Scholar (published research or preprints) that examined ChatGPT in the context of health care education, research, or practice. A total of 60 records were eligible for inclusion. Benefits of ChatGPT were cited in 51/60 (85.0%) records and included: (1) improved scientific writing and enhancing research equity and versatility; (2) utility in health care research (efficient analysis of datasets, code generation, literature reviews, saving time to focus on experimental design, and drug discovery and development); (3) benefits in health care practice (streamlining the workflow, cost saving, documentation, personalized medicine, and improved health literacy); and (4) benefits in health care education including improved personalized learning and the focus on critical thinking and problem-based learning. Concerns regarding ChatGPT use were stated in 58/60 (96.7%) records including ethical, copyright, transparency, and legal issues, the risk of bias, plagiarism, lack of originality, inaccurate content with risk of hallucination, limited knowledge, incorrect citations, cybersecurity issues, and risk of infodemics. The promising applications of ChatGPT can induce paradigm shifts in health care education, research, and practice. However, the embrace of this AI chatbot should be conducted with extreme caution considering its potential limitations. As it currently stands, ChatGPT does not qualify to be listed as an author in scientific articles unless the ICMJE/COPE guidelines are revised or amended. An initiative involving all stakeholders in health care education, research, and practice is urgently needed. This will help to set a code of ethics to guide the responsible use of ChatGPT among other LLMs in health care and academia.",success
34782696,False,Journal Article,,,,,,,,True,"Problem framing is critical to developing risk prediction models because all subsequent development work and evaluation takes place within the context of how a problem has been framed and explicit documentation of framing choices makes it easier to compare evaluation metrics between published studies. In this work, we introduce the basic concepts of framing, including prediction windows, observation windows, window shifts and event-triggers for a prediction that strongly affects the risk of clinician fatigue caused by false positives. Building on this, we apply four different framing structures to the same generic dataset, using a sepsis risk prediction model as an example, and evaluate how framing affects model performance and learning. Our results show that an apparently good model with strong evaluation results in both discrimination and calibration is not necessarily clinically usable. Therefore, it is important to assess the results of objective evaluations within the context of more subjective evaluations of how a model is framed.",success
27561321,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Ventricular tachycardia (VT) is a potentially fatal tachyarrhythmia, which causes a rapid heartbeat as a result of improper electrical activity of the heart. This is a potentially life-threatening arrhythmia because it can cause low blood pressure and may lead to ventricular fibrillation, asystole, and sudden cardiac death. To prevent VT, we developed an early prediction model that can predict this event one hour before its onset using an artificial neural network (ANN) generated using 14 parameters obtained from heart rate variability (HRV) and respiratory rate variability (RRV) analysis. De-identified raw data from the monitors of patients admitted to the cardiovascular intensive care unit at Asan Medical Center between September 2013 and April 2015 were collected. The dataset consisted of 52 recordings obtained one hour prior to VT events and 52 control recordings. Two-thirds of the extracted parameters were used to train the ANN, and the remaining third was used to evaluate performance of the learned ANN. The developed VT prediction model proved its performance by achieving a sensitivity of 0.88, specificity of 0.82, and AUC of 0.93.",success
25834725,False,Journal Article,,,,,,,,True,"As research laboratories and clinics collaborate to achieve precision medicine, both communities are required to understand mandated electronic health/medical record (EHR/EMR) initiatives that will be fully implemented in all clinics in the United States by 2015. Stakeholders will need to evaluate current record keeping practices and optimize and standardize methodologies to capture nearly all information in digital format. Collaborative efforts from academic and industry sectors are crucial to achieving higher efficacy in patient care while minimizing costs. Currently existing digitized data and information are present in multiple formats and are largely unstructured. In the absence of a universally accepted management system, departments and institutions continue to generate silos of information. As a result, invaluable and newly discovered knowledge is difficult to access. To accelerate biomedical research and reduce healthcare costs, clinical and bioinformatics systems must employ common data elements to create structured annotation forms enabling laboratories and clinics to capture sharable data in real time. Conversion of these datasets to knowable information should be a routine institutionalized process. New scientific knowledge and clinical discoveries can be shared via integrated knowledge environments defined by flexible data models and extensive use of standards, ontologies, vocabularies, and thesauri. In the clinical setting, aggregated knowledge must be displayed in user-friendly formats so that physicians, non-technical laboratory personnel, nurses, data/research coordinators, and end-users can enter data, access information, and understand the output. The effort to connect astronomical numbers of data points, including '-omics'-based molecular data, individual genome sequences, experimental data, patient clinical phenotypes, and follow-up data is a monumental task. Roadblocks to this vision of integration and interoperability include ethical, legal, and logistical concerns. Ensuring data security and protection of patient rights while simultaneously facilitating standardization is paramount to maintaining public support. The capabilities of supercomputing need to be applied strategically. A standardized, methodological implementation must be applied to developed artificial intelligence systems with the ability to integrate data and information into clinically relevant knowledge. Ultimately, the integration of bioinformatics and clinical data in a clinical decision support system promises precision medicine and cost effective and personalized patient care.",success
36713089,False,Journal Article,,,,,,,,True,"In 1955, when John McCarthy and his colleagues proposed their first study of artificial intelligence, they suggested that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it'. Whether that might ever be possible would depend on how we define intelligence, but what is indisputable is that new methods are needed to analyse and interpret the copious information provided by digital medical images, genomic databases, and biobanks. Technological advances have enabled applications of artificial intelligence (AI) including machine learning (ML) to be implemented into clinical practice, and their related scientific literature is exploding. Advocates argue enthusiastically that AI will transform many aspects of clinical cardiovascular medicine, while sceptics stress the importance of caution and the need for more evidence. This report summarizes the main opposing arguments that were presented in a debate at the 2021 Congress of the European Society of Cardiology. Artificial intelligence is an advanced analytical technique that should be considered when conventional statistical methods are insufficient, but testing a hypothesis or solving a clinical problem-not finding another application for AI-remains the most important objective. Artificial intelligence and ML methods should be transparent and interpretable, if they are to be approved by regulators and trusted to provide support for clinical decisions. Physicians need to understand AI methods and collaborate with engineers. Few applications have yet been shown to have a positive impact on clinical outcomes, so investment in research is essential.",success
32577533,False,Journal Article,,,,,,,,True,"With emerging innovations in artificial intelligence (AI) poised to substantially impact medical practice, interest in training current and future physicians about the technology is growing. Alongside comes the question of what, precisely, should medical students be taught. While competencies for the clinical usage of AI are broadly similar to those for any other novel technology, there are qualitative differences of critical importance to concerns regarding explainability, health equity, and data security. Drawing on experiences at the University of Toronto Faculty of Medicine and MIT Critical Data's ""datathons"", the authors advocate for a dual-focused approach: combining robust data science-focused additions to baseline health research curricula and extracurricular programs to cultivate leadership in this space.",success
35639667,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"The medical field has seen a rapid increase in the development of artificial intelligence (AI)-based prediction models. With the introduction of such AI-based prediction model tools and software in cardiovascular patient care, the cardiovascular researcher and healthcare professional are challenged to understand the opportunities as well as the limitations of the AI-based predictions. In this article, we present 12 critical questions for cardiovascular health professionals to ask when confronted with an AI-based prediction model. We aim to support medical professionals to distinguish the AI-based prediction models that can add value to patient care from the AI that does not.",success
32284873,False,Journal Article,,,,,,,,True,"Machine learning has been used successfully to improve the accuracy of computer-aided diagnosis systems. This paper experimentally assesses the performance of models derived by machine learning techniques by using relevant features chosen by various feature-selection methods. Four commonly used heart disease datasets have been evaluated using principal component analysis, Chi squared testing, ReliefF and symmetrical uncertainty to create distinctive feature sets. Then, a variety of classification algorithms have been used to create models that are then compared to seek the optimal features combinations, to improve the correct prediction of heart conditions. We found the benefits of using feature selection vary depending on the machine learning technique used for the heart datasets we consider. However, the best model we created used a combination of Chi-squared feature selection with the BayesNet algorithm and achieved an accuracy of 85.00% on the considered datasets.",success
34259870,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Optimal risk stratification with machine learning (ML) from myocardial perfusion imaging (MPI) includes both clinical and imaging data. While most imaging variables can be derived automatically, clinical variables require manual collection, which is time-consuming and prone to error. We determined the fewest manually input and imaging variables required to maintain the prognostic accuracy for major adverse cardiac events (MACE) in patients undergoing a single-photon emission computed tomography (SPECT) MPI. This study included 20 414 patients from the multicentre REFINE SPECT registry and 2984 from the University of Calgary for training and external testing of the ML models, respectively. ML models were trained using all variables (ML-All) and all image-derived variables (including age and sex, ML-Image). Next, ML models were sequentially trained by incrementally adding manually input and imaging variables to baseline ML models based on their importance ranking. The fewest variables were determined as the ML models (ML-Reduced, ML-Minimum, and ML-Image-Reduced) that achieved comparable prognostic performance to ML-All and ML-Image. Prognostic accuracy of the ML models was compared with visual diagnosis, stress total perfusion deficit (TPD), and traditional multivariable models using area under the receiver-operating characteristic curve (AUC). ML-Minimum (AUC 0.798) obtained comparable prognostic accuracy to ML-All (AUC 0.799, P = 0.19) by including 12 of 40 manually input variables and 11 of 58 imaging variables. ML-Reduced achieved comparable accuracy (AUC 0.796) with a reduced set of manually input variables and all imaging variables. In external validation, the ML models also obtained comparable or higher prognostic accuracy than traditional multivariable models. Reduced ML models, including a minimum set of manually collected or imaging variables, achieved slightly lower accuracy compared to a full ML model but outperformed standard interpretation methods and risk models. ML models with fewer collected variables may be more practical for clinical implementation.",success
36050271,False,Journal Article;Review,,,,,,,,True,"Big data is important to new developments in global clinical science that aim to improve the lives of patients. Technological advances have led to the regular use of structured electronic health-care records with the potential to address key deficits in clinical evidence that could improve patient care. The COVID-19 pandemic has shown this potential in big data and related analytics but has also revealed important limitations. Data verification, data validation, data privacy, and a mandate from the public to conduct research are important challenges to effective use of routine health-care data. The European Society of Cardiology and the BigData@Heart consortium have brought together a range of international stakeholders, including representation from patients, clinicians, scientists, regulators, journal editors, and industry members. In this Review, we propose the CODE-EHR minimum standards framework to be used by researchers and clinicians to improve the design of studies and enhance transparency of study methods. The CODE-EHR framework aims to develop robust and effective utilisation of health-care data for research purposes.",success
35396247,False,Journal Article,,,,,,,,True,"The American College of Cardiology and the American Heart Association guidelines on primary prevention of atherosclerotic cardiovascular disease (ASCVD) recommend using 10-year ASCVD risk estimation models to initiate statin treatment. For guideline-concordant decision-making, risk estimates need to be calibrated. However, existing models are often miscalibrated for race, ethnicity and sex based subgroups. This study evaluates two algorithmic fairness approaches to adjust the risk estimators (group recalibration and equalised odds) for their compatibility with the assumptions underpinning the guidelines' decision rules.MethodsUsing an updated pooled cohorts data set, we derive unconstrained, group-recalibrated and equalised odds-constrained versions of the 10-year ASCVD risk estimators, and compare their calibration at guideline-concordant decision thresholds. We find that, compared with the unconstrained model, group-recalibration improves calibration at one of the relevant thresholds for each group, but exacerbates differences in false positive and false negative rates between groups. An equalised odds constraint, meant to equalise error rates across groups, does so by miscalibrating the model overall and at relevant decision thresholds. Hence, because of induced miscalibration, decisions guided by risk estimators learned with an equalised odds fairness constraint are not concordant with existing guidelines. Conversely, recalibrating the model separately for each group can increase guideline compatibility, while increasing intergroup differences in error rates. As such, comparisons of error rates across groups can be misleading when guidelines recommend treating at fixed decision thresholds. The illustrated tradeoffs between satisfying a fairness criterion and retaining guideline compatibility underscore the need to evaluate models in the context of downstream interventions.",success
34337435,False,Journal Article,,,,,,,,True,"Healthcare systems are using big data-driven methods to realize the vision of learning health systems and improve care quality. In so doing, many are partnering with third-party commercial companies to provide novel data processing and analysis capabilities, while also providing personal health information to a for-profit industry that may store and sell data. In this research we describe the public's comfort with sharing health data with third-party commercial companies for patient and business purposes and how this comfort is associated with demographic factors (sex, age, race/ethnicity, education, employment, income, insurance status, and self-reported health status), perceived healthcare access, and concerns about privacy. We surveyed the US public (<i>n</i> = 1841) to assess comfort with sharing health data with third-party commercial companies for patient or business purposes and examined whether there was a difference between comfort with data sharing for patient or business purposes. Univariate and stepwise regression modeling is used here to estimate the relationship between comfort with third-party commercial companies for patient and business purposes (outcomes) and demographic factors, self-reported health status, perceived healthcare access, and privacy concerns. The public is more comfortable sharing health data with third party commercial companies for patient purposes as compared to business purposes (paired <i>t</i> = 39.84, <i>p</i> < 0.001). Higher education was associated with greater comfort with sharing health data for patient purposes (<i>β</i> = 0.205, <i>p</i> < 0.001) and decreased comfort with sharing health data for business purposes (<i>β</i> = -0.145, <i>p</i> = 0.079). An inverse relationship exists between privacy concerns and comfort with sharing health data for both patient (<i>β</i> = -0.223, <i>p</i> < 0.001) and business purposes (<i>β</i> = -0.246, <i>p</i> < 0.001). Participants ages 45-59 were less comfortable sharing health data with third party commercial companies for patient purposes (<i>β</i> = -0.154, <i>p</i> = 0.0012) than participants aged 18-29. Proactive acknowledgment of privacy concerns and better communication of the steps being taken to protect the privacy of health data can increase patient comfort. Healthcare systems may be able to increase public and patient comfort with sharing health data with third-party commercial companies by emphasizing the patient-centered benefits of these partnerships.",success
31932798,False,Journal Article,,,,,,,,False,,success
28537812,False,Journal Article,,,,,,,,True,"Purpose To inform the evolving implementation of CancerLinQ and other rapid-learning systems for oncology care, we sought to evaluate perspectives of patients with cancer regarding ethical issues. Methods Using the GfK Group online research panel, representative of the US population, we surveyed 875 patients with cancer; 621 (71%) responded. We evaluated perceptions of appropriateness (scored from 1 to 10; 10, very appropriate) using scenarios and compared responses by age, race, and education. We constructed a scaled measure of comfort with secondary use of deidentified medical information and evaluated its correlates in a multivariable model. Results Of the sample, 9% were black and 9% Hispanic; 38% had completed high school or less, and 59% were age ≥ 65 years. Perceptions of appropriateness were highest when consent was obtained and university researchers used data to publish a research study (weighted mean appropriateness, 8.47) and lowest when consent was not obtained and a pharmaceutical company used data for marketing (weighted mean appropriateness, 2.7). Most respondents (72%) thought secondary use of data for research was very important, although those with lower education were less likely to endorse this (62% v 78%; P < .001). Overall, 35% believed it was necessary to obtain consent each time such research was to be performed; this proportion was higher among blacks/Hispanics than others (48% v 33%; P = .02). Comfort with the use of deidentified information from medical records varied by scenario and overall was associated with distrust in the health care system. Conclusion Perceptions of patients with cancer regarding secondary data use depend on the user and the specific use of the data, while also frequently differing by patient sociodemographic factors. Such information is critical to inform ongoing efforts to implement oncology learning systems.",success
35916854,False,Journal Article,,,,,,,,True,This study describes reported interest in notification regarding use of personal health information and biospecimens for research and preference-associated factors among a sample of the US population in 2019.,success
31504588,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Predictive analytics have begun to change the workflows of healthcare by giving insight into our future health. Deploying prognostic models into clinical workflows should change behavior and motivate interventions that affect outcomes. As users respond to model predictions, downstream characteristics of the data, including the distribution of the outcome, may change. The ever-changing nature of healthcare necessitates maintenance of prognostic models to ensure their longevity. The more effective a model and intervention(s) are at improving outcomes, the faster a model will appear to degrade. Improving outcomes can disrupt the association between the model's predictors and the outcome. Model refitting may not always be the most effective response to these challenges. These problems will need to be mitigated by systematically incorporating interventions into prognostic models and by maintaining robust performance surveillance of models in clinical use. Holistically modeling the outcome and intervention(s) can lead to resilience to future compromises in performance.",success
18086926,False,Journal Article,,,,,,,,False,,success
31584609,False,Journal Article,,,,,,,,False,,success
34235936,False,Journal Article;Review,,,,,,,,True,"There is growing evidence that people who are transgender and gender diverse (TGD) are impacted by disparities across a variety of cardiovascular risk factors compared with their peers who are cisgender. Prior literature has characterized disparities in cardiovascular morbidity and mortality as a result of a higher prevalence of health risk behaviors. Mounting research has revealed that cardiovascular risk factors at the individual level likely do not fully account for increased risk in cardiovascular health disparities among people who are TGD. Excess cardiovascular morbidity and mortality is hypothesized to be driven in part by psychosocial stressors across the lifespan at multiple levels, including structural violence (eg, discrimination, affordable housing, access to health care). This American Heart Association scientific statement reviews the existing literature on the cardiovascular health of people who are TGD. When applicable, the effects of gender-affirming hormone use on individual cardiovascular risk factors are also reviewed. Informed by a conceptual model building on minority stress theory, this statement identifies research gaps and provides suggestions for improving cardiovascular research and clinical care for people who are TGD, including the role of resilience-promoting factors. Advancing the cardiovascular health of people who are TGD requires a multifaceted approach that integrates best practices into research, health promotion, and cardiovascular care for this understudied population.",success
23612036,False,"Journal Article;Research Support, N.I.H., Extramural;Systematic Review",,,,,,,,True,"Although cardiovascular health has been improving for many Americans, this is not true of those in ""vulnerable populations."" To address this growing disparity, communities and researchers have worked for decades, and as a result of their work, a growing body of literature supports the use of community engagement as a component of successful interventions. However, little literature synthesizes community-based interventions that address this disparity among a wide range of vulnerable populations. This article provides a critical review of community-based cardiovascular disease interventions to improve cardiovascular health behaviors and factors among vulnerable populations based on the American Heart Association's 7 metrics of ideal cardiovascular health. In February 2011, 4 databases (PubMed, PsychInfo, CINAHL, and Scopus) were searched using the following keywords: vulnerable populations OR healthcare disparities AND cardiovascular disease AND clinical trials OR public health practice AND English. This search strategy resulted in the retrieval of 7120 abstracts. Each abstract was reviewed by at least 2 authors, and eligibility for the systematic review was confirmed after reading the full article. Thirty-two studies met eligibility criteria. Education was the most common intervention (41%), followed by counseling or support (38%) and exercise classes (28%). Half of the interventions were multicomponent. Healthcare providers were the most frequent interventionists. Interventions aimed at decreasing blood pressure were the most promising, whereas behavior change interventions were the most challenging. Almost all of the interventions were at the individual level and were proof-of-concept or efficacy trials. This analysis provides a step toward understanding the current literature on cardiovascular interventions for vulnerable population. The next step should be integrating the identified successful interventions into larger health systems and/or social policies.",success
30128552,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning-based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.",success
29374661,True,"Journal Article;Multicenter Study;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, Non-P.H.S.;Validation Study",,,,,,,,True,"We validate a machine learning-based sepsis-prediction algorithm (<i>InSight</i>) for the detection and prediction of three sepsis-related gold standards, using only six vital signs. We evaluate robustness to missing data, customisation to site-specific data using transfer learning and generalisability to new settings. A machine-learning algorithm with gradient tree boosting. Features for prediction were created from combinations of six vital sign measurements and their changes over time. A mixed-ward retrospective dataset from the University of California, San Francisco (UCSF) Medical Center (San Francisco, California, USA) as the primary source, an intensive care unit dataset from the Beth Israel Deaconess Medical Center (Boston, Massachusetts, USA) as a transfer-learning source and four additional institutions' datasets to evaluate generalisability. 684 443 total encounters, with 90 353 encounters from June 2011 to March 2016 at UCSF. None. Area under the receiver operating characteristic (AUROC) curve for detection and prediction of sepsis, severe sepsis and septic shock. For detection of sepsis and severe sepsis, <i>InSight</i> achieves an AUROC curve of 0.92 (95% CI 0.90 to 0.93) and 0.87 (95% CI 0.86 to 0.88), respectively. Four hours before onset, <i>InSight</i> predicts septic shock with an AUROC of 0.96 (95% CI 0.94 to 0.98) and severe sepsis with an AUROC of 0.85 (95% CI 0.79 to 0.91). <i>InSight</i> outperforms existing sepsis scoring systems in identifying and predicting sepsis, severe sepsis and septic shock. This is the first sepsis screening system to exceed an AUROC of 0.90 using only vital sign inputs. <i>InSight</i> is robust to missing data, can be customised to novel hospital data using a small fraction of site data and retains strong discrimination across all institutions.",success
32984550,False,Journal Article,,,,,,,,True,"At the beginning of the artificial intelligence (AI)/machine learning (ML) era, the expectations are high, and experts foresee that AI/ML shows potential for diagnosing, managing and treating a wide variety of medical conditions. However, the obstacles for implementation of AI/ML in daily clinical practice are numerous, especially regarding the regulation of these technologies. Therefore, we provide an insight into the currently available AI/ML-based medical devices and algorithms that have been approved by the US Food & Drugs Administration (FDA). We aimed to raise awareness of the importance of regulatory bodies, clearly stating whether a medical device is AI/ML based or not. Cross-checking and validating all approvals, we identified 64 AI/ML based, FDA approved medical devices and algorithms. Out of those, only 29 (45%) mentioned any AI/ML-related expressions in the official FDA announcement. The majority (85.9%) was approved by the FDA with a 510(k) clearance, while 8 (12.5%) received de novo pathway clearance and one (1.6%) premarket approval (PMA) clearance. Most of these technologies, notably 30 (46.9%), 16 (25.0%), and 10 (15.6%) were developed for the fields of Radiology, Cardiology and Internal Medicine/General Practice respectively. We have launched the first comprehensive and open access database of strictly AI/ML-based medical technologies that have been approved by the FDA. The database will be constantly updated.",success
33196064,False,Journal Article,,,,,,,,True,"Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as <i>hidden stratification</i>, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.",success
31433479,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Patients increasingly demand transparency in and control of how their medical records and biospecimens are shared for research. How much they are willing to share and what factors influence their sharing preferences remain understudied in real settings. To examine whether and how various presentations of consent forms are associated with differences in electronic health record and biospecimen sharing rates and whether these rates vary according to user interface design, data recipients, data and biospecimen items, and patient characteristics. For this survey study, a data and biospecimen sharing preference survey was conducted at 2 academic hospitals from May 1, 2017, to September 31, 2018, after simple randomization of patients to 1 of 4 options with different layout and formats of indicating sharing preferences: opt-in simple, opt-in detailed, opt-out simple, and opt-out detailed. All participants were presented with a list of data and biospecimen items that could be shared for research within the same health care organization or with other nonprofit or for-profit institutions. Participating patients were randomly asked to select the items that they would share (opt-in) or were asked to select items they would not share (opt-out). Patients in these 2 groups were further randomized to select only among 18 categories vs 59 detailed items (simple vs detailed form layout). The primary end points were the percentages of patients willing to share data and biospecimen categories or items. Among 1800 eligible participants, 1246 (69.2%) who completed their data sharing survey were included in the analysis, and 850 of these patients (mean [SD] age, 51.1 [16.7] years; 507 [59.6%] female; 677 [79.6%] white) responded to the satisfaction survey. A total of 46 participants (3.7%) declined sharing with the home institution, 352 (28.3%) with nonprofit institutions, and 590 (47.4%) with for-profit institutions. A total of 836 (67.1%) indicated that they would share all items with researchers from the home institution. When comparing opt-out with opt-in interfaces, all 59 sharing choice variables (100%) were associated with the sharing decision. When comparing simple with detailed forms, only 14 variables (23.7%) were associated with the sharing decision. The findings suggest that most patients are willing to share their data and biospecimens for research. Allowing patients to decide with whom they want to share certain types of data may affect research that involves secondary use of electronic health records and/or biosamples for research.",success
33840342,False,Journal Article;Systematic Review,,,,,,,,True,"Patient access to their own electronic health records (EHRs) is likely to become an integral part of healthcare systems worldwide. It has the potential to decrease the healthcare provision costs, improve access to healthcare data, self-care, quality of care, and health and patient-centered outcomes. This systematic literature review is aimed at identifying the impact in terms of benefits and issues that have so far been demonstrated by providing patients access to their own EHRs, via providers' secure patient portals from primary healthcare centers and hospitals. Searches were conducted in PubMed, MEDLINE, CINHAL, and Google scholar. Over 2000 papers were screened and were filtered based on duplicates, then by reading the titles and finally based on their abstracts or full text. In total, 74 papers were retained, analyzed, and summarized. Papers were included if providing patient access to their own EHRs was the primary intervention used in the study and its impact or outcome was evaluated. The search technique used to identify relevant literature for this paper involved input from five experts. While findings from 54 of the 74 papers showed positive outcome or benefits of patient access to their EHRs via patient portals, 10 papers have highlighted concerns, 8 papers have highlighted both and 2 have highlighted absence of negative outcomes. The benefits range from re-assurance, reduced anxiety, positive impact on consultations, better doctor-patient relationship, increased awareness and adherence to medication, and improved patient outcomes (e.g., improving blood pressure and glycemic control in a range of study populations). In addition, patient access to their health information was found to improve self-reported levels of engagement or activation related to self-management, enhanced knowledge, and improve recovery scores, and organizational efficiencies in a tertiary level mental health care facility. However, three studies did not find any statistically significant effect of patient portals on health outcomes. The main concerns have been around security, privacy and confidentiality of the health records, and the anxiety it may cause amongst patients. This literature review identified some benefits, concerns, and attitudes demonstrated by providing patients' access to their own EHRs. This access is often part of government strategies when developing patient-centric self-management elements of a sustainable healthcare system. The findings of this review will give healthcare providers a framework to analyze the benefits offered by promoting patient access to EHRs and decide on the best approach for their own specialties and clinical setup. A robust cost-benefit evaluation of such initiatives along with its impact on major stakeholders within the healthcare system would be essential in understanding the overall impact of such initiatives. Implementation of patient access to their EHRs could help governments to appropriately prioritize the development or adoption of national standards, whilst taking care of local variations and fulfilling the healthcare needs of the population, e.g., UK Government is aiming to make full primary care records available online to every patient. Ultimately, increasing transparency and promoting personal responsibility are key elements of a sustainable healthcare system for future generations.",success
32414183,False,Journal Article;Review,,,,,,,,True,"The Internet of Medical Things, Smart Devices, Information Systems, and Cloud Services have led to a digital transformation of the healthcare industry. Digital healthcare services have paved the way for easier and more accessible treatment, thus making our lives far more comfortable. However, the present day healthcare industry has also become the main victim of external as well as internal attacks. Data breaches are not just a concern and complication for security experts; they also affect clients, stakeholders, organizations, and businesses. Though the data breaches are of different types, their impact is almost always the same. This study provides insights into the various categories of data breaches faced by different organizations. The main objective is to do an in-depth analysis of healthcare data breaches and draw inferences from them, thereby using the findings to improve healthcare data confidentiality. The study found that hacking/IT incidents are the most prevalent forms of attack behind healthcare data breaches, followed by unauthorized internal disclosures. The frequency of healthcare data breaches, magnitude of exposed records, and financial losses due to breached records are increasing rapidly. Data from the healthcare industry is regarded as being highly valuable. This has become a major lure for the misappropriation and pilferage of healthcare data. Addressing this anomaly, the present study employs the simple moving average method and the simple exponential soothing method of time series analysis to examine the trend of healthcare data breaches and their cost. Of the two methods, the simple moving average method provided more reliable forecasting results.",success
33158905,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,False,,success
28923988,False,Journal Article;Review,,,,,,,,True,"Cardiogenic shock is a high-acuity, potentially complex, and hemodynamically diverse state of end-organ hypoperfusion that is frequently associated with multisystem organ failure. Despite improving survival in recent years, patient morbidity and mortality remain high, and there are few evidence-based therapeutic interventions known to clearly improve patient outcomes. This scientific statement on cardiogenic shock summarizes the epidemiology, pathophysiology, causes, and outcomes of cardiogenic shock; reviews contemporary best medical, surgical, mechanical circulatory support, and palliative care practices; advocates for the development of regionalized systems of care; and outlines future research priorities.",success
35835491,False,"Journal Article;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural",NCT04682483,databank,NCT04682483,NCT04682483,NCT04682483,NCT04682483|databank,NCT04682483|databank,True,"Risk-stratifying patients with cardiogenic shock (CS) is a major unmet need. The recently proposed Society for Cardiovascular Angiography and Interventions (SCAI) staging system for CS severity lacks uniform criteria defining each stage. The purpose of this study was to test parameters that define SCAI stages and explore their utility as predictors of in-hospital mortality in CS. The CS Working Group registry includes patients from 17 hospitals enrolled between 2016 and 2021 and was used to define clinical profiles for CS. We selected parameters of hypotension and hypoperfusion and treatment intensity, confirmed their association with mortality, then defined formal criteria for each stage and tested the association between both baseline and maximum Stage and mortality. Of 3,455 patients, CS was caused by heart failure (52%) or myocardial infarction (32%). Mortality was 35% for the total cohort and higher among patients with myocardial infarction, out-of-hospital cardiac arrest, and treatment with increasing numbers of drugs and devices. Systolic blood pressure, lactate level, alanine transaminase level, and systemic pH were significantly associated with mortality and used to define each stage. Using these criteria, baseline and maximum stages were significantly associated with mortality (n = 1,890). Lower baseline stage was associated with a higher incidence of stage escalation and a shorter duration of time to reach maximum stage. We report a novel approach to define SCAI stages and identify a significant association between baseline and maximum stage and mortality. This approach may improve clinical application of the staging system and provides new insight into the trajectory of hospitalized CS patients. (Cardiogenic Shock Working Group Registry [CSWG]; NCT04682483).",success
31104355,False,Journal Article;Practice Guideline;Consensus Development Conference,,,,,,,,True,"The outcome of cardiogenic shock complicating myocardial infarction has not appreciably changed in the last 30 years despite the development of various percutaneous mechanical circulatory support options. It is clear that there are varying degrees of cardiogenic shock but there is no robust classification scheme to categorize this disease state. A multidisciplinary group of experts convened by the Society for Cardiovascular Angiography and Interventions was assembled to derive a proposed classification schema for cardiogenic shock. Representatives from cardiology (interventional, advanced heart failure, noninvasive), emergency medicine, critical care, and cardiac nursing all collaborated to develop the proposed schema. A system describing stages of cardiogenic shock from A to E was developed. Stage A is ""at risk"" for cardiogenic shock, stage B is ""beginning"" shock, stage C is ""classic"" cardiogenic shock, stage D is ""deteriorating"", and E is ""extremis"". The difference between stages B and C is the presence of hypoperfusion which is present in stages C and higher. Stage D implies that the initial set of interventions chosen have not restored stability and adequate perfusion despite at least 30 minutes of observation and stage E is the patient in extremis, highly unstable, often with cardiovascular collapse. This proposed classification system is simple, clinically applicable across the care spectrum from pre-hospital providers to intensive care staff but will require future validation studies to assess its utility and potential prognostic implications.",success
34368248,False,Journal Article,,,,,,,,True,"<b>Background:</b> Advanced age is associated with poor outcomes in cardiovascular emergencies. We sought to determine the association of age, use of support devices and shock severity on mortality in cardiogenic shock (CS). <b>Methods:</b> Characteristics and outcomes in CS patients included in the Cardiogenic Shock Work Group (CSWG) registry from 8 US sites between 2016 and 2019 were retrospectively reviewed. Patients were subdivided by age into quintiles and Society for Cardiovascular Angiography & Interventions (SCAI) shock severity. <b>Results:</b> We reviewed 1,412 CS patients with a mean age of 59.9 ± 14.8 years, including 273 patients > 73 years of age. Older patients had significantly higher comorbidity burden including diabetes, hypertension and coronary artery disease. Veno-arterial extracorporeal membrane oxygenation was used in 332 (23%) patients, Impella in 410 (29%) and intra-aortic balloon pump in 770 (54%) patients. Overall in-hospital survival was 69%, which incrementally decreased with advancing age (<i>p</i> < 0.001). Higher age was associated with higher mortality across all SCAI stages (<i>p</i> = 0.003 for SCAI stage C; <i>p</i> < 0.001 for SCAI stage D; <i>p</i> = 0.005 for SCAI stage E), regardless of etiology (<i>p</i> < 0.001). <b>Conclusion:</b> Increasing age is associated with higher in-hospital mortality in CS across all stages of shock severity. Hence, in addition to other comorbidities, increasing age should be prioritized during patient selection for device support in CS.",success
34126755,False,Journal Article;Review,,,,,,,,True,"Over the past few decades, advances in pharmacological, catheter-based, and surgical reperfusion have improved outcomes for patients with acute myocardial infarctions. However, patients with large infarcts or those who do not receive timely revascularization remain at risk for mechanical complications of acute myocardial infarction. The most commonly encountered mechanical complications are acute mitral regurgitation secondary to papillary muscle rupture, ventricular septal defect, pseudoaneurysm, and free wall rupture; each complication is associated with a significant risk of morbidity, mortality, and hospital resource utilization. The care for patients with mechanical complications is complex and requires a multidisciplinary collaboration for prompt recognition, diagnosis, hemodynamic stabilization, and decision support to assist patients and families in the selection of definitive therapies or palliation. However, because of the relatively small number of high-quality studies that exist to guide clinical practice, there is significant variability in care that mainly depends on local expertise and available resources.",success
34823659,True,"Journal Article;Multicenter Study;Research Support, N.I.H., Extramural",,,,,,,,True,"Recent trends, including survival beyond 30 days, in aortic valve replacement (AVR) following the expansion of indications for transcatheter aortic valve replacement (TAVR) are not well-understood. The authors sought to characterize the trends in characteristics and outcomes of patients undergoing AVR. The authors analyzed Medicare beneficiaries who underwent TAVR and SAVR in 2012 to 2019. They evaluated case volume, demographics, comorbidities, 1-year mortality, and discharge disposition. Cox proportional hazard models were used to assess the annual change in outcomes. Per 100,000 beneficiary-years, AVR increased from 107 to 156, TAVR increased from 19 to 101, whereas SAVR declined from 88 to 54. The median [interquartile range] age remained similar from 77 [71-83] years to 78 [72-84] years for overall AVR, decreased from 84 [79-88] years to 81 [75-86] years for TAVR, and decreased from 76 [71-81] years to 72 [68-77] years for SAVR. For all AVR patients, the prevalence of comorbidities remained relatively stable. The 1-year mortality for all AVR decreased from 11.9% to 9.4%. Annual change in the adjusted odds of 1-year mortality was 0.93 (95% CI: 0.92-0.94) for TAVR and 0.98 (95% CI: 0.97-0.99) for SAVR, and 0.94 (95% CI: 0.93-0.95) for all AVR. Patients discharged to home after AVR increased from 24.2% to 54.7%, primarily driven by increasing home discharge after TAVR. The advent of TAVR has led to about a 60% increase in overall AVR in older adults. Improving outcomes in AVR as a whole following the advent of TAVR with increased access is a reassuring trend.",success
34625129,False,Journal Article;Review,,,,,,,,True,"Cardiogenic shock (CS) is a condition associated with high mortality rates in which prognostication is uncertain for a variety of reasons, including its myriad causes, its rapidly evolving clinical course and the plethora of established and emerging therapies for the condition. A number of validated risk scores are available for CS prognostication; however, many of these are tedious to use, are designed for application in a variety of populations and fail to incorporate contemporary hemodynamic parameters and contemporary mechanical circulatory support interventions that can affect outcomes. It is important to separate patients with CS who may recover with conservative pharmacological therapies from those in who may require advanced therapies to survive; it is equally important to identify quickly those who will succumb despite any therapy. An ideal risk-prediction model would balance incorporation of key hemodynamic parameters while still allowing dynamic use in multiple scenarios, from aiding with early decision making to device weaning. Herein, we discuss currently available CS risk scores, perform a detailed analysis of the variables in each of these scores that are most predictive of CS outcomes and explore a framework for the development of novel risk scores that consider emerging therapies and paradigms for this challenging clinical entity.",success
35115207,False,Consensus Development Conference;Journal Article,,,,,,,,False,,success
33682260,False,Journal Article;Review,,,,,,,,True,"Acute myocardial infarction (AMI) complicated by cardiogenic shock (CS) is associated with significant morbidity and mortality. We provide an overview of previously conducted studies on the use of mechanical circulatory support (MCS) devices in the treatment of AMI-CS and difficulties which may be encountered in conducting such trials in the United States. Well powered randomized control trials are difficult to conduct in a critically ill patient population due to physician preferences, perceived lack of equipoise and challenges obtaining informed consent. With growth in utilization of MCS devices in patients with AMI-CS, efforts to perform well-powered, randomized control trials must be undertaken.",success
33121700,False,Journal Article;Review,,,,,,,,True,"Cardiogenic shock is a hemodynamically complex syndrome characterized by a low cardiac output that often culminates in multiorgan system failure and death. Despite recent advances, clinical outcomes remain poor, with mortality rates exceeding 40%. In the absence of adequately powered randomized controlled trials to guide therapy, best practices for shock management remain nonuniform. Emerging data from North American registries, however, support the use of standardized protocols focused on rapid diagnosis, early intervention, ongoing hemodynamic assessment, and multidisciplinary longitudinal care. In this review, the authors examine the pathophysiology and phenotypes of cardiogenic shock, benefits and limitations of current therapies, and they propose a standardized and team-based treatment algorithm. Lastly, they discuss future research opportunities to address current gaps in clinical knowledge.",success
33464952,False,Journal Article,,,,,,,,True,"Previous studies have defined preshock as isolated hypotension or isolated hypoperfusion, whereas shock has been variably defined as hypoperfusion with or without hypotension. We aimed to evaluate the mortality risk associated with hypotension and hypoperfusion at the time of admission in a cardiac intensive care unit population. We analyzed Mayo Clinic cardiac intensive care unit patients admitted between 2007 and 2015. Hypotension was defined as systolic blood pressure <90 mm Hg or mean arterial pressure <60 mm Hg, and hypoperfusion as admission lactate >2 mmol/L, oliguria, or rising creatinine. Associations between hypotension and hypoperfusion with hospital mortality were estimated using multivariable logistic regression. Among 10 004 patients with a median age of 69 years, 43.1% had acute coronary syndrome, and 46.1% had heart failure. Isolated hypotension was present in 16.7%, isolated hypoperfusion in 15.3%, and 8.7% had both hypotension and hypoperfusion. Stepwise increases in hospital mortality were observed with hypotension and hypoperfusion compared with neither hypotension nor hypoperfusion (3.3%; all <i>P</i><0.001): isolated hypotension, 9.3% (adjusted odds ratio, 1.7 [95% CI, 1.4-2.2]); isolated hypoperfusion, 17.2% (adjusted odds ratio, 2.3 [95% CI, 1.9-3.0]); both hypotension and hypoperfusion, 33.8% (adjusted odds ratio, 2.8 [95% CI, 2.1-3.6]). Adjusted hospital mortality in patients with isolated hypoperfusion was higher than in patients with isolated hypotension (<i>P</i>=0.02) and not significant different from patients with both hypotension and hypoperfusion (<i>P</i>=0.18). Hypotension and hypoperfusion are both associated with increased mortality in cardiac intensive care unit patients. Hospital mortality is higher with isolated hypoperfusion or concomitant hypotension and hypoperfusion (classic shock). We contend that preshock should refer to isolated hypotension without hypoperfusion, while patients with hypoperfusion can be considered to have shock, irrespective of blood pressure.",success
34227396,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Background Cardiogenic shock (CS) is a heterogeneous syndrome with varied presentations and outcomes. We used a machine learning approach to test the hypothesis that patients with CS have distinct phenotypes at presentation, which are associated with unique clinical profiles and in-hospital mortality. Methods and Results We analyzed data from 1959 patients with CS from 2 international cohorts: CSWG (Cardiogenic Shock Working Group Registry) (myocardial infarction [CSWG-MI; n=410] and acute-on-chronic heart failure [CSWG-HF; n=480]) and the DRR (Danish Retroshock MI Registry) (n=1069). Clusters of patients with CS were identified in CSWG-MI using the consensus k means algorithm and subsequently validated in CSWG-HF and DRR. Patients in each phenotype were further categorized by their Society of Cardiovascular Angiography and Interventions staging. The machine learning algorithms revealed 3 distinct clusters in CS: ""non-congested (I)"", ""cardiorenal (II),"" and ""cardiometabolic (III)"" shock. Among the 3 cohorts (CSWG-MI versus DDR versus CSWG-HF), in-hospital mortality was 21% versus 28% versus 10%, 45% versus 40% versus 32%, and 55% versus 56% versus 52% for clusters I, II, and III, respectively. The ""cardiometabolic shock"" cluster had the highest risk of developing stage D or E shock as well as in-hospital mortality among the phenotypes, regardless of cause. Despite baseline differences, each cluster showed reproducible demographic, metabolic, and hemodynamic profiles across the 3 cohorts. Conclusions Using machine learning, we identified and validated 3 distinct CS phenotypes, with specific and reproducible associations with mortality. These phenotypes may allow for targeted patient enrollment in clinical trials and foster development of tailored treatment strategies in subsets of patients with CS.",success
33580778,False,Journal Article,,,,,,,,True,"Cardiogenic shock (CS) is associated with poor outcomes in older patients, but it remains unclear if this is due to higher shock severity. We sought to determine the associations between age and shock severity on mortality among patients with CS. Patients with a diagnosis of CS from Mayo Clinic (2007-15) and University Clinic Hamburg (2009-17) were subdivided by age. Shock severity was graded using the Society for Cardiovascular Angiography and Intervention (SCAI) shock stages. Predictors of 30-day survival were determined using Cox proportional-hazards analysis. We included 1749 patients (934 from Mayo Clinic and 815 from University Clinic Hamburg), with a mean age of 67.6 ± 14.6 years, including 33.6% females. Acute coronary syndrome was the cause of CS in 54.0%. The distribution of SCAI shock stages was 24.1%; C, 28.0%; D, 33.2%; and E, 14.8%. Older patients had similar overall shock severity, more co-morbidities, worse kidney function, and decreased use of mechanical circulatory support compared to younger patients. Overall 30-day survival was 53.3% and progressively decreased as age or SCAI shock stage increased, with a clear gradient towards lower 30-day survival as a function of increasing age and SCAI shock stage. Progressively older age groups had incrementally lower adjusted 30-day survival than patients aged <50 years. Older patients with CS have lower short-term survival, despite similar shock severity, with a high risk of death in older patients with more severe shock. Further research is needed to determine the optimal treatment strategies for older CS patients.",success
34010224,False,Journal Article;Review,,,,,,,,True,"Cardiogenic shock is a complex clinical syndrome of end-organ hypoperfusion due to impaired cardiac performance. Although cardiogenic shock has traditionally been viewed as a monolithic disorder predominantly caused by severe left ventricular dysfunction complicating acute myocardial infarction (AMI), there is increasing recognition of the diverse causes of cardiogenic shock and wide spectrum of clinical severity. The purpose of this review is to describe the contemporary epidemiology of cardiogenic shock, including trends in clinical outcomes and recent efforts to refine risk assessment. The incidence of cardiogenic shock among patients with AMI has remained remarkably stable at 3-10%; however, the proportion of cardiogenic shock cases related to AMI has decreased over time to ∼30%, while the proportion of cardiogenic shock cases due to acute decompensated heart failure has steadily increased. Estimated in-hospital mortality from cardiogenic shock in contemporary registries is approximately 30-40%, suggesting modest improvement in cardiogenic shock outcomes over the last decade. There is a wide spectrum of clinical severity among patients presenting with cardiogenic shock, which is described by the Society for Cardiovascular Angiography and Interventions clinical staging criteria. Improved clinical characterization and risk assessment of patients with cardiogenic shock may facilitate more effective clinical investigations of this morbid clinical syndrome.",success
35275890,False,Journal Article;Review,,,,,,,,True,"Despite novel technologies for treating shock patients, cardiogenic shock mortality remains high. Trends of cardiogenic shock associated with acute myocardial infarction (AMI) have previously been described, though little is known about cardiogenic shock resulting from other causes, which has progressively been documented as a distinct entity from AMI-cardiogenic shock. Herein, we review the evolving epidemiology, novel classification schema, and future perspectives of cardiogenic shock. While AMI or mechanical complications of AMI are the most common causes, the incidence of etiologies of cardiogenic shock not related to AMI, particularly acute on chronic heart failure, may be increasing, with a growing burden of noncoronary structural heart disease. Mortality in cardiogenic shock remains high. Overall, these findings highlight the need to address the lack of effective treatments in this field, particularly for cardiogenic shock caused by diseases other than AMI. Novel classification systems may facilitate cardiogenic shock research.",success
32469155,False,Journal Article;Review;Consensus Development Conference,,,,,,,,True,"Cardiogenic shock (CS) is a complex multifactorial clinical syndrome with extremely high mortality, developing as a continuum, and progressing from the initial insult (underlying cause) to the subsequent occurrence of organ failure and death. There is a large spectrum of CS presentations resulting from the interaction between an acute cardiac insult and a patient's underlying cardiac and overall medical condition. Phenotyping patients with CS may have clinical impact on management because classification would support initiation of appropriate therapies. CS management should consider appropriate organization of the health care services, and therapies must be given to the appropriately selected patients, in a timely manner, whilst avoiding iatrogenic harm. Although several consensus-driven algorithms have been proposed, CS management remains challenging and substantial investments in research and development have not yielded proof of efficacy and safety for most of the therapies tested, and outcome in this condition remains poor. Future studies should consider the identification of the new pathophysiological targets, and high-quality translational research should facilitate incorporation of more targeted interventions in clinical research protocols, aimed to improve individual patient outcomes. Designing outcome clinical trials in CS remains particularly challenging in this critical and very costly scenario in cardiology, but information from these trials is imperiously needed to better inform the guidelines and clinical practice. The goal of this review is to summarize the current knowledge concerning the definition, epidemiology, underlying causes, pathophysiology and management of CS based on important lessons from clinical trials and registries, with a focus on improving in-hospital management.",success
30879324,True,"Journal Article;Multicenter Study;Research Support, N.I.H., Extramural;Research Support, N.I.H., Intramural",,,,,,,,True,"Background Clinical investigations of shock in cardiac intensive care units (CICUs) have primarily focused on acute myocardial infarction (AMI) complicated by cardiogenic shock (AMICS). Few studies have evaluated the full spectrum of shock in contemporary CICUs. Methods and Results The Critical Care Cardiology Trials Network is a multicenter network of advanced CICUs in North America. Anytime between September 2017 and September 2018, each center (n=16) contributed a 2-month snap-shot of all consecutive medical admissions to the CICU. Data were submitted to the central coordinating center (TIMI Study Group, Boston, MA). Shock was defined as sustained systolic blood pressure <90 mm Hg with end-organ dysfunction ascribed to the hypotension. Shock type was classified by site investigators as cardiogenic, distributive, hypovolemic, or mixed. Among 3049 CICU admissions, 677 (22%) met clinical criteria for shock. Shock type was varied, with 66% assessed as cardiogenic shock (CS), 7% as distributive, 3% as hypovolemic, 20% as mixed, and 4% as unknown. Among patients with CS (n=450), 30% had AMICS, 18% had ischemic cardiomyopathy without AMI, 28% had nonischemic cardiomyopathy, and 17% had a cardiac cause other than primary myocardial dysfunction. Patients with mixed shock had cardiovascular comorbidities similar to patients with CS. The median CICU stay was 4.0 days (interquartile range [IQR], 2.5-8.1 days) for AMICS, 4.3 days (IQR, 2.1-8.5 days) for CS not related to AMI, and 5.8 days (IQR, 2.9-10.0 days) for mixed shock versus 1.9 days (IQR, 1.0-3.6) for patients without shock ( P<0.01 for each). Median Sequential Organ Failure Assessment scores were higher in patients with mixed shock (10; IQR, 6-13) versus AMICS (8; IQR, 5-11) or CS without AMI (7; IQR, 5-11; each P<0.01). In-hospital mortality rates were 36% (95% CI, 28%-45%), 31% (95% CI, 26%-36%), and 39% (95% CI, 31%-48%) in AMICS, CS without AMI, and mixed shock, respectively. Conclusions The epidemiology of shock in contemporary advanced CICUs is varied, and AMICS now represents less than one-third of all CS. Despite advanced therapies, mortality in CS and mixed shock remains high. Investigation of management strategies and new therapies to treat shock in the CICU should take this epidemiology into account.",success
26339723,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Despite advances in the management of patients with acute coronary syndrome (ACS), cardiogenic shock (CS) remains the leading cause of death in these patients. We describe the evolution of clinical characteristics, in-hospital management, and outcome of patients with CS complicating ACS. We analysed data from five Italian nationwide prospective registries, conducted between 2001 and 2014, including consecutive patients with ACS. Out of 28 217 ACS patients enrolled, 1209 (4.3%) had CS: 526 (44%) at the time of admission and 683 (56%) later on during hospitalization. Over the years, a reduction in the incidence of CS was observed, even though this was not statistically significant (P for trend = 0.17). The proportions of CS patients with a history of heart failure declined, whereas the proportion of those with hypertension, renal dysfunction, previous PCI, and AF significantly increased. The use of PCI considerably increased from 2001 to 2014 [19% to 60%; percentage change 41, 95% confidence interval (CI) 29-51]. In-hospital mortality of CS patients decreased from 68% (95% CI 59-76) in 2001 to 38% (95% CI 29-47) in 2014 (percentage change -30, 95% CI -41 to -18). Compared with 2001, the risk of death was significantly lower in all of the registries, with reductions in adjusted mortality between 45% and 66%. Over the last 14 years, substantial changes occurred in the clinical characteristics and management of patients with CS complicating ACS, with a greater use of PCI and a significant reduction in adjusted mortality rate.",success
36695182,False,Journal Article;Review,,,,,,,,True,"The American Heart Association, in conjunction with the National Institutes of Health, annually reports the most up-to-date statistics related to heart disease, stroke, and cardiovascular risk factors, including core health behaviors (smoking, physical activity, diet, and weight) and health factors (cholesterol, blood pressure, and glucose control) that contribute to cardiovascular health. The Statistical Update presents the latest data on a range of major clinical heart and circulatory disease conditions (including stroke, congenital heart disease, rhythm disorders, subclinical atherosclerosis, coronary heart disease, heart failure, valvular disease, venous disease, and peripheral artery disease) and the associated outcomes (including quality of care, procedures, and economic costs). The American Heart Association, through its Epidemiology and Prevention Statistics Committee, continuously monitors and evaluates sources of data on heart disease and stroke in the United States to provide the most current information available in the annual Statistical Update with review of published literature through the year before writing. The 2023 Statistical Update is the product of a full year's worth of effort in 2022 by dedicated volunteer clinicians and scientists, committed government professionals, and American Heart Association staff members. The American Heart Association strives to further understand and help heal health problems inflicted by structural racism, a public health crisis that can significantly damage physical and mental health and perpetuate disparities in access to health care, education, income, housing, and several other factors vital to healthy lives. This year's edition includes additional COVID-19 (coronavirus disease 2019) publications, as well as data on the monitoring and benefits of cardiovascular health in the population, with an enhanced focus on health equity across several key domains. Each of the chapters in the Statistical Update focuses on a different topic related to heart disease and stroke statistics. The Statistical Update represents a critical resource for the lay public, policymakers, media professionals, clinicians, health care administrators, researchers, health advocates, and others seeking the best available data on these factors and conditions.",success
37675262,False,Journal Article,,,,,,,,True,"As a result of improved and novel treatment strategies, the spectrum of patients with cardiovascular disease is consistently changing. Overall, those patients are typically older and characterized by increased burden with comorbidities. Limited data on the prognostic impact of age in cardiogenic shock (CS) is available. Therefore, this study investigates the prognostic impact of age in patients with CS. From 2019 to 2021, consecutive patients with CS of any cause were included. The prognostic value of age (i.e., 60-80 years and > 80 years) was investigated for 30-day all-cause mortality. Spearman's correlations, Kaplan-Meier analyses, as well as multivariable Cox proportional regression analyses were performed for statistics. Subsequent risk assessment was performed based on the presence or absence of CS related to acute myocardial infarction (AMI). 223 CS patients were included with a median age of 77 years (interquartile range: 69-82 years). No significant difference in 30-day all-cause mortality was observed for both age-groups (54.6% <i>vs.</i> 63.4%, log-rank <i>P</i> = 0.169; HR = 1.273, 95% CI: 0.886-1.831, <i>P</i> = 0.192). In contrast, when analyzing subgroups stratified by CS-etiology, AMI-related CS patients of the group > 80 years showed an increased risk of 30-day all-cause mortality (78.1% <i>vs.</i> 60.0%, log-rank <i>P</i> = 0.032; HR = 1.635, 95% CI: 1.000-2.673, <i>P</i> = 0.050), which was still evident after multivariable adjustment (HR = 2.072, 95% CI: 1.174-3.656, <i>P</i> = 0.012). Age was not associated with 30-day all-cause mortality in patients with CS of mixed etiology. However, increasing age was shown to be a significant predictor of increased mortality-risk in the subgroup of patients presenting with AMI-CS.",success
25820680,True,"Journal Article;Multicenter Study;Observational Study;Research Support, Non-U.S. Gov't",NCT01374867,databank,NCT01374867,NCT01374867,NCT01374867,NCT01374867|databank,NCT01374867|databank,True,"The aim of this study was to investigate the clinical picture and outcome of cardiogenic shock and to develop a risk prediction score for short-term mortality. The CardShock study was a multicentre, prospective, observational study conducted between 2010 and 2012. Patients with either acute coronary syndrome (ACS) or non-ACS aetiologies were enrolled within 6 h from detection of cardiogenic shock defined as severe hypotension with clinical signs of hypoperfusion and/or serum lactate >2 mmol/L despite fluid resuscitation (n = 219, mean age 67, 74% men). Data on clinical presentation, management, and biochemical variables were compared between different aetiologies of shock. Systolic blood pressure was on average 78 mmHg (standard deviation 14 mmHg) and mean arterial pressure 57 (11) mmHg. The most common cause (81%) was ACS (68% ST-elevation myocardial infarction and 8% mechanical complications); 94% underwent coronary angiography, of which 89% PCI. Main non-ACS aetiologies were severe chronic heart failure and valvular causes. In-hospital mortality was 37% (n = 80). ACS aetiology, age, previous myocardial infarction, prior coronary artery bypass, confusion, low LVEF, and blood lactate levels were independently associated with increased mortality. The CardShock risk Score including these variables and estimated glomerular filtration rate predicted in-hospital mortality well (area under the curve 0.85). Although most commonly due to ACS, other causes account for one-fifth of cases with shock. ACS is independently associated with in-hospital mortality. The CardShock risk Score, consisting of seven common variables, easily stratifies risk of short-term mortality. It might facilitate early decision-making in intensive care or guide patient selection in clinical trials. NCT01374867.",success
26033984,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Extracorporeal membrane oxygenation (ECMO) may provide mechanical pulmonary and circulatory support for patients with cardiogenic shock refractory to conventional medical therapy. Prediction of survival in these patients may assist in management of these patients and comparison of results from different centers. To identify pre-ECMO factors which predict survival from refractory cardiogenic shock requiring ECMO and create the survival after veno-arterial-ECMO (SAVE)-score. Patients with refractory cardiogenic shock treated with veno-arterial ECMO between January 2003 and December 2013 were extracted from the international Extracorporeal Life Support Organization registry. Multivariable logistic regression was performed using bootstrapping methodology with internal and external validation to identify factors independently associated with in-hospital survival. Of 3846 patients with cardiogenic shock treated with ECMO, 1601 (42%) patients were alive at hospital discharge. Chronic renal failure, longer duration of ventilation prior to ECMO initiation, pre-ECMO organ failures, pre-ECMO cardiac arrest, congenital heart disease, lower pulse pressure, and lower serum bicarbonate (HCO3) were risk factors associated with mortality. Younger age, lower weight, acute myocarditis, heart transplant, refractory ventricular tachycardia or fibrillation, higher diastolic blood pressure, and lower peak inspiratory pressure were protective. The SAVE-score (area under the receiver operating characteristics [ROC] curve [AUROC] 0.68 [95%CI 0.64-0.71]) was created. External validation of the SAVE-score in an Australian population of 161 patients showed excellent discrimination with AUROC = 0.90 (95%CI 0.85-0.95). The SAVE-score may be a tool to predict survival for patients receiving ECMO for refractory cardiogenic shock (www.save-score.com).",success
26825953,True,Journal Article;Multicenter Study,,,,,,,,True,"This study was designed to identify factors associated with in-intensive care unit (ICU) death and develop a practical mortality risk score for venoarterial-extracorporeal membrane oxygenation (VA-ECMO)-treated acute myocardial infarction (AMI) patients. Long-term survivors' health-related quality of life (HRQOL), anxiety, depression, and post-traumatic stress disorder (PTSD) frequencies were also assessed. Data from 138 ECMO-treated AMI patients admitted to two French ICUs (2008-2013) were analyzed. ICU survivors contacted >6 months post-ICU discharge were assessed for HRQOL, psychological and PTSD status. Sixty-five patients (47%) survived to ICU discharge. On the basis of multivariable logistic regression analyses, the ENCOURAGE score was constructed with seven pre-ECMO parameters: age >60, female sex, body mass index >25 kg/m(2), Glasgow coma score <6, creatinine >150 μmol/L, lactate (<2, 2-8, or >8 mmol/L), and prothrombin activity <50%. Six months after ECMO, probabilities of survival were 80, 58, 25, 20, and 7% for ENCOURAGE score classes 0-12, 13-18, 19-22, 23-27, and ≥28, respectively. The ENCOURAGE score ROC AUC [0.84 (95% CI 0.77-0.91)] was significantly better than those of the SAVE, SAPS II, and SOFA scores. Survivors' HRQOL evaluated after median follow-up of 32 months revealed satisfactory mental health but persistent physical and emotional-related difficulties, with 34% (95% CI 20-49%) anxiety, 20% (95% CI 8-32%) depression, and 5% (95% CI 0-12%) PTSD symptoms reported. The ENCOURAGE score might be a useful tool to predict mortality of severe cardiogenic shock AMI patients who received VA-ECMO. However, it now needs prospective validation on other populations of AMI patients.",success
26044753,True,"Journal Article;Multicenter Study;Observational Study;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"To determine the association between age and outcome in a large multicenter cohort of out-of-hospital cardiac arrest patients. Retrospective, observational, cohort study of out-of-hospital cardiac arrest from the CARES registry between 2006 and 2013. Age was categorized into 5-year intervals and the association between age group and outcomes (return of spontaneous circulation (ROSC), survival and good neurological outcome) was assessed in univariable and multivariable analysis. We performed a subgroup analysis in patients who had return of spontaneous circulation. A total of 101,968 people were included. The median age was 66 years (quartiles: 54, 78) and 39% were female. 31,236 (30.6%) of the included patients had sustained ROSC, 9761 (9.6%) survived to hospital discharge and 8058 (7.9%) survived with a good neurological outcome. The proportion of patients with ROSC was highest in those with age <20 years (34.1%) and lowest in those with age 95-99 years (23.5%). Patients with age <20 years had the highest proportion of survival (16.7%) and good neurological outcome (14.8%) whereas those with age 95-99 years had the lowest proportion of survival (1.7%) and good neurological outcome (1.2%). In the full cohort and in the patients with ROSC there appeared to be a progressive decline in survival and good neurological outcome after the age of approximately 45-64 years. Age alone was not a good predictor of outcome. Advanced age is associated with outcomes in out-of-hospital cardiac arrest. We did not identify a specific age threshold beyond which the chance of a meaningful recovery was excluded.",success
35115105,False,"Journal Article;Research Support, N.I.H., Extramural;Review",,,,,,,,True,"With the aging of the world's population, a large proportion of patients seen in cardiovascular practice are older adults, but many patients also exhibit signs of physical frailty. Cardiovascular disease and frailty are interdependent and have the same physiological underpinning that predisposes to the progression of both disease processes. Frailty can be defined as a phenomenon of increased vulnerability to stressors due to decreased physiological reserves in older patients and thus leads to poor clinical outcomes after cardiovascular insults. There are various pathophysiologic mechanisms for the development of frailty: cognitive decline, physical inactivity, poor nutrition, and lack of social supports; these risk factors provide opportunity for various types of interventions that aim to prevent, improve, or reverse the development of frailty syndrome in the context of cardiovascular disease. There is no compelling study demonstrating a successful intervention to improve a global measure of frailty. Emerging data from patients admitted with heart failure indicate that interventions associated with positive outcomes on frailty and physical function are multidimensional and include tailored cardiac rehabilitation. Contemporary cardiovascular practice should actively identify patients with physical frailty who could benefit from frailty interventions and aim to deliver these therapies in a patient-centered model to optimize quality of life, particularly after cardiovascular interventions.",success
30947919,False,Journal Article,,,,,,,,True,"Cardiogenic shock (CS) is a multifactorial, hemodynamically complex syndrome associated with high mortality. Despite advances in reperfusion and mechanical circulatory support, management remains highly variable and outcomes poor. This study investigated whether a standardized team-based approach can improve outcomes in CS and whether a risk score can guide clinical decision making. A total of 204 consecutive patients with CS were identified. CS etiology, patient demographic characteristics, right heart catheterization, mechanical circulatory support use, and survival were determined. Cardiac power output (CPO) and pulmonary arterial pulsatility index (PAPi) were measured at baseline and 24 h after the CS diagnosis. Thresholds at 24 h for lactate (<3.0 mg/dl), CPO (>0.6 W), and PAPi (>1.0) were determined. Using logistic regression analysis, a validated risk stratification score was developed. Compared with 30-day survival of 47% in 2016, 30-day survival in 2017 and 2018 increased to 57.9% and 76.6%, respectively (p < 0.01). Independent predictors of 30-day mortality were age ≥71 years, diabetes mellitus, dialysis, ≥36 h of vasopressor use at time of diagnosis, lactate levels ≥3.0 mg/dl, CPO <0.6 W, and PAPi <1.0 at 24 h after diagnosis and implementation of therapies. Either 1 or 2 points were assigned to each variable, and a 3-category risk score was determined: 0 to 1 (low), 2 to 4 (moderate), and ≥5 (high). This observational study suggests that a standardized team-based approach may improve CS outcomes. A score incorporating demographic, laboratory, and hemodynamic data may be used to quantify risk and guide clinical decision-making for all phenotypes of CS.",success
31580493,False,Journal Article;Review,,,,,,,,True,"Shared decision-making is appropriate for clinical decisions involving multiple reasonable options, which occur frequently in the cardiovascular care of older adults. The process includes the communication of relevant factual information between the patient and the clinician, elicitation of patient preferences, and a mutual agreement on the best course of action to meet the patient's personal goals. For older adults, there are common challenges and considerations with regard to shared decision-making, some of which (eg, cognitive impairment) may be biologically linked to cardiovascular disease. There are tools designed to facilitate the shared decision-making process, known as decision aids, which are broadly effective although have shortcomings when applied to older adults. Novel approaches in clinical research and health systems changes will go some way toward improving shared decision-making for older adults, but the greatest scope for improvement may be within the grass roots areas of communication skills, interdisciplinary teamwork, and simply asking our patients what matters most.",success
25700590,False,Journal Article;Review,,,,,,,,True,"Key components of advance care planning (ACP) for the elderly include choosing a surrogate decision maker, identifying personal values, communicating with surrogates and clinicians, documenting wishes in advance directives, and translating values and preferences for future medical care into medical orders. ACP often involves multiple brief discussions over time. This article outlines common benefits and barriers to ACP in primary care, and provides practical approaches to integrating key ACP components into primary care for older adults. Opportunities for multidisciplinary teams to incorporate ACP into brief clinic visits are highlighted.",success
33002482,False,Journal Article,,,,,,,,True,"The objective of this study was to determine how initial intensive care unit triage decisions impact processes of care and outcomes for emergency department patients hospitalized with cardiogenic shock. Individuals with cardiogenic shock were stratified based upon whether they were initially admitted to a cardiac versus noncardiovascular intensive care setting. Those initially triaged to a noncardiovascular intensive care unit were less likely to receive potentially life-saving interventions, including percutaneous coronary intervention and temporary mechanical circulatory support, and were more likely to see significant delays in these interventions if ultimately used. Additionally, admitting cardiogenic shock patients to noncardiovascular intensive care units may result in worse survival. These findings underscore the importance of appropriate identification and triage of emergency department patients with cardiogenic shock-a potentially critical contribution of contemporary cardiogenic shock teams.",success
30858119,False,Journal Article,,,,,,,,True,"The pulmonary artery catheter (PAC) has been used in a wide range of critically ill patients. It is not indicated for routine care of heart failure (HF), but its role in cardiogenic shock (CS) has not been clarified. We conducted a retrospective cohort study with the use of the National Inpatient Sample and identified a total of 9,431,944 adult patients admitted from 2004 to 2014 with the primary diagnosis of HF (n = 8,516,528) or who developed CS (n = 915,416) during the index hospitalization. Overall, patients with PAC had increased hospital costs, length of stay, and mechanical circulatory support use. In patients with HF, PAC use was associated with higher mortality (9.9% vs 3.3%, OR 3.96; P < .001) but the excess of mortality declined over time. In those with CS, PAC was associated with lower mortality (35.1% vs 39.2%, OR 0.91; P < .001) and in-hospital cardiac arrest (14.9% vs 18.3%, OR 0.77; P < .001); this paradox persisted after propensity score matching. The use of PAC in CS has decreased from 2004 to 2014, although its use is now associated with improved outcomes, which may reflect better selection of patients or better use of the information to guide therapies. Our data provide reassurance that PAC use in this population is an appropriate strategy.",success
37187230,True,"Observational Study;Multicenter Study;Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"Pulmonary artery catheters (PACs) are increasingly used to guide management decisions in cardiogenic shock (CS). The goal of this study was to determine if PAC use was associated with a lower risk of in-hospital mortality in CS owing to acute heart failure (HF-CS). This multicenter, retrospective, observational study included patients with CS hospitalized between 2019 and 2021 at 15 US hospitals participating in the Cardiogenic Shock Working Group registry. The primary end point was in-hospital mortality. Inverse probability of treatment-weighted logistic regression models were used to estimate odds ratios (ORs) and corresponding 95% confidence intervals (CI), accounting for multiple variables at admission. The association between the timing of PAC placement and in-hospital death was also analyzed. A total of 1055 patients with HF-CS were included, of whom 834 (79%) received a PAC during their hospitalization. In-hospital mortality risk for the cohort was 24.7% (n = 261). PAC use was associated with lower adjusted in-hospital mortality risk (22.2% vs 29.8%, OR 0.68, 95% CI 0.50-0.94). Similar associations were found across SCAI stages of shock, both at admission and at maximum SCAI stage during hospitalization. Early PAC use (≤6 hours of admission) was observed in 220 PAC recipients (26%) and associated with a lower adjusted risk of in-hospital mortality compared with delayed (≥48 hours) or no PAC use (17.3% vs 27.7%, OR 0.54, 95% CI 0.37-0.81). This observational study supports PAC use, because it was associated with decreased in-hospital mortality in HF-CS, especially if performed within 6 hours of hospital admission. An observational study from the Cardiogenic Shock Working Group registry of 1055 patients with HF-CS showed that pulmonary artery catheter (PAC) use was associated with a lower adjusted in-hospital mortality risk (22.2% vs 29.8%, odds ratio 0.68, 95% confidence interval 0.50-0.94) compared with outcomes in patients managed without PAC. Early PAC use (≤6 hours of admission) was associated with a lower adjusted risk of in-hospital mortality compared with delayed (≥48 hours) or no PAC use (17.3% vs 27.7%, odds ratio 0.54, 95% confidence interval 0.37-0.81).",success
30236315,False,"Journal Article;Research Support, Non-U.S. Gov't;Review",,,,,,,,True,"Contemporary cardiac intensive care units (CICUs) provide care for an aging and increasingly complex patient population. The medical complexity of this population is partly driven by an increased proportion of patients with respiratory failure needing noninvasive or invasive positive pressure ventilation (PPV). PPV often plays an important role in the management of patients with cardiogenic pulmonary edema, cardiogenic shock, or cardiac arrest, and those undergoing mechanical circulatory support. Noninvasive PPV, when appropriately applied to selected patients, may reduce the need for invasive mechanical PPV and improve survival. Invasive PPV can be lifesaving, but has both favorable and unfavorable interactions with left and right ventricular physiology and carries a risk of complications that influence CICU mortality. Effective implementation of PPV requires an understanding of the underlying cardiac and pulmonary pathophysiology. Cardiologists who practice in the CICU should be proficient with the indications, appropriate selection, potential cardiopulmonary interactions, and complications of PPV.",success
11790214,True,"Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, Non-P.H.S.",,,,,,,,True,"The outcome of patients receiving mechanical ventilation for particular indications has been studied, but the outcome in a large number of unselected, heterogeneous patients has not been reported. To determine the survival of patients receiving mechanical ventilation and the relative importance of factors influencing survival. Prospective cohort of consecutive adult patients admitted to 361 intensive care units who received mechanical ventilation for more than 12 hours between March 1, 1998, and March 31, 1998. Data were collected on each patient at initiation of mechanical ventilation and daily throughout the course of mechanical ventilation for up to 28 days. All-cause mortality during intensive care unit stay. Of the 15 757 patients admitted, a total of 5183 (33%) received mechanical ventilation for a mean (SD) duration of 5.9 (7.2) days. The mean (SD) length of stay in the intensive care unit was 11.2 (13.7) days. Overall mortality rate in the intensive care unit was 30.7% (1590 patients) for the entire population, 52% (120) in patients who received ventilation because of acute respiratory distress syndrome, and 22% (115) in patients who received ventilation for an exacerbation of chronic obstructive pulmonary disease. Survival of unselected patients receiving mechanical ventilation for more than 12 hours was 69%. The main conditions independently associated with increased mortality were (1) factors present at the start of mechanical ventilation (odds ratio [OR], 2.98; 95% confidence interval [CI], 2.44-3.63; P<.001 for coma), (2) factors related to patient management (OR, 3.67; 95% CI, 2.02-6.66; P<.001 for plateau airway pressure >35 cm H(2)O), and (3) developments occurring over the course of mechanical ventilation (OR, 8.71; 95% CI, 5.44-13.94; P<.001 for ratio of PaO(2) to fraction of inspired oxygen <100). Survival among mechanically ventilated patients depends not only on the factors present at the start of mechanical ventilation, but also on the development of complications and patient management in the intensive care unit.",success
19736189,False,Journal Article,,,,,,,,True,"Age and duration of mechanical ventilation (MV) are strongly associated with mortality and hospital discharge disposition. Electronic administrative records from a 425-bed community teaching hospital were obtained for 9,912 patients who were admitted to hospital ICUs between 2003 and 2008. Risk estimates of age and duration of MV for in-hospital mortality and discharge to home vs extended-care facilities (ECFs) also were obtained. Of 9,912 patients, 37 were discharged to hospice care, and 668 were < 18 years of age. Of the remaining 9,207 patients, 4,238 received invasive MV. Mortality or hospital discharge to ECFs increased consistently for each decade of age > 65 years and as the duration of MV increased. Although only 11.7% of patients < 65 years age who received MV for 1 or 2 days died during hospitalization, the mortality rate increased to 72.1% for patients > 85 years of age who had received MV for > 7 days. For patients requiring MV for >or= 7 days, < 10% of the >or= 65 years of age and < 5% of patients >or= 85 years of age survived to be discharged home from the hospital. Multivariate logistic regression analyses showed that age > 65 years and duration of MV remained significantly associated with outcomes, even after adjustment for hospital discharge diagnoses (Charlson scores). This study suggests that age and duration of MV are strongly associated with mortality and posthospital disposition. If confirmed, the simple combination of age and duration of MV provides prognostic information that could be used with trajectory of illness and in the context of patients' values to inform end-of-life discussions with patients or their surrogates during a trial of critical care.",success
32006910,True,"Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"The prevalence of renal disease in cardiac intensive care units (CICUs) is increasing, but little is known about the utilization, concurrent therapies, and outcomes of patients requiring acute renal replacement therapy (RRT) in this specialized environment. In the Critical Care Cardiology Trials Network, 16 centers submitted data on CICU admissions including acute RRT (defined as continuous renal replacement therapy and/or acute intermittent dialysis). Among 2,985 admissions, 178 (6.0%; interhospital range 1.0%-16.0%) received acute RRT. Patients receiving RRT, versus not, were more commonly admitted for cardiogenic shock (15.7% vs 4.2%, P < .01), cardiac arrest (9.6% vs 3.7%, P < .01), and acute general medical diagnoses (10.7% vs 5.8%, P < .01), whereas acute coronary syndromes (16.9% vs 32.1%, P < .01) were less frequent. Variables independently associated with acute RRT included diabetes, heart failure, liver disease, severe valvular disease, shock, cardiac arrest, hypertension, and younger age. In patients receiving acute RRT, versus not, advanced therapies including mechanical ventilation (55.6% vs 18.0%), vasoactive support (73.0% vs 35.2%), invasive hemodynamic monitoring (59.6% vs 29.2%), and mechanical circulatory support (27.5% vs 8.4%) were more common. Acute RRT was associated with higher in-hospital mortality (42.1% vs 9.3%, adjusted odds ratio 3.74, 95% CI, 2.52-5.53) and longer median length of stay (10.0 vs 5.3 days, P < .01). In conclusion, acute RRT in contemporary CICUs was associated with the provision of other advanced therapies and lower survival. These data underscore the risks associated with the provision of renal support in patients with primary cardiovascular problems and the need to develop standardized indications and potential futility measures in this specialized population.",success
19561951,False,Journal Article,,,,,,,,True,"The incidence of acute renal failure (ARF) in the hospital setting is increasing. It portends excessive morbidity and mortality and a considerable burden on hospital resources. Extracorporeal therapies show promise in the management of patients with shock and ARF. It is said that the potential of such therapy goes beyond just providing renal support. The aim of our study was to analyze the clinical setting and outcomes of critically ill ARF patients managed with continuous renal replacement therapy (CRRT). Ours was a retrospective study of 50 patients treated between January 2004 and November 2005. These 50 patients were in clinical shock and had concomitant ARF. All of these patients underwent CVVHDF (continuous veno-venous hemodiafiltration) in the intensive care unit. For the purpose of this study, shock was defined as systolic BP < 100 mm Hg in spite of administration of one or more inotropic agents. SOFA (Sequential Organ Failure Assessment) score before initiation of dialysis support was recorded in all cases. CVVHDF was performed using the Diapact((R)) (Braun) CRRT machine. The vascular access used was as follows: femoral in 32, internal jugular in 8, arteriovenous fistula (AVF) in 4, and subclavian in 6 patients. We used 0.9% or 0.45% (half-normal) saline as a prefilter replacement, with addition of 10% calcium gluconate, magnesium sulphate, sodium bicarbonate, and potassium chloride in separate units, while maintaining careful monitoring of electrolytes. Anticoagulation of the extracorporeal circuit was achieved with systemic heparin in 26 patients; frequent saline flushes were used in the other 24 patients. Of the 50 patients studied, 29 were males and 21 females (1.4:1). The average age was 52.88 years (range: 20-75 years). Causes of ARF included sepsis in 24 (48%), hemodynamically mediated renal failure (HMRF) in 18 (36%), and acute over chronic kidney disease in 8 (16%) patients. The overall mortality was 74%. The average SOFA score was 14.31. The variables influencing mortality on multivariate analysis were: age [odds ratio (OR):1.65; 95% CI: 1.35 to 1.92; P = 0.04], serum creatinine (OR:1.68; 95% CI: 1.44 to 1.86; P = 0.03), and serum bicarbonate (OR: 0.76; 95% CI: 0.55 to 0.94; P = 0.01). On univariate analysis the SOFA score was found to be a useful predictor of mortality. Despite advances in treating critically ill patients with newer extracorporeal therapies, mortality is dismally high. Multiorgan dysfunction adversely affects outcome of CRRT. Older age, level of azotemia, and severity of metabolic acidosis are important predictors of adverse outcome.",success
31037106,False,Journal Article,,,,,,,,True,"Elderly patients are frequently considered poor candidates for continuous renal replacement therapy in intensive care units, but with little evidence base. We gathered data regarding patients requiring continuous renal replacement therapy at our institution's intensive care unit during 2012-2014, and compared outcomes between patients of 75 years or older, and younger patients. Older patients had similar intensive care unit mortality to younger patients (41.5% vs. 36.1%, <i>p</i> = 0.21), but higher hospital mortality (54.2% vs. 44.0%, <i>p</i> = 0.02), and one-year mortality (63.6% vs. 50.6%, <i>p</i> = 0.005). There were no significant differences in dialysis-dependence rates between older and younger patients at intensive care unit discharge (31.9% vs. 35.8%, <i>p</i> = 0.50), and hospital discharge (18.5% vs. 24.2%, 0.32). Rates of new dialysis-dependence between older and younger patients at time of hospital discharge were similar (10.2% vs. 6.0%, <i>p</i> = 0.20). Intensivists should not withhold continuous renal replacement therapy based on age alone. Other factors should be considered in triage of patients for intensive care unit and continuous renal replacement therapy.",success
28629462,False,Journal Article;Review,,,,,,,,True,"Dialysis initiation rates among older adults, aged 75 years or greater, are increasing at a faster rate than for younger age groups. Older adults with advanced CKD (eGFR < 30 ml/min/1.73 m<sup>2</sup>) typically lose renal function slowly, often suffer from significant comorbidity and thus may die from associated comorbidities before they require dialysis.A patient's pattern of renal function loss over time in relation to their underlying comorbidities can serve as a guide to the probability of a future dialysis requirement. Most who start dialysis, initiate treatment ""early"", at an estimated glomerulofiltration rate (eGFR) >10 ml/min/1.73 m<sup>2</sup> and many initiate dialysis in hospital, often in association with an episode of acute renal failure. In the US older adults start dialysis at a mean e GFR of 12.6 ml/min/1.73 m<sup>2</sup> and 20.6% die within six months of dialysis initiation. In both the acute in hospital and outpatient settings, many older adults appear to be initiating dialysis for non-specific, non-life threatening symptoms and clinical contexts. Observational data suggests that dialysis does not provide a survival benefit for older adults with poor mobility and high levels of comorbidity. To optimize the care of this population, early and repeat shared decision making conversations by health care providers, patients, and their families should consider the risks, burdens, and benefits of dialysis versus conservative management, as well as the patient specific symptoms and clinical situations that could justify dialysis initiation. The potential advantages and disadvantages of dialysis therapy should be considered in conjunction with each patient's unique goals and priorities.In conclusion, when considering the morbidity and quality of life impact associated with dialysis, many older adults may prefer to delay dialysis until there is a definitive indication or may opt for conservative management without dialysis. This approach can incorporate all CKD treatments other than dialysis, provide psychosocial and spiritual support and active symptom management and may also incorporate a palliative care approach with less medical monitoring of lab parameters and more focus on the use of drug therapies directed to relief of a patient's symptoms.",success
12804733,False,Comparative Study;Journal Article,,,,,,,,False,,success
15199357,False,Journal Article,,,,,,,,True,"Subgroup analysis from the Should We Emergently Revascularize Occluded Coronaries for Cardiogenic Shock (SHOCK) trial indicated that patients with acute myocardial infarction (MI) complicated by cardiogenic shock (CS) who were > or =75 years old did not benefit from early revascularization and may have been harmed; their mortality rate at 30 days was 75%. The applicability of this subset analysis from a select patient population enrolled in a randomized trial to the general population is unclear. At the Mayo Clinic between 1991 and 2000, we evaluated the outcome of all patients > or =75 years old with CS caused by MI who underwent urgent percutaneous coronary intervention (PCI). The study included 61 patients with a mean age of 79.5 +/- 3 years; 21% of these patients had a history of prior coronary artery bypass grafting (CABG), 41% had had a prior MI, 28% had diabetes mellitus, and 18% had a history of a cerebrovascular accident (CVA). PCI was performed 8.0 +/- 7.2 hours after onset of MI. Angiographic success (<50% residual stenosis) was achieved in 91% of the lesions that were dilated. In hospital outcomes included death (44%), CABG (1.6%), and CVA (4.9%). The 30-day mortality rate was 47%. The estimated survival rate 1 year after discharge (Kaplan Meier method) was 75%. These data confirm a high early mortality rate among patients > or =75 years old with MI complicated by CS, but suggest that among patients referred for angiography, outcomes may be better than previously believed when early revascularization is performed. In this population, 56% of patients survived to be discharged from the hospital, and of the hospital survivors, 75% were alive at 1 year.",success
15976798,True,"Comparative Study;Journal Article;Randomized Controlled Trial;Research Support, N.I.H., Extramural;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"In the SHOCK trial, the group of patients aged >or=75 years did not appear to derive the mortality benefit from early revascularization (ERV) versus initial medical stabilization (IMS) that was seen in patients aged <75 years. We sought to determine the reason for this finding by examining the baseline characteristics and outcomes of the 2 treatment groups by age. Patients with cardiogenic shock (CS) secondary to left ventricular (LV) failure were randomized to ERV within 6 hours or to a period of IMS. We compared the characteristics by treatment group of patients aged >or=75 years and of their younger counterparts. Of the 56 enrolled patients aged >or=75 years, those assigned to ERV had lower LV ejection fraction at baseline than IMS-assigned patients (27.5% +/- 12.7% vs 35.6% +/- 11.6%, P = .051). In the elderly ERV and IMS groups, 54.2% and 31.3%, respectively, were women ( P = .105) and 62.5% and 40.6%, respectively, had an anterior infarction (P = .177). The 30-day mortality rate in the ERV group was 75.0% in patients aged >or=75 years and 41.4% in those aged <75 years. In the IMS group, 30-day mortality was 53.1% for those aged >or=75 years, similar to the 56.8% for patients aged <75 years. Overall, the elderly randomized to ERV did not have better survival than elderly IMS patients. Despite the strong association of age and death post-CS, elderly patients assigned to IMS had a 30-day mortality rate similar to that of IMS patients aged <75 years, suggesting that this was a lower-risk group with more favorable baseline characteristics. The lack of apparent benefit from ERV in elderly patients in the SHOCK trial may thus be due to differences in important baseline characteristics, specifically LV function, and play of chance arising from the small sample size. Therefore, the SHOCK trial overall finding of a 12-month survival benefit for ERV should be viewed as applicable to all patients, including those >or=75 years of age, with acute myocardial infarction complicated by CS.",success
16186436,True,"Journal Article;Randomized Controlled Trial;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"The Should We Emergently Revascularize Occluded Coronaries for Cardiogenic Shock (SHOCK) trial demonstrated the survival advantage of emergency revascularization versus initial medical stabilization in patients developing cardiogenic shock after acute myocardial infarction. The relative merits of coronary artery bypass grafting (CABG) versus percutaneous coronary intervention (PCI) in patients with shock have not been defined. The objective of this analysis was to compare the effects of PCI and CABG on 30-day and 1-year survival in the SHOCK trial. Of the 302 trial patients, 128 with predominant left ventricular failure had emergency revascularization. The selection of revascularization procedures was individualized. Eighty-one patients (63.3%) had PCI, and 47 (36.7%) had CABG. The median time from randomization to intervention was 0.9 hours (interquartile range [IQR], 0.3 to 2.2 hours) for PCI and 2.7 hours (IQR, 1.3 to 5.5 hours) for CABG. Baseline demographics and hemodynamics were similar, except that there were more diabetics (48.9% versus 26.9%; P=0.02), 3-vessel disease (80.4% versus 60.3%; P=0.03), and left main coronary disease (41.3% versus 13.0%; P=0.001) in the CABG group. In the PCI group, 12.3% had 2-vessel and 2.5% had 3-vessel interventions. In the CABG group, 84.8% received > or =2 grafts, 52.2% received > or =3 grafts, and 87.2% were deemed completely revascularized. The survival rates were 55.6% in the PCI group compared with 57.4% in the CABG group at 30 days (P=0.86) and 51.9% compared with 46.8%, respectively, at 1 year (P=0.71). Among SHOCK trial patients randomized to emergency revascularization, those treated with CABG had a greater prevalence of diabetes and worse coronary disease than those treated with PCI. However, survival rates were similar. Emergency CABG is an important component of an optimal treatment strategy in patients with cardiogenic shock, and should be considered a complementary treatment option in patients with extensive coronary disease.",success
17070154,False,Journal Article,,,,,,,,True,"Age is a strong predictor of cardiogenic shock (CS) and death in patients with acute myocardial infarction (AMI). Few data on the impact of a routine early percutaneous revascularization strategy in elderly patients with CS complicating AMI exist. We performed an analysis of age-related differences in outcome in 280 consecutive patients with AMI complicated by CS who underwent primary percutaneous coronary intervention (PCI) between January 1995 and September 2004 and who were included in a single-center prospective registry of primary PCI for AMI. Of the 280 patients with CS, 104 (37%) were > or = 75 years. The mean age of the elderly group was 81 +/- 5 years, and half of the patients were > or = 80 years. Most patients in both groups underwent PCI within 6 hours of their symptom onset. The PCI success rates were 92% in the elderly group and 97% in the younger patient group (P = .062). The 6-month mortality rates were 56% in the elderly group and 26% in the younger patient group (P < .001). At multivariate analysis, the variables independently related to the risk of 1-year mortality in the elderly group were age (hazard ratio 1.07, 95% CI 1.02-1.12, P = .005) and PCI failure (hazard ratio 4.01, 95% CI 1.53-10.51, P = .005). A strategy of routine emergency PCI in elderly patients with CS complicating AMI is highly feasible. Among elderly patients, age remains to be a strong predictor of mortality. However, outcome after successful PCI is better than previously reported.",success
19463417,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"We sought to assess clinical outcomes of elderly patients (age >or=75 years) undergoing percutaneous coronary intervention (PCI) for acute myocardial infarction (MI) complicated by cardiogenic shock (CS) in a contemporary multicenter PCI registry. Although benefits of early PCI have been shown in younger groups, few studies have reported on clinical outcomes in elderly shock patients using current PCI techniques. We analyzed baseline characteristics and procedural and clinical outcomes in 143 consecutive patients presenting with MI and CS who underwent PCI from the Melbourne Interventional Group registry between 2004 and 2007. Of the 143 patients, 31.5% (n = 45) were elderly and 68.5% were younger (age <75 years). Elderly patients were more likely to be female (46.7% vs. 22.4%, p < 0.01) and have hypertension (77.8% vs. 46.4%, p < 0.01), previous MI (31.1% vs. 15.5%, p = 0.03), renal failure (24.4% vs. 11.3%, p < 0.05) and multivessel coronary artery disease (93.1% vs. 68.3%, p < 0.01). Stent (86.7% vs. 94.8%, p = 0.09), glycoprotein IIb/IIIa inhibitor (68.9% vs. 65.3%, p = 0.67), and intra-aortic balloon pump (57.8% vs. 58.2%, p = 0.97) use were similar in both groups. In-hospital, 30-day, and 1-year mortality in the elderly group versus the younger group were 42.2% vs. 33.7% (p = 0.32), 43.2% vs. 36.1% (p = 0.42), and 52.6% vs. 46.8% (p = 0.56), respectively. In this study, the 1-year survival of elderly patients with acute MI complicated by CS undergoing PCI was comparable to younger patients. These data suggest that in elderly patients presenting with CS, benefit is possible with selective use of early revascularization and merits further investigation.",success
22920912,True,"Journal Article;Multicenter Study;Randomized Controlled Trial;Research Support, Non-U.S. Gov't",NCT00491036,databank,NCT00491036,NCT00491036,NCT00491036,NCT00491036|databank,NCT00491036|databank,True,"In current international guidelines, intraaortic balloon counterpulsation is considered to be a class I treatment for cardiogenic shock complicating acute myocardial infarction. However, evidence is based mainly on registry data, and there is a paucity of randomized clinical trials. In this randomized, prospective, open-label, multicenter trial, we randomly assigned 600 patients with cardiogenic shock complicating acute myocardial infarction to intraaortic balloon counterpulsation (IABP group, 301 patients) or no intraaortic balloon counterpulsation (control group, 299 patients). All patients were expected to undergo early revascularization (by means of percutaneous coronary intervention or bypass surgery) and to receive the best available medical therapy. The primary efficacy end point was 30-day all-cause mortality. Safety assessments included major bleeding, peripheral ischemic complications, sepsis, and stroke. A total of 300 patients in the IABP group and 298 in the control group were included in the analysis of the primary end point. At 30 days, 119 patients in the IABP group (39.7%) and 123 patients in the control group (41.3%) had died (relative risk with IABP, 0.96; 95% confidence interval, 0.79 to 1.17; P=0.69). There were no significant differences in secondary end points or in process-of-care measures, including the time to hemodynamic stabilization, the length of stay in the intensive care unit, serum lactate levels, the dose and duration of catecholamine therapy, and renal function. The IABP group and the control group did not differ significantly with respect to the rates of major bleeding (3.3% and 4.4%, respectively; P=0.51), peripheral ischemic complications (4.3% and 3.4%, P=0.53), sepsis (15.7% and 20.5%, P=0.15), and stroke (0.7% and 1.7%, P=0.28). The use of intraaortic balloon counterpulsation did not significantly reduce 30-day mortality in patients with cardiogenic shock complicating acute myocardial infarction for whom an early revascularization strategy was planned. (Funded by the German Research Foundation and others; IABP-SHOCK II ClinicalTrials.gov number, NCT00491036.).",success
34265831,False,Journal Article,,,,,,,,True,"Despite advances in treatment of patients with cardiogenic shock following acute myocardial infarction (AMICS) in-hospital mortality remains around 50%. Outcome varies among patient subsets and the elderly often have a poor a priori prognosis. We sought to investigate outcome among elderly AMICS patients referred to evaluation and treatment at a tertiary university hospital. Current analysis was based on the RETROSHOCK registry comprising consecutive AMICS patients admitted to tertiary care. Patients in the registry were individually identified and validated. Of 1,716 admitted patients, 496 (28.9%) patients were ≥75 years old. Older patients were less likely to be admitted directly to a tertiary centre (59.4% vs. 69.9%, P = 0.003), receive mechanical support devices (i.e., Impella® (8.9% vs. 15.0%, P = 0.003), and undergo revascularization attempt (76.8% vs. 90.2%, P < 0.001). Thirty-day survivors ≥75 years were characterized by having higher left ventricular ejection fraction (30.2% ± 12.5% vs. 26.5% ± 11.8%, P = 0.004) and lower arterial lactate (3.2[2.2-5.2] mmol/L vs. 5.5[3.3-8.2] mmol/L, P < 0.001) at admission. In a multivariable analysis of patients ≥75 years, higher age (HR 1.09, 95% CI 1.05-1.14, P < 0.001), higher heart rate (HR 1.01, 95% CI 1.001-1.014, P = 0.03), and higher lactate (HR 1.11, 95% CI 1.07-1.16, P < 0.001) at admission were associated with an increased risk of 30-day mortality. Among patients ≥75 years with AMICS referred for tertiary specialized treatment, 30-day mortality was 73.4%. Survivors were characterized by lower arterial lactate and heart rate at admission.",success
31973608,False,Journal Article;Review,,,,,,,,False,,success
37705890,False,Journal Article,,,,,,,,True,"Older adults with cardiovascular disease (CVD) contend with deficits across multiple domains of health due to age-related physiological changes and the impact of CVD. Multimorbidity, polypharmacy, cognitive changes, and diminished functional capacity, along with changes in the social environment, result in complexity that makes provision of CVD care to older adults challenging. In this review, we first describe the history of geriatric cardiology, an orientation that acknowledges the unique needs of older adults with CVD. Then, we introduce 5 essential principles for meeting the needs of older adults with CVD: 1) recognize and consider the potential impact of multicomplexity; 2) evaluate and integrate constructs of cognition into decision-making; 3) evaluate and integrate physical function into decision-making; 4) incorporate social environmental factors into management decisions; and 5) elicit patient priorities and health goals and align with care plan. Finally, we review future steps to maximize care provision to this growing population.",success
36265932,False,"Journal Article;Review;Research Support, Non-U.S. Gov't;Research Support, N.I.H., Extramural;Consensus Development Conference",,,,,,,,True,"In the United States, the frequency of using percutaneous mechanical circulatory support devices for acute myocardial infarction complicated by cardiogenic shock is increasing. These devices require large-bore vascular access to provide left, right, or biventricular cardiac support, frequently under urgent/emergent circumstances. Significant technical and logistical variability exists in device insertion, care, and removal in the cardiac catheterization laboratory and in the cardiac intensive care unit. This variability in practice may contribute to adverse outcomes observed in centers that receive patients with cardiogenic shock, who are at higher risk for circulatory insufficiency, venous stasis, bleeding, and arterial hypoperfusion. In this position statement, we aim to: 1) describe the public health impact of bleeding and vascular complications in cardiogenic shock; 2) highlight knowledge gaps for vascular safety and provide a roadmap for a regulatory perspective necessary for advancing the field; 3) propose a minimum core set of process elements, or ""vascular safety bundle""; and 4) develop a possible study design for a pragmatic trial platform to evaluate which structured approach to vascular access drives most benefit and prevents vascular and bleeding complications in practice.",success
18250266,False,"Evaluation Study;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"There exist few studies that characterize contemporary clinical features and outcomes or risk factors for operative mortality in cardiogenic shock (CS) patients undergoing coronary artery bypass grafting (CABG). We evaluated data of 708,593 patients with and without CS undergoing CABG enrolled in the Society of Thoracic Surgeons National Cardiac Database (2002-2005). Clinical, angiographic, and operative features and in-hospital outcomes were evaluated in patients with and without CS. Logistic regression was used to identify predictors of operative mortality and to estimate weights for an additive risk score. Patients with preoperative CS constituted 14,956 (2.1%) of patients undergoing CABG yet accounted for 14% of all CABG deaths. Operative mortality in CS patients was high and surgery specific, rising from 20% for isolated CABG to 33% for CABG plus valve surgery and 58% for CABG plus ventricular septal repair. Although mortality for CABG surgery overall declined significantly over time (P for trend <0.0001), mortality for CS patients undergoing CABG did not change significantly during the 4-year study period (P=0.07). Factors associated with higher death risk for CS patients undergoing CABG were identified by multivariable analysis and summarized into a simple bedside risk score (c statistic=0.74) that accurately stratified those with low (<10%) to very high (>60%) mortality risk. Patients with CS represent a minority of those undergoing CABG yet have persistently high operative risks, accounting for 14% of deaths in CABG patients. Estimation of patient-specific risk of mortality is feasible with the simplified additive risk tool developed in our study with the use of routinely available preprocedural data.",success
16046651,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Early mechanical revascularization in patients with acute myocardial infarction (AMI) complicated by cardiogenic shock is a therapeutic strategy that reduces mortality. It has been a class I recommendation in guidelines from the American College of Cardiology and the American Heart Association since 1999 for patients younger than 75 years. However, little is known about implementation of these guidelines in practice. To assess trends in early revascularization and mortality for patients with cardiogenic shock complicating AMI and to determine whether the national guidelines affect revascularization rates. Prospective, observational study of 293,633 patients with ST-elevation myocardial infarction (25,311 [8.6%] had cardiogenic shock; 7356 [29%] had cardiogenic shock at hospital presentation) enrolled in the National Registry of Myocardial Infarction (NRMI) from January 1995 to May 2004 at 775 US hospitals with revascularization capability (defined as the capability to perform cardiac catheterization, percutaneous coronary intervention [PCI], and open-heart surgery). Management patterns and in-hospital mortality rates. There was an increase in primary PCI rates from 27.4% to 54.4% (P<.001) in hospitals with revascularization capability that paralleled the change in PCI for ST-elevation myocardial infarction. There was no significant change in rates of immediate coronary artery bypass graft surgery (from 2.1% to 3.2%). Propensity-adjusted multivariable analyses demonstrated that primary PCI was associated with a decreased odds of death during hospitalization (odds ratio, 0.46; 95% confidence interval, 0.40-0.53). There were no differences in the rates of change in revascularization rates based on the date when the guidelines were released regardless of patient age. Overall in-hospital cardiogenic shock mortality decreased from 60.3% in 1995 to 47.9% in 2004 (P<.001). The use of PCI for patients with cardiogenic shock was associated with improved survival in a large group of hospitals with revascularization capability. The American College of Cardiology and American Heart Association guidelines had no detectable temporal impact on revascularization rates. These findings support the need for increased adherence to these guidelines.",success
26718859,True,Journal Article;Multicenter Study,,,,,,,,True,"Acute myocardial infarction complicated by cardiogenic shock (AMI-CS) is associated with substantial mortality. We evaluated outcomes of patients in The Society of Thoracic Surgeons Adult Cardiac Surgery Database who underwent coronary artery bypass graft surgery (CABG) in the setting of AMI-CS. All patients with AMI-CS who underwent nonelective CABG or CABG with ventricular assist device implantation within 7 days after myocardial infarction were enrolled. The primary analysis sample consisted of patients who underwent surgery between June 2011 and December 2013. Baseline characteristics, operative findings, outcomes, and the utilization of mechanical circulatory support (MCS) were assessed in detail in this population. We also evaluated trends in unadjusted mortality for all patients undergoing CABG or CABG with ventricular assist device for AMI-CS from January 2005 to December 2013. A total of 5,496 patients met study criteria, comprising 1.5% of all patients undergoing CABG during the study period. Overall operative mortality was 18.7%, decreasing from 19.3% in 2005 to 18.1% in 2013 (p < 0.001). Use of MCS increased from 5.8% in 2011 to 8.8% in 2013 (p = 0.008). Patients receiving MCS had a high proportion of cardiovascular risk factors or high clinical acuity. Patients requiring preoperative and patients requiring intraoperative or postoperative MCS had operative mortality of 37.2% and 58.4%, respectively. Patients undergoing CABG as a salvage procedure had an operative mortality of 53.3%, and a high incidence of reoperation (21.8%), postoperative respiratory failure requiring prolonged ventilation (59.7%), and renal failure (18.5%). Most patients undergoing CABG for AMI-CS have a sizeable but not prohibitive risk. Patients who require MCS and those undergoing operation as a salvage procedure reflect higher risk populations.",success
26303635,True,Journal Article;Multicenter Study,,,,,,,,True,"Salvage coronary artery bypass grafting (CABG) is often performed for cardiogenic shock on compassionate basis without clinical data justifying this aggressive approach. The aim of this study was to analyze early and intermediate outcomes after salvage CABG. We retrospectively reviewed the data of 85 patients who underwent salvage CABG at 11 European cardiac surgery centers. Salvage CABG was defined according to the EuroSCORE criteria, that is, a procedure performed in patients requiring cardiopulmonary resuscitation (external cardiac massage) en route to the operating theater or before induction of anesthesia. A percutaneous coronary intervention procedure preceded salvage CABG in 55 patients (64.7%). Thirty patients (35.3%) died during the inhospital stay. The mean EuroSCORE II was 32.0% and the observed-to-expected ratio was 1.08. Salvage CABG was associated with high rates of postoperative stroke (9.4%), resternotomy for bleeding (23.5%), resternotomy for hemodynamic instability (15.3%), dialysis (18.8%), severe gastrointestinal complications (12.9%), and deep sternal wound infection (10.6%). Survival at 1, 3, and 5 years was 58.6%, 49.8%, and 40.9%, respectively. Twenty patients (23.5%) were postoperatively treated with extracorporeal membrane oxygenation (ECMO). The rates of adverse events after ECMO were particularly high (stroke 40%, resternotomy for bleeding 60%, dialysis 35%, gastrointestinal complications 30%, and deep sternal wound infection 30%). Of patients treated with ECMO, 8 (40%) survived to discharge, and 1-year survival was 29.2%. Salvage CABG is associated with high risk of immediate mortality and severe adverse events. However, the observed immediate and intermediate outcome justify coronary surgery in these critically ill patients. A number of these patients are currently treated by ECMO, and its results are encouraging.",success
29929641,False,"Comparative Study;Journal Article;Observational Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"The authors sought to examine outcomes and identify independent predictors of mortality among patients undergoing urgent/emergent transcatheter aortic valve replacement (TAVR). Data on urgent/emergent TAVR as a rescue therapy for decompensated severe aortic stenosis (AS) are limited. The Society of Thoracic Surgeons and the American College of Cardiology Transcatheter Valve Therapy (STS/ACC TVT) Registry linked with Centers for Medicare & Medicaid Services claims was used to identify patients who underwent urgent/emergent versus elective TAVR between November 2011 and June 2016. Outcomes assessed were device success rate, in-hospital major adverse events, and 30-day and 1-year mortality. Independent predictors of mortality after urgent/emergent TAVR were examined. Of 40,042 patients who underwent TAVR, 3,952 (9.9%) were urgent/emergent (median STS PROM score 11.8 [interquartile range: 7.6 to 17.9]). Device success rate was statistically lower, but not clinically different after urgent/emergent versus elective TAVR (92.6% vs. 93.7%; p = 0.007). Rates of major and/or life-threatening bleeding, major vascular complications, myocardial infarction, stroke, new permanent pacemaker placement, conversion to SAVR, and paravalvular regurgitation were similar between the 2 groups. Compared with elective TAVR, patients undergoing urgent/emergent TAVR had higher rates of acute kidney injury and/or new dialysis (8.2% vs. 4.2%; p < 0.001), 30-day mortality (8.7% vs. 4.3%, adjusted hazard ratio: 1.28, 95% confidence interval: 1.10 to 1.48), and 1-year mortality (29.1% vs. 17.5%, adjusted hazard ratio: 1.20, 95% confidence interval: 1.10 to 1.31). In patients undergoing urgent/emergent TAVR, non-femoral access and cardiopulmonary bypass were associated with increased risk, whereas use of balloon-expandable valve was associated with decreased risk of 30-day and 1-year mortality. Urgent/emergent TAVR is feasible with acceptable outcomes and may be a reasonable option in a selected group of patients with severe AS.",success
37245143,False,Practice Guideline;Journal Article,,,,,,,,False,,success
36074476,True,Comparative Study;Journal Article;Multicenter Study;Observational Study;Randomized Controlled Trial,NCT03982979,databank,NCT03982979;NCT02224755,NCT03982979;NCT02224755,NCT03982979;NCT02224755,NCT03982979|databank;NCT02224755|databank,NCT03982979|databank;NCT02224755|databank,True,"Although durable left ventricular assist device (LVAD) therapy has emerged as an important treatment option for patients with advanced heart failure refractory to pharmacological support, outcomes, including survival, beyond 2 years remain poorly characterized. To report the composite end point of survival to transplant, recovery, or LVAD support free of debilitating stroke (Modified Rankin Scale score >3) or reoperation to replace the pump 5 years after the implant in participants who received the fully magnetically levitated centrifugal-flow HeartMate 3 or axial-flow HeartMate II LVAD in the MOMENTUM 3 randomized trial and were still receiving LVAD therapy at the 2-year follow-up. This observational study was a 5-year follow-up of the MOMENTUM 3 trial, conducted in 69 US centers, that demonstrated superiority of the centrifugal-flow LVAD to the axial-flow pump with respect to survival to transplant, recovery, or LVAD support free of debilitating stroke or reoperation to replace the pump at 2 years. A total of 295 patients were enrolled between June 2019 to April 2021 in the extended-phase study, with 5-year follow-up completed in September 2021. Of 1020 patients in the investigational device exemption per-protocol population, 536 were still receiving LVAD support at 2 years, of whom 289 received the centrifugal-flow pump and 247 received the axial-flow pump. There were 10 end points evaluated at 5 years in the per-protocol population, including a composite of survival to transplant, recovery, or LVAD support free of debilitating stroke or reoperation to replace the pump between the centrifugal-flow and axial-flow pump groups and overall survival between the 2 groups. A total of 477 patients (295 enrolled and 182 provided limited data) of 536 patients still receiving LVAD support at 2 years contributed to the extended-phase analysis (median age, 62 y; 86 [18%] women). The 5-year Kaplan-Meier estimate of survival to transplant, recovery, or LVAD support free of debilitating stroke or reoperation to replace the pump in the centrifugal-flow vs axial-flow group was 54.0% vs 29.7% (hazard ratio, 0.55 [95% CI, 0.45-0.67]; P < .001). Overall Kaplan-Meier survival was 58.4% in the centrifugal-flow group vs 43.7% in the axial-flow group (hazard ratio, 0.72 [95% CI, 0.58-0.89]; P = .003). Serious adverse events of stroke, bleeding, and pump thrombosis were less frequent in the centrifugal-flow pump group. In this observational follow-up study of patients from the MOMENTUM 3 randomized trial, per-protocol analyses found that receipt of a fully magnetically levitated centrifugal-flow LVAD vs axial-flow LVAD was associated with a better composite outcome and higher likelihood of overall survival at 5 years. These findings support the use of the fully magnetically levitated LVAD. ClinicalTrials.gov Identifier: NCT02224755 and NCT03982979.",success
29365092,False,Journal Article,,,,,,,,True,"Ethical and health care economic concerns surround the use of venous-arterial extracorporeal membrane oxygenation (VA-ECMO) in elderly patients. Patients requiring VA-ECMO are often in critical condition and the decision to cannulate is time-sensitive. We investigated the relationship between age and VA-ECMO outcomes to better inform this decision. This is a retrospective study of 355 patients placed on VA-ECMO between March 2007 and August 2016 at our institution. Using piecewise modelling, age became associated with in-hospital mortality after 63 years. Based on further analysis with the χ2 statistic maximization, patients were divided into 2 age groups: ≤72 years old [Group Y (Young), n = 310] and >72 years old [Group O (Old), n = 45]. Multivariable logistic regression was performed to identify preoperative predictors of in-hospital mortality. Patients over the age of 72 had a significantly higher prevalence of comorbidities, including coronary disease, previous strokes and chronic kidney disease. Weaning from ECMO was achieved in 76% of Group Y and 47% of Group O (P < 0.001). In-hospital mortality was 52% among Group Y and 69% among Group O (P = 0.037). Multivariable logistic regression using preoperative risk factors identified coronary artery disease, acute decompensated heart failure and an age >72 years as independent predictors of mortality (age >72 years: odds ratio 2.71, 95% confidence interval 1.22-6.00; P = 0.01). VA-ECMO in-hospital mortality is considerable across all age groups. However, age only becomes associated with mortality after 63 years and rises dramatically after 72 years. This study provides useful insight into these time-sensitive decisions for the development of possible practice guidelines.",success
30297066,False,Journal Article;Review,,,,,,,,True,"Severe systolic dysfunction combined with symptoms of heart failure has a high rate of morbidity and mortality. Medical management remains the mainstay of therapy but mechanical circulatory support (MCS) is one of the treatment options for end-stage heart failure. The left ventricular assist device (LVAD) is the most common type of device used for durable MCS. There have been significant improvements in the technology, surgical technique, and patient management. There is not a single model that is able to accurately assess risk in these patients due to the variety of variables. It is essential to assess all risk in the decision-making process.",success
32029401,True,Journal Article;Multicenter Study,,,,,,,,True,"Patients with cardiogenic shock (CS) needing temporary circulatory support (TCS) have poor survival rates after implantation of durable ventricular assist device (dVAD). We aimed to characterize post-dVAD adverse event burden and survival rates in patients requiring pre-operative TCS. We analyzed 13,511 adults (Interagency Registry for Mechanically Assisted Circulatory Support [INTERMACS] Profiles 1-3) with continuous-flow dVADs in International Society for Heart and Lung Transplantation Registry for Mechanically Assisted Circulatory Support (2013-2017) according to the need for pre-operative TCS (n = 5,632) vs no TCS (n = 7,879). Of these, 726 (5.4%) had biventricular assist devices (BiVAD). Furthermore, we compared prevalent rates (events/100 patient-months) of bleeding, device-related infection, hemorrhagic and ischemic cerebrovascular accidents (hemorrhagic cerebral vascular accident [hCVA], and ischemic cerebral vascular accident [iCVA]) in early (<3 months) and late (≥3 months) post-operative periods. TCS included extracorporeal membrane oxygenation (ECMO) (n = 1,138), intra-aortic balloon pump (IABP) (n = 3,901), and other TCS (n = 593). Within 3 post-operative months, there were more major bleeding and cerebrovascular accidents (CVAs) in patients with pre-operative ECMO (events/100 patient-months rates: bleeding = 19, hCVA = 1.6, iCVA = 2.8) or IABP (bleeding = 17.3, hCVA = 1.5, iCVA = 1.5) vs no TCS (bleeding = 13.2, hCVA = 1.1, iCVA = 1.2, all p < 0.05). After 3 months, adverse events were lower and similar in all groups. Patients with ECMO had the worst short- and long-term survival rates. Patients with BiVAD had the worst survival rate regardless of need for pre-operative TCS. CVA and multiorgan failures were the common causes of death for patients with TCS and patients without TCS. Patients requiring TCS before dVAD had a sicker phenotype and higher rates of early post-operative adverse events than patients without TCS. ECMO was associated with very high early ischemic stroke, bleeding, and mortality. The extreme CS phenotype needing ECMO warrants a higher-level profile status, such as INTERMACS ""0.""",success
22172859,False,Comparative Study;Journal Article,,,,,,,,True,"Advanced age has been viewed as a contraindication to orthotopic heart transplantation (OHT). We analyzed the outcome of OHT in patients who were aged 70 years or older and compared the results with those in younger patients during a two-decade period. A total of 519 patients underwent first-time single-organ OHT at our institution from 1988 to 2009. Patients were divided into three groups by age: ≥70-years old (group 1, n=37), 60 to 69-years old (group 2, n=206), and ≤60-years old (group 3, n=276). Primary endpoints were 30-days, and 1-, 5-, and 10-years survival. Secondary outcomes included re-operation for bleeding, postoperative need for dialysis, and length of postoperative intubation. There was no significant difference in survival between the greater than or equal to 70-year-old group and the two younger age groups for the first 10 years after OHT. Survival rates at 30 days, and 1-, 5-, and 10-years, and median survival in group 1 recipients were 100%, 94.6%, 83.2%, 51.7%, and 10.9 years (CI 7.1-11.0), respectively; in group 2 those numbers were 97.6%, 92.7%, 73.8%, 47.7%, and 9.1 years (CI 6.7-10.9), respectively; and in group 3 those numbers were 96.4%, 92.0%, 74.7%, 57.1%, and 12.2 years (CI 10.7-15.4; P=NS), respectively. There was no significant difference in secondary outcomes of re-operation for bleeding, postoperative need for dialysis, and prolonged intubation among the three age groups. Patients who are aged 70 years and older can undergo heart transplantation with similar morbidity and mortality when compared with younger recipients. Advanced heart failure patients who are aged 70 years and older should not be excluded from transplant consideration based solely on an age criterion. Stringent patient selection, however, is necessary.",success
35266620,False,Journal Article,,,,,,,,True,"As we enter the third year of the new adult heart allocation policy, we are faced with the new challenges of the COVID-19 pandemic. In 2020, new listings (adult and pediatric) decreased slightly, with 4000 new listings in 2020, compared with 4087 in 2019; however, the number of adult heart transplants performed continued to increase, to 3715 in 2020. The number of pediatric heart transplants declined from 509 in 2019 to 465 in 2020. One-year and six-month posttransplant mortality rates in adult recipients have increased slightly since 2015 but have not significantly changed over the past decade. Overall, posttransplant mortality rates for adult recipients were 7.4% at six months and 9.4% at one year for transplants in 2019, 14.0% at three years for transplants in 2017, and 19.1% at five years for transplants in 2015. Although shorter-term posttransplant mortality rates have slightly increased, there has been a steady downward trend in longer-term mortality. Mortality rates for pediatric recipients were 5.7% at six months and 8.1% at one year for transplants in 2019, 11.6% at three years for transplants in 2017, and 15.2% at five years for transplants in 2015.",success
26632028,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"With increasing age of patients with heart failure, it is important to understand the potential role for orthotopic heart transplant (OHT) in elderly patients. We examined recipient and donor characteristics and long-term outcomes of older recipients of OHT in the United States. Using the United Network for Organ Sharing database, we identified OHT recipients from the years 1987-2014 and stratified them by age 18-59 years old, 60-69 years old, and ≥70 years old. We compared baseline characteristics of recipients and donors and assessed outcomes across groups. During this period, 50,432 patients underwent OHT; 71.8% (n = 36,190) were 18-59 years old, 26.8% (n = 13,527) were 60-69 years old, and 1.4% (n = 715) were ≥70 years old. Comparing the ≥70 years old group and 60-69 years old group, older patients had higher rates of ischemic etiology (53.6% vs 44.9%) and baseline renal dysfunction (61.4% vs 56.4%) and at the time of OHT were less likely to be currently hospitalized (45.0% vs 50.9%) or supported with left ventricular assist device therapy (21.0% vs 28.3%). Older recipients received organs from older donors (median age 36 years old vs 30 years old) who were more likely to have diabetes and substance use. After OHT, the median length of stay was similar between groups. At 1 year, of patients alive, patients ≥70 years old had fewer rejection episodes (17.8%) compared with patients 60-69 years old (29.5%). The 5-year mortality was 26.9% for recipients 18-59 years old, 29.3% for recipients 60-69 years old, and 30.8% for recipients ≥70 years old. Despite advanced age and less ideal donors, OHT recipients in their 70s had similar outcomes to recipients in their 60s. Selected older patients should not routinely be excluded from consideration for OHT.",success
23551862,False,Journal Article,,,,,,,,True,"Cardiac arrhythmias are an important cause of morbidity in infants. Although the spectrum of types of arrhythmia has been reported, there has been no previous population-based study of the incidence of arrhythmias in infancy. Our aim was to define the population incidence of arrhythmia in infants. We based this study on the Northern Region of England with a resident population of 3.1 million and an annual live birth rate of 33,000. We identified all clinically significant arrhythmias in infants in 1991-2010 from the regional cardiac database. All diagnoses were based on analysis of the electrocardiogram. Infants with only the substrate for arrhythmia (such as QT prolongation or ventricular pre-excitation) were excluded. In 20 years, there were 662,698 live births. We identified 162 cases of newly diagnosed arrhythmia of which 22 had associated structural cardiovascular malformations. The incidence of arrhythmia was 24.4 per 100,000 live births. The most common arrhythmia was atrioventricular re-entry tachycardia with an incidence of 16.3 per 100,000. Complete atrioventricular block and atrial flutter both occurred at 2.1 cases per 100,000 live births, and other arrhythmias were rare. This study is the first to report a population incidence of arrhythmia in infants.",success
24763516,False,Journal Article;Review,,,,,,,,True,"The goal of this statement is to review available literature and to put forth a scientific statement on the current practice of fetal cardiac medicine, including the diagnosis and management of fetal cardiovascular disease. A writing group appointed by the American Heart Association reviewed the available literature pertaining to topics relevant to fetal cardiac medicine, including the diagnosis of congenital heart disease and arrhythmias, assessment of cardiac function and the cardiovascular system, and available treatment options. The American College of Cardiology/American Heart Association classification of recommendations and level of evidence for practice guidelines were applied to the current practice of fetal cardiac medicine. Recommendations relating to the specifics of fetal diagnosis, including the timing of referral for study, indications for referral, and experience suggested for performance and interpretation of studies, are presented. The components of a fetal echocardiogram are described in detail, including descriptions of the assessment of cardiac anatomy, cardiac function, and rhythm. Complementary modalities for fetal cardiac assessment are reviewed, including the use of advanced ultrasound techniques, fetal magnetic resonance imaging, and fetal magnetocardiography and electrocardiography for rhythm assessment. Models for parental counseling and a discussion of parental stress and depression assessments are reviewed. Available fetal therapies, including medical management for arrhythmias or heart failure and closed or open intervention for diseases affecting the cardiovascular system such as twin-twin transfusion syndrome, lung masses, and vascular tumors, are highlighted. Catheter-based intervention strategies to prevent the progression of disease in utero are also discussed. Recommendations for delivery planning strategies for fetuses with congenital heart disease including models based on classification of disease severity and delivery room treatment will be highlighted. Outcome assessment is reviewed to show the benefit of prenatal diagnosis and management as they affect outcome for babies with congenital heart disease. Fetal cardiac medicine has evolved considerably over the past 2 decades, predominantly in response to advances in imaging technology and innovations in therapies. The diagnosis of cardiac disease in the fetus is mostly made with ultrasound; however, new technologies, including 3- and 4-dimensional echocardiography, magnetic resonance imaging, and fetal electrocardiography and magnetocardiography, are available. Medical and interventional treatments for select diseases and strategies for delivery room care enable stabilization of high-risk fetuses and contribute to improved outcomes. This statement highlights what is currently known and recommended on the basis of evidence and experience in the rapidly advancing and highly specialized field of fetal cardiac care.",success
29246961,False,Journal Article;Meta-Analysis;Systematic Review,,,,,,,,True,"There is no consensus on the most effective and best tolerated first-line antiarrhythmic treatment for fetal tachyarrhythmia. The purpose of this systematic review and meta-analysis was to compare the efficacy, safety, and fetal-maternal tolerance of first-line monotherapies for fetal supraventricular tachycardia and atrial flutter. A comprehensive search of several databases was conducted through January 2017. Only studies that made a direct comparison between first-line treatments of fetal tachyarrhythmia were included. Outcomes of interest were termination of fetal tachyarrhythmia, fetal demise, and maternal complications. Ten studies met inclusion criteria, with 537 patients. Overall, 291 patients were treated with digoxin, 137 with flecainide, 102 with sotalol, and 7 with amiodarone. Digoxin achieved a lower rate of supraventricular tachycardia termination compared with flecainide (odds ratio [OR]: 0.773; 95% confidence interval [CI], 0.605-0.987; I<sup>2</sup>=34%). In fetuses with hydrops fetalis, digoxin had lower rates of tachycardia termination compared with flecainide (OR: 0.412; 95% CI, 0.268-0.632; I<sup>2</sup>=0%). There was no significant difference in the incidence of maternal side effects between digoxin and flecainide groups (OR: 1.134; 95% CI, 0.129-9.935; I<sup>2</sup>=80.79%). The incidence of maternal side effects was higher in patients treated with digoxin compared with sotalol (OR: 3.148; 95% CI, 1.468-6.751; I<sup>2</sup>=0%). There was no difference in fetal demise between flecainide and digoxin (OR: 0.767; 95% CI, 0.140-4.197; I<sup>2</sup>=44%). Flecainide may be more effective treatment than digoxin as a first-line treatment for fetal supraventricular tachycardia.",success
28713331,False,Journal Article;Review,,,,,,,,True,"Graves' disease is the most common cause of thyrotoxicosis in women of childbearing age. Approximately 1% of pregnant women been treated before, or are being treated during pregnancy for Graves' hyperthyroidism. In pregnancy, as in not pregnant state, thyroid-stimulating hormone (TSH) receptor (TSHR) antibodies (TRAbs) are the pathogenetic hallmark of Graves' disease. TRAbs are heterogeneous for molecular and functional properties and are subdivided into activating (TSAbs), blocking (TBAbs), or neutral (N-TRAbs) depending on their effect on TSHR. The typical clinical features of Graves' disease (goiter, hyperthyroidism, ophthalmopathy, dermopathy) occur when TSAbs predominate. Graves' disease shows some peculiarities in pregnancy. The TRAbs disturb the maternal as well as the fetal thyroid function given their ability to cross the placental barrier. The pregnancy-related immunosuppression reduces the levels of TRAbs in most cases although they persist in women with active disease as well as in women who received definitive therapy (radioiodine or surgery) before pregnancy. Changes of functional properties from stimulating to blocking the TSHR could occur during gestation. Drug therapy is the treatment of choice for hyperthyroidism during gestation. Antithyroid drugs also cross the placenta and therefore decrease both the maternal and the fetal thyroid hormone production. The management of Graves' disease in pregnancy should be aimed at maintaining euthyroidism in the mother as well as in the fetus. Maternal and fetal thyroid dysfunction (hyperthyroidism as well as hypothyroidism) are in fact associated with several morbidities. Monitoring of the maternal thyroid function, TRAbs measurement, and fetal surveillance are the mainstay for the management of Graves' disease in pregnancy. This review summarizes the biochemical, immunological, and therapeutic aspects of Graves' disease in pregnancy focusing on the role of the TRAbs in maternal and fetal function.",success
15851326,False,Case Reports;Journal Article,,,,,,,,False,,success
16216960,False,Journal Article,,,,,,,,True,"Isomerism is associated with a complex spectrum of anomalies. There is paucity of data on prenatally detected cases. Between January 1990 and February 2004, 83 of 166 cases (50%) had a prenatal diagnosis of left isomerism (LAI; 52 of 97) or right isomerism (RAI; 31 of 69) at our institution. The spectrum of anomalies, management, and outcomes was compared for fetal and postnatal diagnoses of LAI and RAI. RAI more often than LAI was associated with AV septal defect (90% versus 56%; P<0.0001), pulmonary outflow obstruction (91% versus 37%; P<0.0001), total anomalous pulmonary venous drainage (73% versus 13%; P<0.0001), and abnormal VA connections (68% versus 33%; P<0.0001), whereas inferior vena cava interruption (3% versus 93%; P<0.0001), complete AV block (0% versus 13%; P=0.004), aortic obstruction (6% versus 33%; P<0.0001), and extracardiac defects (5% versus 25%; P=0.006) were less common. The spectrum of lesions was comparable for fetal and postnatal cases, except for AV block (fetal, 25%; postnatal, 0%; P=0.0002) and AV septal defect (fetal, 67%; postnatal, 42%; P=0.023) in LAI. Fetal demise was due mainly to pregnancy termination (LAI, 42%; RAI, 45%). Survival of actively managed children with LAI was significantly better than for those with RAI (P<0.0001) but did not differ with regard to fetal versus postnatal diagnosis. Most LAI cases required no intervention or underwent successful biventricular cardiac surgery (65%), unlike RAI cases (13%; P<0.0001). Prenatal diagnosis did not affect overall survival despite facilitated care. The prognosis of RAI was worse compared with LAI because of more complex associated cardiac defects and the inability to perform successful surgical procedures.",success
20012231,False,Journal Article;Review,,,,,,,,True,"Anti-Ro/SSA antibodies are associated with neonatal lupus (congenital heart block (CHB), neonatal transient skin rash, hematological and hepatic abnormalities), but do not negatively affects other gestational outcomes, and the general outcome of these pregnancies is now good, when followed by experienced multidisciplinary teams. The prevalence of CHB, defined as an atrioventricular block diagnosed in utero, at birth, or within the neonatal period (0-27 days after birth), in the offspring of an anti-Ro/SSA-positive women is 1-2%, of neonatal lupus rash around 10-20%, while laboratory abnormalities in asymptomatic babies can be detected in up to 27% of cases. The risk of recurrence of CHB is ten times higher. Most of the mothers are asymptomatic at delivery and are identified only by the birth of an affected child. Half of these asymptomatic women develop symptoms of a rheumatic disease, most commonly arthralgias and xerophtalmia, but few develop lupus nephritis. A standard therapy for CHB is still matter of investigation, although fluorinated corticosteroids have been reported to be effective for associated cardiomyopathy. Serial echocardiograms and obstetric sonograms, performed at least every 1-2 weeks starting from the 16th week of gestational age, are recommended in anti-Ro/SSA-positive pregnant women to detect early fetal abnormalities that might be a target of preventive therapy.",success
30309472,False,Journal Article;Observational Study,NCT02920346,databank,NCT02920346,NCT02920346,NCT02920346,NCT02920346|databank,NCT02920346|databank,True,"Fetal atrioventricular block (AVB) occurs in 2% to 4% of anti-Ro antibody-positive pregnancies and can develop in <24 h. Only rarely has standard fetal heart rate surveillance detected AVB in time for effective treatment. Outcome of anti-Ro pregnancies was surveilled with twice-daily home fetal heart rate and rhythm monitoring (FHRM) and surveillance echocardiography. Anti-Ro pregnant women were recruited from 16 international centers in a prospective observational study. Between 18 and 26 weeks' gestation, mothers checked FHRM twice daily with a commercially available Doppler monitor and underwent weekly or biweekly surveillance fetal echocardiograms. If FHRM was abnormal, a diagnostic echocardiogram was performed. Cardiac cycle length and atrioventricular interval were measured, and cardiac function was assessed on all echocardiograms. After 26 weeks, home FHRM and echocardiograms were discontinued, and mothers were monitored during routine obstetrical visits. Postnatal electrocardiograms were performed. Most mothers (273 of 315, 87%) completed the monitoring protocol, generating 1,752 fetal echocardiograms. Abnormal FHRM was detected in 21 mothers (6.7%) who sought medical attention >12 h (n = 7), 3 to 12 h (n = 9), or <3 h (n = 5) after abnormal FHRM. Eighteen fetuses had benign rhythms, and 3 had second- or third-degree AVB. Treatment of second-degree AVB <12 h after abnormal FHRM restored sinus rhythm. Four fetuses had first-degree AVB diagnosed by echocardiography; none progressed to second-degree AVB. No AVB was missed by home FHRM or developed after FHRM. Home FHRM confirms the rapid progression of normal rhythm to AVB and can define a window of time for successful therapy. (Prospective Maternal Surveillance of SSA [Sjögren Syndrome A] Positive Pregnancies Using a Hand-held Fetal Heart Rate Monitor; NCT02920346).",success
34113918,False,Journal Article,,,,,,,,True,"Fetal tachyarrhythmia is a condition that may lead to cardiac dysfunction, hydrops, and death. Despite a transplacental treatment, failure to obtain or maintain sinus rhythm may occur. We aimed to analyze the perinatal outcomes of sustained fetal tachyarrhythmias after in utero treatment. We performed a retrospective evaluation of 69 cases with sustained fetal tachyarrhythmia. We compared the perinatal and long-term outcomes of prenatally converted and drug-resistant fetuses. Tachyarrhythmia subtypes were also evaluated. Conversion to sinus rhythm was obtained in 74% of cases; 26% of cases were drug-resistant and delivered arrhythmic. Three perinatal deaths occurred in both groups (6.7% vs 17%, <i>P</i> = .34). Neonates delivered arrhythmic were more frequently admitted to neonatal intensive care units (75% vs 31%, <i>P</i> < .01), and their hospital stay was longer (20.9 vs 6.64 days, <i>P</i> < .001). Multiple neonatal recurrences (81% vs 11%, <i>P</i> < .001), temporary hemodynamic dysfunction or heart failure (50% vs 6.7%, <i>P</i> < .001), and postnatal use of a combination treatment (44% vs 13%, <i>P</i> = .028) were also more frequent in this population. Beyond the neonatal period, rates of recurrences within the first 16 months were higher in drug-resistant fetuses (HR = 16.14, CI 95% [4.485; 193.8], <i>P</i> < .001). In this population, postnatal electrocardiogram revealed an overrepresentation of rare mechanisms, especially permanent junctional reciprocating tachycardia (PJRT) (31%). Prenatal conversion to stable sinus rhythm is a major determinant of perinatal and long-term outcomes in fetal tachyarrhythmias. The underlying electrophysiological mechanisms have a major role in predicting these differential outcomes with an overrepresentation of PJRT in the drug-resistant population.",success
31416531,True,"Clinical Trial;Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"Standardized treatment of fetal tachyarrhythmia has not been established. This study sought to evaluate the safety and efficacy of protocol-defined transplacental treatment for fetal supraventricular tachycardia (SVT) and atrial flutter (AFL). In this multicenter, single-arm trial, protocol-defined transplacental treatment using digoxin, sotalol, and flecainide was performed for singleton pregnancies from 22 to <37 weeks of gestation with sustained fetal SVT or AFL ≥180 beats/min. The primary endpoint was resolution of fetal tachyarrhythmia. Secondary endpoints were fetal death, pre-term birth, and neonatal arrhythmia. Adverse events (AEs) were also assessed. A total of 50 patients were enrolled at 15 institutions in Japan from 2010 to 2017; short ventriculoatrial (VA) SVT (n = 17), long VA SVT (n = 4), and AFL (n = 29). One patient with AFL was excluded because of withdrawal of consent. Fetal tachyarrhythmia resolved in 89.8% (44 of 49) of cases overall and in 75.0% (3 of 4) of cases of fetal hydrops. Pre-term births occurred in 20.4% (10 of 49) of patients. Maternal AEs were observed in 78.0% (39 of 50) of patients. Serious AEs occurred in 1 mother and 4 fetuses, thus resulting in discontinuation of protocol treatment in 4 patients. Two fetal deaths occurred, mainly caused by heart failure. Neonatal tachyarrhythmia was observed in 31.9% (15 of 47) of neonates within 2 weeks after birth. Protocol-defined transplacental treatment for fetal SVT and AFL was effective and tolerable in 90% of patients. However, it should be kept in mind that serious AEs may take place in fetuses and that tachyarrhythmias may recur within the first 2 weeks after birth.",success
8989479,False,Journal Article,,,,,,,,True,"Maternally administered digoxin for the treatment of fetal supraventricular tachycardia (SVT) complicated by hydrops fetalis may be ineffective secondary to poor transplacental drug transfer. We present our experience with eight pregnancies treated with transplacental therapy or combined maternal and direct fetal intramuscular therapy. Response to treatment following maternal intravenous administration (MIV) of digoxin or a combination of fetal intramuscular (FIM) digoxin and MIV is described for eight hydropic fetuses during nine successful pharmacologic conversions. The MIV digoxin was administered using standard loading and maintenance protocols. FIM was administered at a dose of 88 micrograms/kg q 12-24 hours, to a maximum of three injections in the fetal buttock. Time to onset of the first two hours of sinus rhythm (TO2 degrees), time to onset > 90% sinus rhythm (TO > 90%), and time to resolution of hydrops fetalis (HF) were noted. The mean heart rate was 257 +/- 36 beats/minute and the mean gestational age was 29 +/- 4.8 weeks. Fetal SVT was due to a reentrant mechanism in all cases. For the three fetuses that underwent successful cardioversion following MIV digoxin (all required additional maternal antiarrhythmic drugs), TO2 degrees was 145 +/- 114 hours, TO > 90% was 176 +/- 55 hours, and HF resolved in 41 +/- 37 days. Initial combined FIM and MIV therapy in four fetuses resulted in a TO2 degrees of 5.5 +/- 4 hours, TO > 90% of 22 +/- 14 hours, and resolution of HF in 25 +/- 21 days. For the two failed cardioversions with transplacental treatment alone (one fetus had recurrent SVT with hydrops after initial successful cardioversion with MIV), TO2 degrees was 203 +/- 180 hours and TO > 90% was 313 +/- 270 hours. Once FIM was begun in these fetuses, TO2 degrees was 17 +/- 7 hours and TO > 90% was 60 +/- 13 hours; HF resolved in 45 days in one fetus, whereas the other fetus never had resolution of hydrops despite 100 days of antiarrhythmic therapy. Direct fetal intramuscular injection of digoxin combined with transplacental therapy appears to shorten the time to initial conversion of SVT and to sustain sinus rhythm in the fetus with SVT complicated by hydrops fetalis.",success
28833310,False,Comparative Study;Journal Article;Meta-Analysis;Systematic Review,,,,,,,,True,"Multiple transplacental medications can be used to treat fetal tachycardia. We sought to perform a systematic review and meta-analysis to determine whether digoxin, flecainide, or sotalol was the most efficacious therapy for converting fetal tachycardia to sinus rhythm. We performed a systematic review and meta-analysis to compare digoxin, flecainide, or sotalol as first-line therapy for fetal tachycardia. Studies were identified by a search of PubMed (Medline), Web of Science, and Scopus. There were 21 studies included. Flecainide (OR: 1.4, 95% CI: 1.1-2.0, I<sup>2</sup>  = 60%, P = 0.03) and sotalol (OR:1.4, 95% CI:1.1-2.0, I<sup>2</sup>  = 30%, P = 0.02) were superior to digoxin for conversion of fetal tachycardia to sinus rhythm. In those with hydrops, the benefit over digoxin was more notable for both flecainide (OR: 5.0, 95% CI: 2.5-10.0, I<sup>2</sup>  = 0%, P < 0.001) and sotalol (OR: 2.5, 95% CI: 1.7-5.0, I<sup>2</sup>  = 0%, P < 0.001). When limited to atrioventricular reentrant tachycardia, flecainide was superior to digoxin (OR:1.7, 95% CI:1.1-3.3, I<sup>2</sup>  = 62%, P = 0.03) and sotalol (OR:1.3, 95% CI:1.1-1.7, I<sup>2</sup>  = 0%, P = 0.01). Digoxin should not be first-line therapy for fetal tachycardia, particularly in the presence of hydrops fetalis. Flecainide should be the first-line therapy of choice in atrioventricular reentrant tachycardia. Further study may identify further sub-populations responding differently.",success
27554948,False,"Comparative Study;Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"The optimal treatment for fetal supraventricular tachycardia (SVT) with 1:1 atrioventricular relationship is unclear. We compared the effectiveness of transplacental treatment protocols used in 2 centers. Pharmacologic treatment was used in 84 fetuses. Maternal oral flecainide was the primary therapy in center 1 (n = 34) and intravenous maternal digoxin in center 2 (n = 50). SVT mechanism was classified by mechanical ventriculoatrial (VA) time intervals as short VA or long VA. Treatment success was defined as conversion to sinus rhythm (SR), or rate control, defined as >15% rate reduction. Short VA interval occurred in 67 fetuses (80%) and long VA in 17 (20%). Hydrops was present 28 of 84 (33%). For short VA SVT, conversion to SR was 29 of 42 (69%) for digoxin and 24 of 25 (96%) for flecainide (P = .01). For long VA SVT, conversion to SR and rate control was 4 of 8 (50%) and 0 of 8, respectively, for digoxin, and 6 of 9 (67%) and 2 of 9 (cumulative 89%) for flecainide (P = .13). In nonhydropic fetuses, digoxin was successful in 23 of 29 (79%) and flecainide in 26 of 27 (96%) (P = .10). In hydrops, digoxin was successful in 8 of 21 (38%), flecainide alone in 6 of 7 (86%, P = .07 vs digoxin), and flecainide ± amiodarone in 7 of 7 (100%) (P = .01). Intrauterine or neonatal death occurred in 9 of 21 hydropic fetuses treated with digoxin (43%), compared to 0 of 7 (P = .06) treated with flecainide. Flecainide was more effective than digoxin, especially when hydrops was present. No adverse fetal outcomes were attributed to flecainide.",success
26829115,False,Journal Article;Observational Study,,,,,,,,True,"Fetal tachyarrhythmia can lead to fetal hydrops due to heart failure. Flecainide is often considered as second-line therapy when digoxin monotherapy fails, which is more likely in hydropic fetuses. Time to conversion to sinus rhythm (SR) is critical in cases presenting with hydrops. The aim of this study was to evaluate the efficacy and time to conversion to SR of transplacental treatment, especially flecainide. This is a retrospective observational study of 46 fetuses with fetal tachyarrhythmia. Treatment was either flecainide (n = 28, 60.9%), digoxin+flecainide combination (n = 4, 8.7%), or digoxin (n = 10, 21.7%). In 4 fetuses (8.7%), no treatment was necessary. In our study population, 26 of the 32 fetuses (81.2%) that were treated with flecainide as a first-line therapy (flecainide or digoxin+flecainide) converted to SR. The median time to conversion to SR was 3 days (range 1-7 days) with flecainide monotherapy and 11.5 days (range 3-14 days) with a combination therapy. Seventy-two percent (13/18) of hydropic fetuses and 90% (9/10) of nonhydropic fetuses converted to SR when treated with flecainide monotherapy. There was no statistical difference in rates of conversion to SR in hydropic and nonhydropic fetuses (P = .37) or time to conversion to SR in the 2 groups (P = .9). In the majority of the remaining fetuses, there was a partial response with decreased ventricular heart rates that were well tolerated. Flecainide is highly effective in achieving SR in hydropic and nonhydropic fetuses with supraventricular tachycardia in a median time of 3 days. In our opinion, flecainide should be considered as first-line therapy in fetal supraventricular tachycardia with and without hydrops.",success
7930263,True,"Comparative Study;Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"The aim of this study was to evaluate fetal tachycardia and the efficacy of maternally administered antiarrhythmic agents and the effect of this therapy on delivery and postpartum management. Sustained fetal tachycardia is a potentially life-threatening condition in which pharmacologic therapy is reported to be effective. There is ongoing discussion about optimal management. A group of 51 patients with M-mode echocardiographically documented fetal tachycardia was studied retrospectively. Thirty-three fetuses had supraventricular tachycardia; 15 had atrial flutter; 1 had two episodes of both; and 2 had ventricular tachycardia. Fetal hydrops was seen in 22 patients. Thirty-four fetuses received maternal therapy with either digoxin or flecainide as the first administered drug (additional drugs were given in 12). Drug treatment was successful in establishing acceptable rhythm control in 82% (84% without, 80% with hydrops). In the latter group the median number of drugs and number of days to conversion were higher. Three patients with fetal hydrops died. In 50% of cases, tachycardia reappeared at delivery: 9 neonates presented with atrial flutter, 14 with supraventricular tachycardia and 1 with ventricular tachycardia. Seventy-eight percent of the group had pharmacologic therapy by 1 month of age and 14% by 3 years. Fetal tachycardia can be treated adequately in the majority of patients, even in the presence of hydrops, and therefore emergency delivery might not be indicated. Digoxin and flecainide were drugs of first choice and produced no serious adverse effects in this series of patients. The majority of patients do not require prolonged therapy.",success
36106782,False,"Journal Article;Review;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"One of the most successful achievements of fetal intervention is the pharmacologic management of fetal arrhythmias. This management usually takes place during the second or third trimester. While most arrhythmias in the fetus are benign, both tachy- and bradyarrhythmias can lead to fetal hydrops or cardiac dysfunction and require treatment under certain conditions. This review will highlight precise diagnosis by fetal echocardiography and magnetocardiography, the 2 primary means of diagnosing fetuses with arrhythmia. Additionally, transient or hidden arrhythmias such as bundle branch block, QT prolongation, and torsades de pointes, which can lead to cardiomyopathy and sudden unexplained death in the fetus, may also need pharmacologic treatment. The review will address the types of drug therapies; current knowledge of drug usage, efficacy, and precautions; and the transition to neonatal treatments when indicated. Finally, we will highlight new assessments, including the role of the nurse in the care of fetal arrhythmias. The prognosis for the human fetus with arrhythmias continues to improve as we expand our ability to provide intensive care unit-like monitoring, to better understand drug treatments, to optimize subsequent pregnancy monitoring, to effectively predict timing for delivery, and to follow up these conditions into the neonatal period and into childhood. Coordinated initiatives that facilitate clinical fetal research are needed to address gaps in knowledge and to facilitate fetal drug and device development.",success
35001672,False,Journal Article,,,,,,,,True,"Background Transplacental fetal treatment of immune-mediated fetal heart disease, including third-degree atrioventricular block (AVB III) and endocardial fibroelastosis, is controversial. Methods and Results To study the impact of routine transplacental fetal treatment, we reviewed 130 consecutive cases, including 108 with AVB III and 22 with other diagnoses (first-degree/second-degree atrioventricular block [n=10]; isolated endocardial fibroelastosis [n=9]; atrial bradycardia [n=3]). Dexamethasone was started at a median of 22.4 gestational weeks. Additional treatment for AVB III included the use of a β-agonist (n=47) and intravenous immune globulin (n=34). Fetal, neonatal, and 1-year survival rates with AVB III were 95%, 93%, and 89%, respectively. Variables present at diagnosis that were associated with perinatal death included an atrial rate <90 beats per minute (odds ratio [OR], 258.4; 95% CI, 11.5-5798.9; <i>P</i><0.001), endocardial fibroelastosis (OR, 28.9; 95% CI, 1.6-521.7; <i>P</i><0.001), fetal hydrops (OR, 25.5; 95% CI, 4.4-145.3; <i>P</i><0.001), ventricular dysfunction (OR, 7.6; 95% CI, 1.5-39.4; <i>P</i>=0.03), and a ventricular rate <45 beats per minute (OR, 12.9; 95% CI, 1.75-95.8; <i>P</i>=0.034). At a median follow-up of 5.9 years, 85 of 100 neonatal survivors were paced, and 1 required a heart transplant for dilated cardiomyopathy. Cotreatment with intravenous immune globulin was used in 16 of 22 fetuses with diagnoses other than AVB III. Neonatal and 1-year survival rates of this cohort were 100% and 95%, respectively. At a median age of 3.1 years, 5 of 21 children were paced, and all had normal ventricular function. Conclusions Our findings reveal a low risk of perinatal mortality and postnatal cardiomyopathy in fetuses that received transplacental dexamethasone±other treatment from the time of a new diagnosis of immune-mediated heart disease.",success
20384469,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"The purpose of this study is to describe an in utero management strategy for fetuses with immune-mediated 2° or 3° atrioventricular (AV) block. The management strategy as applied to 29 fetuses consisted of three parts. First, using fetal echocardiography and obstetrical ultrasound, we assessed fetal heart rate (FHR), heart failure, growth and a modified biophysical profile score (BPS) assessing fetal movement, breathing and tone. Second, we treated all fetuses with transplacental dexamethasone, adding terbutaline if the FHR was<56 bpm. Digoxin and/or intravenous immune globulin (IVIG) was added for progressive fetal heart failure. Third, we delivered fetuses by cesarean section for specific indications that included abnormal BPS, maternal/fetal conditions, progression of heart failure, or term pregnancy. We assessed perinatal survival, predictors of delivery and maternal/fetal complications in 29 fetuses with 3° (n=23) or 2° (n=6) AV block. There were no fetal deaths. In utero therapy included dexamethasone (n=29), terbutaline (n=13), digoxin (n=3) and/or IVIG (n=1). Delivery indications included term gestation (66%), fetal/maternal condition (14%), low BPS (10%) and progression of fetal heart failure (10%). An abnormal BPS correlated with urgent delivery. These results suggest that applying this specific management strategy that begins in utero can improve perinatal outcome of immune-mediated AV block.",success
26284740,False,Journal Article;Review,,,,,,,,True,"Cardiac neonatal lupus syndrome is due to anti-SSA or SSB antibodies and mainly includes congenital heart block (CHB) and dilated cardiomyopathy (DCM). Its optimal management is still debated. We report a large series of autoimmune high degree CHB. Inclusion criteria in this retrospective study were fetuses or neonates with high-degree CHB associated with maternal anti-SSA/SSB antibodies. 214 CHB were included: 202 detected in utero at a median term of 23 weeks' gestation (WG) [range 16 to 39 WG] and 12 neonatal cases diagnosed at a median age of 0 days [range birth to 8 days]. The 214 cases of CHB included 202 (94.4%) third-degree CHB, 8 (3.7%) second-degree CHB, and 4 (1.9%) intermittent CHB. In multivariate analysis, the factors associated with feto-neonatal deaths (15.7%) were hydrops (p<0.001; hazard ratio [HR] 12.4 [95% confidence interval (95%CI) 4.7-32.7]) and prematurity (p=0.002; HR 17.1 [95%CI 2.8-103.1]). During a median follow-up of 7 years [birth to 36 years], 148 of 187 children born alive (79.1%) had a pacemaker, 35 (18.8%, one missing data) had DCM, and 22 (11.8%) died. In multivariate analysis, factors associated with child death were in utero DCM (p=0.0157; HR 6.37 [95%CI: 1.25-32.44]), postnatal DCM (p<0.0001; HR 227.58[95%CI: 24.33-2128.46]) and pacemaker implantation (p=0.0035; HR 0.11[95%CI: 0.02-0.51]). The use of fluorinated steroids was neither associated with survival nor with regression of 2nd degree CHB. In this second largest series of CHB, we confirm some of the previous results. We were unable to find data supporting the routine use of in utero fluorinated steroids.",success
22308531,False,"Journal Article;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",,,,,,,,True,"The autopsy and clinical information on children dying with anti-SSA/Ro-associated cardiac manifestations of neonatal lupus (cardiac NL) were examined to identify patterns of disease, gain insight into pathogenesis and enhance the search for biomarkers and preventive therapies. A retrospective analysis evaluating reports from 18 autopsies of cardiac NL cases and clinical data from the Research Registry for Neonatal Lupus was performed. Of the 18 cases with autopsies, 15 had advanced heart block, including 3 who died in the second trimester, 9 in the third trimester and 3 post-natally. Three others died of cardiomyopathy without advanced block, including two dying pre-natally and one after birth. Pathological findings included fibrosis/calcification of the atrioventricular (AV) node, sinoatrial (SA) node and bundle of His, endocardial fibroelastosis (EFE), papillary muscle fibrosis, valvular disease, calcification of the atrial septum and mononuclear pancarditis. There was no association of pathology with the timing of death except that in the third-trimester deaths more valvular disease and/or extensive conduction system abnormalities were observed. Clinical rhythm did not always correlate with pathology of the conduction system, and the pre-mortem echocardiograms did not consistently detect the extent of pathology. Fibrosis of the AV node/distal conduction system is the most characteristic histopathological finding. Fibrosis of the SA node and bundle of His, EFE and valve damage are also part of the anti-Ro spectrum of injury. Discordance between echocardiograms and pathology findings should prompt the search for more sensitive methods to accurately study the phenotype of antibody damage.",success
11153745,False,Journal Article,,,,,,,,True,"We report 16 infants with complete congenital heart block (CHB) who developed late-onset dilated cardiomyopathy despite early institution of cardiac pacing. Isolated CHB has an excellent prognosis following pacemaker implantation. Most early deaths result from delayed initiation of pacing therapy or hemodynamic abnormalities associated with congenital heart defects. A multi-institutional study was performed to identify common clinical features and possible risk factors associated with late-onset dilated cardiomyopathy in patients born with congenital CHB. Congenital heart block was diagnosed in utero in 12 patients and at birth in four patients. Ten of 16 patients had serologic findings consistent with neonatal lupus syndrome (NLS). A pericardial effusion was evident on fetal ultrasound in six patients. In utero determination of left ventricular (LV) function was normal in all. Following birth, one infant exhibited a rash consistent with NLS and two had elevated hepatic transaminases and transient thrombocytopenia. In the early postnatal period, LV function was normal in 15 patients (shortening fraction [SF] = 34 +/- 7%) and was decreased in one (SF = 20%). A cardiac pacemaker was implanted during the first two weeks of life in 15 patients and at seven months in one patient. Left ventricular function significantly decreased during follow-up (14 days to 9.3 years, SF = 9% +/- 5%). Twelve of 16 patients developed congestive heart failure before age 24 months. Myocardial biopsy revealed hypertrophy in 11 patients, interstitial fibrosis in 11 patients, and myocyte degeneration in two patients. Clinical status during follow-up was guarded: four patients died from congestive heart failure; seven required cardiac transplantation; one was awaiting cardiac transplantation; and four exhibited recovery of SF (31 +/- 2%). Despite early institution of cardiac pacing, some infants with CHB develop LV cardiomyopathy. Patients with CHB require close follow-up not only of their cardiac rate and rhythm, but also ventricular function.",success
32674792,True,"Clinical Trial, Phase II;Journal Article;Multicenter Study;Randomized Controlled Trial;Research Support, N.I.H., Extramural;Research Support, Non-U.S. Gov't",NCT01379573,databank,NCT01379573,NCT01379573,NCT01379573,NCT01379573|databank,NCT01379573|databank,True,"Experimental and clinical evidence support the role of macrophage Toll-like receptor signaling in maternal anti-SSA/Ro-mediated congenital heart block (CHB). Hydroxychloroquine (HCQ), an orally administered Toll-like receptor antagonist widely used in lupus including during pregnancy, was evaluated for efficacy in reducing the historical 18% recurrence rate of CHB. This multicenter, open-label, single-arm, 2-stage clinical trial was designed using Simon's optimal approach. Anti-SSA/Ro-positive mothers with a previous pregnancy complicated by CHB were recruited (n = 19 Stage 1; n = 35 Stage 2). Patients received 400 mg daily of HCQ prior to completion of gestational week 10, which was maintained through pregnancy. The primary outcome was 2° or 3° CHB any time during pregnancy, and secondary outcomes included isolated endocardial fibroelastosis, 1° CHB at birth and skin rash. By intention-to-treat (ITT) analysis, 4 of 54 evaluable pregnancies resulted in a primary outcome (7.4%; 90% confidence interval: 3.4% to 15.9%). Because 9 mothers took potentially confounding medications (fluorinated glucocorticoids and/or intravenous immunoglobulin) after enrollment but prior to a primary outcome, to evaluate HCQ alone, 9 additional mothers were recruited and followed the identical protocol. In the per-protocol analysis restricted to pregnancies exposed to HCQ alone, 4 of 54 (7.4%) fetuses developed a primary outcome as in the ITT. Secondary outcomes included mild endocardial fibroelastosis (n = 1) and cutaneous neonatal lupus (n = 4). These prospective data support that HCQ significantly reduces the recurrence of CHB below the historical rate by >50%, suggesting that this drug should be prescribed for secondary prevention of fetal cardiac disease in anti-SSA/Ro-exposed pregnancies. (Preventive Approach to Congenital Heart Block With Hydroxychloroquine [PATCH]; NCT01379573).",success
26585398,False,Journal Article;Review,,,,,,,,True,"One-third of women with heart disease use medication for the treatment of cardiovascular disease (CVD) during pregnancy. Increased plasma volume, renal clearance, and liver enzyme activity in pregnant women change the pharmacokinetics of these drugs, often resulting in the need for an increased dose. Fetal well-being is a major concern among pregnant women. Fortunately, many drugs used to treat CVD can be used safely during pregnancy, with the exception of high-dose warfarin in the first trimester, angiotensin-converting-enzyme inhibitors, angiotensin-receptor blockers, amiodarone, and spironolactone. A timely and thorough discussion between the cardiologist and the pregnant patient about the potential benefits and adverse effects of medication for CVD is important. Noncompliance with necessary treatment for cardiovascular disorders endangers not only the mother, but also the fetus. This Review is an overview of the pharmacokinetic changes in medications for CVD during pregnancy and the safety of these drugs for the fetus. The implications for maternal treatment are discussed. The Review also includes a short section on the cardiovascular effects of medication used for obstetric indications.",success
11876808,False,Journal Article,,,,,,,,True,"To assess the efficacy of flecainide in the intrauterine treatment of fetal supraventricular tachycardia (SVT) with 1 : 1 atrioventricular conduction. Twenty fetuses (21-35 weeks of gestation) with SVT ranging between 215 and 280 bpm were analyzed retrospectively. Fetuses received flecainide and digoxin as either first, second or third line therapy. Intracardiac blood flow, venous Doppler waveforms and cardiotocograms were evaluated before and after drug induced conversion to sinus rhythm. After initiation of combined flecainide and digoxin therapy, the median time interval until final conversion to sinus rhythm was 5 days (range, 0-14 days). The majority of fetuses (n = 15; 75%) converted to sinus rhythm within 7 days of treatment, whereas the remaining five (25%) showed initial reduction of the heart rate to 160-215 bpm over several days, with restoration of a triphasic venous blood flow pattern before late conversion within 7-14 days after initiation of flecainide treatment. One of these fetuses showed a decrease in fetal heart rate to 160-190 bpm without conversion to sinus rhythm but with resolution of hydrops. All fetuses survived. Flecainide is safe and highly effective in the intrauterine treatment of hydropic fetuses with supraventricular tachycardia. Conversion into sinus rhythm can be expected 72 h after initiation of therapy but may take up to 14 days. Therefore therapy should be continued beyond 72 h, especially when an initial decrease of fetal heart rate is observed which may represent an early therapeutic response.",success
11817986,False,Journal Article;Review,,,,,,,,True,"The pharmacological treatment of fetal tachycardia (FT) has been described in various publications. We present a study reviewing the necessity for treatment of FT, the regimens of drugs used in the last two decades and their mode of administration. The absence of reliable predictors of fetal hydrops (FH) has led most centers to initiate treatment as soon as the diagnosis of FT has been established, although a small minority advocate nonintervention. As the primary form of pharmacological intervention, oral maternal transplacental therapy is generally preferred. Digoxin is the most common drug used to treat FT; however, effectiveness remains a point of discussion. After digoxin, sotalol seems to be the most promising agent, specifically in atrial flutter and nonhydropic supraventricular tachycardia (SVT). Flecainide is a very effective drug in the treatment of fetal SVT, although concerns about possible pro-arrhythmic effects have limited its use. Amiodarone has been described favorably, but is frequently excluded due to its poor tolerability. Verapamil is contraindicated as it may increase mortality. Conclusions on other less frequently used drugs cannot be drawn. In severely hydropic fetuses and/or therapy-resistant FT, direct fetal therapy is sometimes initiated. To minimize the number of invasive procedures, fetal intramuscular or intraperitoneal injections that provide a more sustained release are preferred. Based on these data we propose a drug protocol of sotalol 160 mg twice daily orally, increased to a maximum of 480 mg daily. Whenever sinus rhythm is not achieved, the addition of digoxin 0.25 mg three times daily is recommended, increased to a maximum of 0.5 mg three times daily. Only in SVT complicated by FH, either maternal digoxin 1 to 2mg IV in 24 hours, and subsequently 0.5 to 1 mg/day IV, or flecainide 200 to 400 mg/day orally is proposed. Initiating direct fetal therapy may follow failure of transplacental therapy.",success
30704579,False,Journal Article;Review,,,,,,,,True,"Cardiovascular disease complicating pregnancy is rising in prevalence secondary to advanced maternal age, cardiovascular risk factors, and the successful management of congenital heart disease conditions. The physiological changes of pregnancy may alter drug properties affecting both mother and fetus. Familiarity with both physiological and pharmacological attributes is key for the successful management of pregnant women with cardiac disease. This review summarizes the published data, available guidelines, and recommendations for use of cardiovascular medications during pregnancy. Care of the pregnant woman with cardiovascular disease requires a multidisciplinary team approach with members from cardiology, maternal fetal medicine, anesthesia, and nursing.",success
30278179,False,Journal Article,,,,,,,,True,"Cardiac disease in pregnancy is the number one indirect cause of maternal mortality in the United States. We propose a triad solution that includes universal screening for cardiovascular disease in pregnancy and postpartum women, patient education, and institution of a multidisciplinary cardiac team. Additionally, we emphasize essential elements to maximize care for the pregnant cardiac patient based on our experience at our institution in Bronx, NY.",success
32830393,False,Letter,,,,,,,,False,,success
37128167,False,Journal Article,,,,,,,,True,"While in-utero treatment of sustained fetal supraventricular arrhythmia (SVA) is standard practice in the previable and preterm fetus, data are limited on best practice for late preterm (34 + 0 to 36 + 6 weeks), early term (37 + 0 to 38 + 6 weeks) and term (> 39 weeks) fetuses with SVA. We reviewed the delivery and postnatal outcomes of fetuses at ≥ 35 weeks of gestation undergoing treatment rather than immediate delivery. This was a retrospective case series of fetuses presenting at ≥ 35 weeks of gestation with sustained SVA and treated transplacentally at six institutions between 2012 and 2022. Data were collected on gestational age at presentation and delivery, SVA diagnosis (short ventriculoatrial (VA) tachycardia, long VA tachycardia or atrial flutter), type of antiarrhythmic medication used, interval between treatment and conversion to sinus rhythm and postnatal SVA recurrence. Overall, 37 fetuses presented at a median gestational age of 35.7 (range, 35.0-39.7) weeks with short VA tachycardia (n = 20), long VA tachycardia (n = 7) or atrial flutter (n = 10). Four (11%) fetuses were hydropic. In-utero treatment led to restoration of sinus rhythm in 35 (95%) fetuses at a median of 2 (range, 1-17) days; this included three of the four fetuses with hydrops. Antiarrhythmic medications included flecainide (n = 11), digoxin (n = 7), sotalol (n = 11) and dual therapy (n = 8). Neonates were liveborn at 36-41 weeks via spontaneous vaginal delivery (23/37 (62%)) or Cesarean delivery (14/37 (38%)). Cesarean delivery was indicated for fetal SVA in two fetuses, atrial ectopy or sinus bradycardia in three fetuses and obstetric reasons in nine fetuses that were in sinus rhythm at the time of delivery. Twenty-one (57%) cases were treated for recurrent SVA after birth. In-utero treatment of the near term and term (≥ 35-week) SVA fetus is highly successful even in the presence of hydrops, with the majority of cases delivered vaginally closer to term, thereby avoiding unnecessary Cesarean section. © 2023 International Society of Ultrasound in Obstetrics and Gynecology.",success
16549207,False,Journal Article,,,,,,,,True,"There is mounting evidence that infants born late preterm (34-36 weeks) are at greater risk for morbidity than term infants. This article examines the changing epidemiology of gestational length among singleton births in the United States, from 1992 to 2002. Analyzing gestational age by mode of delivery, the distribution of spontaneous births shifted to the left, with 39 weeks becoming the most common length of gestation in 2002, compared with 40 weeks in 1992 (P < 0.001). Deliveries at > or =40 weeks gestation markedly decreased, accompanied by an increase in those at 34 to 39 weeks (P < 0.001). Singleton births with PROM or medical interventions had similar trends. Changes in the distribution of all singleton births differed by race/ethnicity, with non-Hispanic white infants having the largest increase in late preterm births. These observations, in addition to emerging evidence of increased morbidity, suggest the need for investigation of optimal obstetric and neonatal management of these late preterm infants.",success
18456072,False,Journal Article;Review,,,,,,,,True,"Delivery of infants who are physiologically mature and capable of successful transition to the extrauterine environment is an important priority for obstetric practitioner. A corollary of this goal is to avoid iatrogenic complications of prematurity and maternal complications from delivery. The purpose of this review is to describe the consequences of birth before physiologic maturity in late preterm and term infants, to identify factors contributing to the decline in gestational age of deliveries in the United States, and to describe strategies to reduce premature delivery of late preterm and early term infants.",success
7756199,False,Journal Article,,,,,,,,True,"To establish whether the timing of delivery between 37 and 42 weeks gestation influences neonatal respiratory outcome and thus provide information which can be used to aid planning of elective delivery at term. All cases of respiratory distress syndrome or transient tachypnoea at term requiring admission to the neonatal intensive care unit were recorded prospectively for nine years. Rosie Maternity Hospital, Cambridge. During this time 33,289 deliveries occurred at or after 37 weeks of gestation. This information enabled calculation of the relative risk of respiratory morbidity for respiratory distress syndrome or transient tachypnoea in relation to mode of delivery and onset of parturition for each week of gestation at term. The incidence of respiratory distress syndrome at term was 2.2/1000 deliveries (95% CI; 1.7-2.7). The incidence of transient tachypnoea was 5.7/1000 deliveries (95% CI; 4.9-6.5). The incidence of respiratory morbidity was significantly higher for the group delivered by caesarean section before the onset of labour (35.5/1000) compared with caesarean section during labour (12.2/1000) (odds ratio, 2.9; 95% CI 1.9-4.4; P < 0.001), and compared with vaginal delivery (5.3/1000) (odds ratio, 6.8; 95% CI 5.2-8.9; P < 0.001). The relative risk of neonatal respiratory morbidity for delivery by caesarean section before the onset of labour during the week 37+0 to 37+6 compared with the week 38+0 to 38+6 was 1.74 (95% CI 1.1-2.8; P < 0.02) and during the week 38+0 to 38+6 compared with the week 39+0 to 39+6 was 2.4 (95% CI 1.2-4.8; P < 0.02). A significant reduction in neonatal respiratory morbidity would be obtained if elective caesarean section was performed in the week 39+0 to 39+6 of pregnancy.",success
35462052,False,"Journal Article;Research Support, Non-U.S. Gov't",,,,,,,,True,"Antiarrhythmic treatment of fetal supraventricular tachycardia (SVT) is used to prevent morbidity and mortality. The postnatal management of survivors is often arbitrary and varied. The purpose of this study was to examine the utility of a risk-based postnatal management strategy. Sixty-six prenatally treated newborns with fetal long or short ventriculoatrial tachycardia were reviewed. Postnatal diagnoses included atrioventricular reentrant tachycardia, atrial ectopic tachycardia, and permanent junctional reciprocating tachycardia. Unless SVT persisted to birth, early neonatal observation without treatment was recommended. For newborns without spontaneous arrhythmia after ≥2 days of observation, inducibility was tested by transesophageal pacing study (TEPS). Postnatal therapy was advised for spontaneous or inducible SVT. Characteristics associated with these outcomes were analyzed. Twenty-eight patients (42%) experienced SVT at or early after birth, which was associated with fetal long ventriculoatrial tachycardia (odds ratio [OR] 6.8; 95% confidence interval [CI] 1.88-24.57; P = .0029); delayed in utero cardioversion with treatment (median 11 days vs 5.5 days; P < .0001); prenatal treatment with multiple antiarrhythmics (OR 4.42; 95% CI 1.56-12.55; P = .0059); and postnatal atrial ectopic tachycardia/permanent junctional reciprocating (OR 18.0; 95% CI 2.11-153.9; P = .0013). Of the 38 neonates undergoing TEPS, 19 (50%) had inducible tachyarrhythmias. Recurrence of SVT during infancy or childhood was documented in 4 of 6 patients with SVT at birth (66%), 8 of 22 patients with early neonatal SVT (36%), 4 of 19 patients with inducible SVT (21%), and 0 of 19 untreated patients without inducible SVT (0%) (P = .0032). The postnatal risk of SVT is related to the arrhythmia mechanism and prenatal treatment response. In newborns without spontaneous SVT, TEPS may be useful to guide the need for postnatal treatment on the basis of SVT inducibility.",success
22639009,False,Journal Article,,,,,,,,True,"The diagnosis and management of prenatal tachyarrhythmias is well established; however, the postnatal course and outcomes are not. The purpose of our study was to review the natural history of patients with fetal tachycardia, determine the incidence of postnatal arrhythmias, and determine whether there are factors to predict which fetuses will develop postnatal arrhythmias. A retrospective chart review of patients with fetal tachyarrhythmias investigated at British Columbia Children's and Women's Hospitals between 1983 and 2010 was conducted. Sixty-nine mother-fetus pairs were eligible for the study. Fifty-two had fetal supraventricular tachycardia, and 17 had fetal atrial flutter. Conversion to sinus rhythm occurred prenatally in 52 % of patients. Postnatal arrhythmia occurred in two thirds of patients, with 82 % of those cases occurring within the first 48 h of life. Hydrops fetalis, female sex, and lack of conversion to sinus rhythm was predictive of postnatal arrhythmia (P = 0.01, P = 0.01, and P = 0.001, respectively). Conversion to sinus rhythm prenatally did not predict postnatal arrhythmia. Median duration of treatment was 9 months. Two postnatal deaths of unknown etiology occurred. Two thirds of all patients with prenatal tachycardia will develop postnatal arrhythmia. Prenatal factors that predict postnatal arrhythmia include hydrops, sex, and whether or not conversion to sinus rhythm occurred prenatally. The majority of patients with postnatal arrhythmia present within 48 h of life, which has clinical implications for monitoring. Postnatal outcome is generally very good with most patients being weaned off medication in 6-12 months.",success
36137720,False,"Journal Article;Research Support, N.I.H., Extramural",,,,,,,,True,"Fetal supraventricular tachycardia (SVT) is rare and proposed predictors of postnatal outcomes in fetal SVT have not been validated. Valid predictors can guide postnatal management. The authors correlated fetal characteristics to the incidence of postnatal SVT and compared SVT outcomes in infants with and without a history of fetal SVT. Mother-fetus dyads with fetal SVT and a structurally normal heart were described and compared with a second cohort of infants with a postnatal diagnosis of SVT. SVT was observed in 78 fetuses and 76 survived to delivery. Maternally administered transplacental antiarrhythmics were used in 49 mother-fetus dyads. Rhythm control was achieved in 37 of 49 (76%). Among fetuses with intermittent SVT, there was no ventricular dysfunction or hydrops. Postnatal SVT occurred in one-half of infants (37 of 76), and 94% presented within the first 2 days of life. The following fetal characteristics were associated with postnatal SVT on univariable analysis: sustained SVT (87% vs 56%), ventricular dysfunction (41% vs 15%), lack of conversion to sinus rhythm (49% vs 10%), and earlier gestational age at delivery (37.6 weeks vs 38.9 weeks; P ≤ 0.01 for each comparison). Compared with infants with a postnatal diagnosis of SVT, infants with a fetal diagnosis presented earlier (median age 0 days vs 17 days; P &lt; 0.01) and had a lower incidence ventricular dysfunction at presentation (5% vs 42%; P &lt; 0.01). One-half of infants with fetal SVT had postnatal SVT, nearly all within 2 days of life. These data and predictors of postnatal SVT may influence parental counseling and postnatal clinical decision-making.",success
33421423,False,Journal Article;Observational Study,,,,,,,,True,"Current estimates of the incidence of tachyarrhythmias in infants rely on clinical documentation and may not reflect the true rate in the general population. Our aim was to describe the epidemiology of tachyarrhythmia detected in a large cohort of infants using direct-to-consumer heart rate (HR) monitoring. Data were collected from Owlet Smart Sock devices used in infants in the US with birthdates between February 2017 and February 2019. We queried the HR data for episodes of tachyarrhythmia (HR of ≥240 bpm for >60 seconds). The study included 100 949 infants (50.8% male) monitored for more than 200 million total hours. We identified 5070 episodes of tachyarrhythmia in 2508 infants. The cumulative incidence of tachyarrhythmia in our cohort was 2.5% over the first year of life. The median age at the time of the first episode of tachyarrhythmia was 36 days (range, 1-358 days). Tachyarrhythmia was more common in infants with congenital heart disease (4.0% vs 2.4%; P = .015) and in females (2.7% vs 2.0%; P < .001). The median length of an episode was 7.3 minutes (range, 60 seconds to 5.4 hours) and the probability of an episode lasting longer than 45 minutes was 16.8% (95% CI, 15.4%-18.3%). We found the cumulative incidence of tachyarrhythmia among infants using direct-to-consumer HR monitors to be higher than previously reported in studies relying on clinical diagnosis. This finding may represent previously undetected subclinical disease in young infants, the significance of which remains uncertain. Clinicians should be prepared to discuss these events with parents.",success
23058149,True,Clinical Trial;Journal Article,,,,,,,,True,"To determine the optimal adenosine dose effective in supraventricular tachycardia (SVT) and underlying conditions affecting the effective dose in children. Experimental study. Department of Cardiology, The Children's Hospital and Institute of Child Health, Lahore, from July 2008 to June 2011. All children presenting with SVT were administered adenosine in rapid boluses according to PALS guidelines using incremental doses of 100, 200 and 300 μg/kg. The response was recorded on 12 lead ECG. Preexcitation was documented and echocardiography performed on all children after attaining sinus rhythm. Mann Whitney test and Kruskal-Wallis test were used as a test of significance to determine any difference in effective adenosine dose between normal heart and various underlying conditions, taking p < 0.05 as significant. Eighty five patients were treated for 110 episodes of SVT with adenosine. M:F ratio was 2.2:1. Their age ranged from 6 days to 14 years with mean age of 27.9 months. Adenosine was effective in reverting 97 episodes of SVT to sinus rhythm (88.2%). A dose of upto 100 μg/kg was only effective in 36.4% episodes of SVT. Two hundred μg/kg was effective in 44.3% of those not responding to 100 μg/kg dose (n = 31/70, cumulative 64.5%). A dose of 300 μg/kg was effective in further 25 patients not responding to lower doses (n = 25/38, 65.8%; cumulative 88.2%). Mean effective dose of adenosine was 185.3 + 81.0 μg/kg with median effective dose of 200 μg/kg. Significantly higher dose of adenosine was required in children with underlying pre-excitation, n = 18/97 (220.8 + 67.6 μg/kg vs. 177.2 + 81.9 μg/kg, p = 0.039). Adenosine is an effective medicine in treating SVT in children. A higher dose of 200 μg/kg may be used as first bolus particularly in children with pre-excitation.",success
16391988,False,Journal Article,,,,,,,,True,"Supraventricular tachycardia is the most common pediatric arrhythmia, but there is no consensus and little evidence to guide its treatment. We sent a questionnaire to pediatric cardiologists in North America to assess the current practice pattern. Of 1534 surveys mailed, 352 (23%) were returned and 295 (19%) had complete data for analysis. In the acute setting, 11 different medications were chosen. The most commonly used in the infant without preexcitation were digoxin (42%), procainamide (21%), esmolol (13%), propranolol (10%), and amiodarone (8%). In the infant with preexcitation, propranolol (34%), procainamide (23%), esmolol (17%), amiodarone (11%), and digoxin (6%) were used. In the chronic setting, 8 different medications were chosen. The most commonly used in this scenario were digoxin (52%), propranolol (33%), amiodarone (4%), and sotalol (3%). In the infant with preexcitation, propranolol (70%), amiodarone (6%), digoxin (6%), atenolol (6%), and flecainide (5%) were used. Medication choices were influenced by additional electrophysiology training and preexcitation. Digoxin was used less in the setting of preexcitation. There are no comparative trials to explain the different medication choices. Although a number of medications may be efficacious, a randomized clinical trial is needed to offer further guidance.",success
22613357,False,Journal Article,,,,,,,,True,"The current drug of choice for reentrant supraventricular tachycardia (SVT) is adenosine followed by verapamil or diltiazem. Although limitations and significant adverse events have been encountered over the years, an alternative effective and safe agent has not been available. Dexmedetomidine has recently been shown to have potential antiarrhythmic effects, and here we describe our experience in the acute termination of reentrant SVT. Retrospective case series. Quaternary University Children's Hospital, Cardiac Intensive Care Unit. Patients who received dexmedetomidine for SVT in the past 5 years. None. SVT episodes terminated with dexmedetomidine were compared with episodes terminated with adenosine. Fifteen patients, median age of 10 days (6-16), were given 27 doses of dexmedetomidine, mean dose 0.7 ± 0.3 mcg/kg, for a total of 27 episodes of SVT. Successful termination occurred in 26 episodes (96%) at a median time of 30 seconds (20-35). Duration of sinus pause was 0.6 ± 0.2 seconds, there was one episode of hypotension and no bradycardia and sedation lasted for 34 ± 8 minutes. Five patients received 27 doses of adenosine, with an overall successful cardioversion in 17 patients (63%) (P= .0017). Transient bradycardia and hypotension was seen in three patients (11%), agitation in 16 patients (59%), and broncospasm in one patient. Median sinus pause was 2.5 seconds (2-9) (P < .001). Dexmedetomidine appears to have novel antiarrhythmic properties for the acute termination of reentrant SVT. Although adenosine is very effective, dexmedetomidine may prove to possess a more favorable therapeutic profile with increased effectiveness and fewer side effects.",success
30904484,False,Journal Article,,,,,,,,True,"Heart Rhythm Society guidelines recommend obtaining thyroid function tests (TFTs) at amiodarone initiation and every 6 months thereafter in adults, with no specific pediatric recommendations. Untreated hypothyroidism in young children negatively affects brain development and somatic growth, yet the optimal screening frequency for pediatric patients remains unclear, and limited data exist on pediatric amiodarone-induced thyroid dysfunction. The purpose of this study was to describe the patterns of amiodarone-induced thyroid dysfunction in pediatric patients. We established a retrospective cohort of 527 pediatric patients who received amiodarone between 1997 and 2017. We defined amiodarone therapy lasting 3-30 days as ""short term"" and >30 days as ""long term."" The final cohort (n = 150) consisted of 27 neonates (18%), 25 infants (16%), 27 young children (18%), and 71 children (47%). Of the children in whom TFTs were checked, half (50.8%) developed a thyroid-stimulating hormone (TSH) value above the reference for age. Neonates had the highest median peak TSH values in both short- and long-term groups: 23.5 mIU/L (interquartile range 11.4-63.1) and 28.8 mIU/L (interquartile range 11.4-34.4), respectively. Although concurrent use of inotropic support was significantly associated with lower initial TSH values, no variable related to cardiac illness or type of heart disease was associated with peak TSH values. Neonates and infants receiving amiodarone had more thyroid dysfunction with greater degrees of TSH elevation than older children. TSH elevations occurred early, even with short-term exposure. Given the concern for brain development and growth in hypothyroid children, our results suggest the need for more rigorous pediatric-specific thyroid monitoring guidelines.",success
35491986,True,"Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"Background There is limited information regarding the clinical use and effectiveness of IV sotalol in pediatric patients and patients with congenital heart disease, including those with severe myocardial dysfunction. A multicenter registry study was designed to evaluate the safety, efficacy, and dosing of IV sotalol. Methods and Results A total of 85 patients (age 1 day-36 years) received IV sotalol, of whom 45 (53%) had additional congenital cardiac diagnoses and 4 (5%) were greater than 18 years of age. In 79 patients (93%), IV sotalol was used to treat supraventricular tachycardia and 4 (5%) received it to treat ventricular arrhythmias. Severely decreased cardiac function by echocardiography was seen before IV sotalol in 7 (9%). The average dose was 1 mg/kg (range 0.5-1.8 mg/kg/dose) over a median of 60 minutes (range 30-300 minutes). Successful arrhythmia termination occurred in 31 patients (49%, 95% CI [37%-62%]) with improvement in rhythm control defined as rate reduction permitting overdrive pacing in an additional 18 patients (30%, 95% CI [19%-41%]). Eleven patients (16%) had significant QTc prolongation to >465 milliseconds after the infusion, with 3 (4%) to >500 milliseconds. There were 2 patients (2%) for whom the infusion was terminated early. Conclusions IV sotalol was safe and effective for termination or improvement of tachyarrhythmias in 79% of pediatric patients and patients with congenital heart disease, including those with severely depressed cardiac function. The most common dose, for both acute and maintenance dosing, was 1 mg/kg over ~60 minutes with rare serious complications.",success
32327076,False,Journal Article,,,,,,,,True,"This study assessed the safety and efficacy of novel and standardized protocols for the use of intravenous (IV) sotalol in pediatric patients. Acute arrhythmia treatments in children remain limited. IV sotalol is a new option but pediatric experience is limited. There is no standardized protocol for rapid infusion during acute arrhythmias. This study assessed a single center's initial experience with IV sotalol in young patients, describing a protocol for rapid infusion for acute treatment, and reviewed the safety and efficacy of maintenance dosing. This is a retrospective study of all patients who received IV sotalol at Rady Children's Hospital. Demographics, arrhythmia, hemodynamics, and effects of IV sotalol were assessed. Thirty-seven patients received IV sotalol from December 2015 to December 2018. Group 1 (n = 26) received sotalol for acute therapy and group 2 (n = 11) received a maintenance dose of sotalol after successful cardioversion with alternate therapies. The groups had similar demographics. Group 1 included patients with atrial flutter (n = 16), patients with supraventricular tachycardia (SVT) (n = 9), and patients with atrial ectopic tachycardia (AET) (n = 1). All 9 patients with SVT (100%) converted to sinus rhythm after failure to convert using adenosine. Median administration time was 15 min, the median dose was 30 mg/m<sup>2</sup>, and mean time to cardioversion was 14 min. Group 2 median infusion time was 120 min, the median dose was 54 mg/m<sup>2</sup>/day, and all patients maintained sinus rhythm. No patients required cessation for adverse effects previously described for IV sotalol. IV sotalol was safe and effective for acute and maintenance therapy in young patients. In acute patients, 30 mg/m<sup>2</sup> over 15 min converted most patients. IV sotalol adds a valuable option to IV therapies in the young.",success
34414216,False,Journal Article,,,,,,,,True,"<b>Background:</b> In April 2015, ivabradine was approved by the Food and Drug Administration for the treatment of patients with coronary artery disease and heart failure (HF). The use of this medication has been linked with improved clinical outcomes and reduced rates of hospitalization in patients with symptomatic HF and a baseline heart rate of 70 bpm and above. Nonetheless, little is known about the use of ivabradine in pediatric patients with supraventricular tachycardia (SVT). This use is not well-studied and is only endorsed by a few case reports and case series. <b>Aim:</b> This study discusses the off-label utilization of ivabradine in pediatric patients with SVT, and highlights its efficacy in treating treatment-resistant (refractory) SVT. <b>Methods:</b> We conducted a retrospective single-center observational study involving pediatric patients with SVT treated at our center between January 2016 and October 2020. We identified the total number of patients with SVT, and the number of patients with refractory SVT treated with Ivabradine. Similarly, we performed a thorough review of the databases of PubMed, Medline and Google Scholar to compare the clinical course of our patients to those described in the literature. <b>Results:</b> Between January 2016 and October 2020, 79 pediatric patients with SVT were seen and treated at our center. A treatment-resistant SVT was noted only in three patients (4%). Ivabradine was used in these patients as a single or combined therapy. The rest (96%) were successfully treated with conventional anti-arrhythmics such as β-blockers, flecainide, and other approved medications. In the ivabradine group, successful reversal to sinus rhythm was achieved in two of the three patients (66%), one patient was treated with a combination therapy of amiodarone and ivabradine, and the other patient was treated only with ivabradine. <b>Conclusion:</b> Overall, promissory results are associated with the use of ivabradine in pediatric patients with refractory SVT. Ivabradine appears to be a safe and well-tolerated medication that can induce adequate suppression of SVT, complete reversal to sinus rhythm, and effective enhancement of left ventricular function.",success
23800976,False,Journal Article;Review,,,,,,,,True,The use of intravenous verapamil for tachyarrhythmia in infants is widely considered contraindicated due to the perceived risk of hemodynamic collapse after administration. This article reviews the relatively limited evidence that led to this well-known contraindication and highlights the interesting process by which medical practice may evolve in the absence of persuasive science.,success
3964800,False,"Journal Article;Research Support, U.S. Gov't, P.H.S.",,,,,,,,True,"The records of 90 patients with Wolff-Parkinson-White syndrome who presented with supraventricular tachycardia in the first 4 months of life were reviewed. Among these, 63% were male. Structural heart disease was present in 20%, most commonly Ebstein's anomaly. All patients presented with a regular narrow QRS tachycardia, and pre-excitation became evident only when normal sinus rhythm was established. Only one infant had atrial flutter and none had atrial fibrillation. Type A Wolff-Parkinson-White syndrome was most common (49%), with heart disease occurring in only 5% of these patients. In contrast, heart disease was identified in 45% of those with type B syndrome. Initially, normal sinus rhythm was achieved in 88% of the 66 infants treated with digoxin with no deaths. Normal sinus rhythm resumed after electrical countershock in 87% of the 15 infants so treated. Maintenance digoxin therapy was used in 85 patients. The Wolff-Parkinson-White pattern disappeared in 36% of the patients. Four infants died of cardiac causes during the mean follow-up period of 6.5 years. Two of these four infants had congenital heart disease; the third, with a normal heart initially, developed ventricular fibrillation and died from a cardiomyopathy considered related to resuscitation. The remaining infant, with a normal heart, died suddenly at 1 month of age. All were receiving digoxin. A wide QRS tachycardia later appeared in three patients, all with heart disease, one of whom died.(ABSTRACT TRUNCATED AT 250 WORDS)",success
21553267,False,Comparative Study;Journal Article,,,,,,,,True,"Our objective was to assess the efficacy and safety of high-dose sotalol in neonates and infants with refractory supraventricular tachycardia (SVT). SVT in neonates and infants can be refractory to primary therapies; therefore, secondary agents, e.g., sotalol, are often required to obtain control of SVT. Age-factor nomogram dosing of sotalol is widely used; however, our institution uses greater doses based on body surface area (approximately 150-200 mg/m(2)/d). A retrospective review of 78 inpatients receiving sotalol, after failing another antiarrthymic medication, at our institution from 2001 to 2008 was performed. Corrected QT intervals (QTc), 24-h Holter-monitoring results, and outpatient records were reviewed to assess safety and efficacy for patients ≤ 2 years of age. Median patient age at the time of initiation of therapy was 24 days (range 3-728). Forty-eight patients (62%) were neonates, and 36 (46%) had congenital heart disease. The median sotalol dosage was 152 mg/m(2)/day (range 65-244). The SVT of 70 patients (90%) was controlled with sotalol. No patients experienced significant QTc prolongation or proarrhythmia. Mean duration of follow-up was 3.3 ± 0.24 years. High-dose sotalol allows for safe and rapid control of refractory tachyarrhythmias in this young age group.",success
35258638,False,Journal Article,,,,,,,,True,"Supraventricular tachycardia (SVT) is the most common arrhythmia in neonates and infants, and pharmacological therapy is recommended to prevent recurrent episodes. This retrospective study aims to describe and analyze the practice patterns, effectiveness, and outcome of drug therapy for SVT in patients within the first year of life. Among the 67 patients analyzed, 48 presented with atrioventricular re-entrant tachycardia, 18 with focal atrial, and one with atrioventricular nodal re-entrant. Fetal tachycardia was reported in 27%. Antiarrhythmic treatment consisted of beta-receptor blocking agents in 42 patients, propafenone in 20, amiodarone in 20, and digoxin in 5. Arrhythmia control was achieved with single drug therapy in 70% of the patients, 21% needed dual therapy, and 6% triple. Propafenone was discontinued in 7 infants due to widening of the QRS complex. After 12 months (6-60), 75% of surviving patients were tachycardia-free and discontinued prophylactic treatment. Patients with fetal tachycardia had a significantly higher risk of persistent tachycardia (p: 0.007). Prophylactic antiarrhythmic medication for SVT in infancy is safe and well tolerated. Arrhythmia control is often achieved with single medication, and after cessation, most patients are free of arrhythmias. Infants with SVT and a history of fetal tachycardia are more prone to suffer from persistent SVT and relapses after cessation of prophylactic antiarrhythmic medication than infants with the first episode of SVT after birth.",success
30875081,False,Comparative Study;Journal Article,,,,,,,,True,"Supraventricular tachycardia (SVT) in children can be difficult to treat when first-line therapies (beta-blockade or digoxin) are not effective. Both flecainide and amiodarone are used as second-line therapies. We sought to compare the efficacy and safety of flecainide and amiodarone in pediatric patients with recurrent SVT. Pediatric patients treated with oral flecainide or oral amiodarone for SVT between 2006 and 2015 were studied. Tachycardia mechanisms included orthodromic reciprocating tachycardia (ORT), intra-atrial reentrant tachycardia (IART), and ectopic atrial tachycardia (EAT). Outcomes were classified as full success, partial success (requiring additional intervention), or failure. Seventy-four patients were included (median age 46 days, range 1 day to 19 years). Flecainide was used in 47 patients and amiodarone in 27 patients. Full success was achieved in 68% and 59%, respectively (P = 0.28). Partial success was achieved in 13% and 19%, respectively (P = 0.12). Treatment failed in 19% and 22%, respectively (P = 0.97). Ten crossover patients received the second medication after the first failed. Of five amiodarone-to-flecainide crossovers, four achieved success on flecainide alone. Of five flecainide-to-amiodarone crossovers, two achieved success. Minor adverse events occurred in 9% of flecainide and 22% of amiodarone patients (P = 0.16). No significant differences were seen by arrhythmia subtype (36 EAT, 28 ORT, 10 IART), congenital heart disease (n = 38), or age group (56 infants). Oral flecainide and amiodarone achieved meaningful arrhythmia control in 81% and 78% of pediatric patients with recurrent SVT, respectively. Those who failed amiodarone had encouraging outcomes when changed to flecainide.",success
33416921,False,Journal Article,,,,,,,,True,"We sought to assess the effect of a shorter medication treatment course (up to 4-6 months of age) on the recurrence of infantile supraventricular tachycardia (SVT). This was a retrospective review of infants with SVT diagnosed at age 0-12 months at Rady Children's Hospital (2010-2017). Infants with structural congenital heart disease, automatic tachycardias, atrial flutter, or lack of follow-up data were excluded. Seventy-four infants met criteria. Median age at diagnosis was 6 days (IQR 0-21 days); 28.4% presented with fetal tachycardia. Median gestational age was 38.4 weeks (IQR 36-40), 30% were preterm. Median age at medication discontinuation was 6.7 months (IQR 4.6-9.8). Therapy was stopped at younger age in patients managed by pediatric electrophysiologist (vs. general pediatric cardiologist): 4.9 vs. 8.6 months (p = 0.03). Thirty-eight patients (51.4%) were treated for < 6 months; 32.4% for 6-12 months, and 16.2% for > 12 months. SVT recurrence was similar for these groups: 13.2% vs. 16.7%, and 33.3%, respectively, (p = 0.27). Most patients with recurrence required emergency care, though none had significant adverse outcomes. Infants with SVT and structurally normal cardiac anatomy, who remain recurrence free on a single agent, have no increased risk of recurrence with shorter treatment courses of 4-6 months, compared to traditional treatment duration of 6-12 months.",success
8916490,False,Journal Article,,,,,,,,True,"Chaotic atrial tachycardia was observed in 7 infants without underlying structural heart disease. Clinical presentation and approach to management are discussed, with particular attention to the use of propafenone for this uncommon pediatric arrhythmia.",success
24769425,True,"Journal Article;Multicenter Study;Research Support, Non-U.S. Gov't",,,,,,,,True,"Permanent junctional reciprocating tachycardia (PJRT) is an uncommon form of supraventricular tachycardia in children. Treatment of this arrhythmia has been considered difficult because of a high medication failure rate and risk of cardiomyopathy. Outcomes in the current era of interventional treatment with catheter ablation have not been published. To describe the presentation and clinical course of PJRT in children. This is a retrospective review of 194 pediatric patients with PJRT managed at 11 institutions between January 2000 and December 2010. The median age at diagnosis was 3.2 months, including 110 infants (57%; aged <1 year). PJRT was incessant in 47%. The ratio of RP interval to cycle length was higher with incessant than with nonincessant tachycardia. Tachycardia-induced cardiomyopathy was observed in 18%. Antiarrhythmic medications were used for initial management in 76%, while catheter ablation was used initially in only 10%. Medications achieved complete resolution in 23% with clinical benefit in an additional 47%. Overall, 140 patients underwent 175 catheter ablation procedures with a success rate of 90%. There were complications in 9% with no major complications reported. Patients were followed for a median of 45.1 months. Regardless of treatment modality, normal sinus rhythm was present in 90% at last follow-up. Spontaneous resolution occurred in 12% of the patients. PJRT in children is frequently incessant at the time of diagnosis and may be associated with tachycardia-induced cardiomyopathy. Antiarrhythmic medications result in complete control in few patients. Catheter ablation is effective, and serious complications are rare.",success
19232902,True,Journal Article;Multicenter Study,,,,,,,,True,"To determine the outcomes of medical management, pacing, and catheter ablation for the treatment of nonpost-operative junctional ectopic tachycardia (JET) in a pediatric population. Nonpost-operative JET is a rare tachyarrhythmia that is associated with a high rate of morbidity and mortality. Most reports of clinical outcomes were published before the routine use of amiodarone or ablation therapies. This is an international, multicenter retrospective outcome study of pediatric patients treated for nonpost-operative JET. A total of 94 patients with JET and 5 patients with accelerated junctional rhythm (age 0.8 year, range fetus to 16 years) from 22 institutions were identified. JET patients presenting at age < or =6 months were more likely to have incessant JET and to have faster JET rates. Antiarrhythmic medications were utilized in a majority of JET patients (89%), and of those, amiodarone was the most commonly reported effective agent (60%). Radiofrequency ablation was conducted in 17 patients and cryoablation in 27, with comparable success rates (82% radiofrequency vs. 85% cryoablation, p = 1.0). Atrioventricular junction ablation was required in 3% and pacemaker implantation in 14%. There were 4 (4%) deaths, all in patients presenting at age < or =6 months. Patients with nonpost-operative JET have a wide range of clinical presentations, with younger patients demonstrating higher morbidity and mortality. With current medical, ablative, and device therapies, the majority of patients have a good clinical outcome.",success
34035688,False,Journal Article,,,,,,,,True,Congenital junctional ectopic tachycardia is a rare and special type of supraventricular arrhythmia. Junctional ectopic tachycardia is characterized by persistently elevated heart rates that may cause an impairment in cardiac function. Junctional ectopic tachycardia is considered one of the most difficult-to-treat conditions even with a combination of antiarrhythmic medications. Ivabradine is a novel antiarrhythmic medication used to decrease the heart rate in adults with angina pectoris. We report a first case of a premature neonate with a normal heart structure who developed junctional ectopic tachycardia and was subsequently treated successfully with ivabradine.,success
16132312,False,Case Reports;Journal Article,,,,,,,,True,"We report a patient with prenatally diagnosed tuberous sclerosis. Fetal ultrasonography demonstrated multiple cardiac tumors and arrhythmia. After birth, because of frequent supraventricular extrasystoles, the infant was admitted to the neonatal intensive care unit. Findings on 24-hour ambulatory electrocardiogram (ECG) showed frequent supraventricular tachycardia and ventricular tachycardia with four beats as the longest run. At the age of 12 days, he developed cardiopulmonary arrest after crying out. A monitored ECG showed ventricular tachycardia. Twenty minutes after onset, a 12-lead ECG showed ventricular fibrillation, which returned to normal sinus rhythm with repeated DC cardioversion. Oral antiarrhythmic therapy with carteolol hydrochloride was effective. The patient showed no further symptoms after oral medication was initiated and the tumors regressed spontaneously.",success
21401653,False,Case Reports;Journal Article,,,,,,,,True,"This report describes a fetus presenting with intrauterine tachycardia and hydrops fetalis. Soon after birth the neonate was noted to be in torsades de pointes that responded dramatically to medical management. Long QT syndrome (LQTS) was diagnosed on electrocardiogram obtained soon after birth. The prognosis is poor when LQTS presents in utero or during the first week of life. However, our infant did well with medical management and has remained free of arrhythmias at follow-up.",success
26256466,False,Journal Article;Review,,,,,,,,True,"Paediatric patients, particularly preterm neonates, present many pharmacological challenges. Due to the difficulty in conducting clinical trials in these populations dosing information is often extrapolated from adult populations. As the processes of absorption, distribution, metabolism and excretion of drugs change throughout growth and development extrapolation presents risk of over or underestimating the doses required. Information about the development these processes, particularly drug metabolism pathways, is still limited with weight based dose adjustment presenting the best method of estimating pharmacokinetic changes due to growth and development. New innovations in pharmacokinetic research, such as population pharmacokinetic modelling, present unique opportunities to conduct clinical trials in these populations improving the safety and effectiveness of the drugs used. More research is required into this area to ensure the best outcomes for our most vulnerable patients.",success
322908,False,Journal Article;Review,,,,,,,,True,"Based on clinical experience, infants with congestive heart failure are given larger doses of digoxin than adults, whether calculated on the basis of body weight or surface area. The reasons for this difference in dosage are not clear. The myocardium of the infants might be more resistant to the effects of digoxin than that of adults, and/or differences might exist between infants and adults concerning the absorption, distribution and elimination of the glycoside. Infants have been found to absorb digoxin in solution at the same rate and to the same extent as adults. The relative distribution of the glycoside to different tissues is also similar in the two age-groups. However, the binding of digoxin to several tissues seems to be more extensive in infants than in adults. In agreement with this, the apparent volume of distribution of the glycoside is larger in infants than in adults. As no enhanced urinary excretion has been found in infants there might be a non-renal elimination of the glycoside. With most prevailing dose schedules for digoxin, serum concentrations higher than those considered optimum for adults are often obtained in infants. It is known that infants tolerate higher serum digoxin concentrations than adults without developing signs of toxicity. However, it is not known whether such high concentrations are necessary for obtaining an adequate inotropic effect on the myocardium of the infants. If the relation between serum concentration and effect is the same in infants and adults, the loading (digitalising) dose generally given to infants is unnecessarily high.",success
23205867,False,Journal Article,,,,,,,,True,"While propranolol pharmacokinetics has been extensively studied in adults, this study reports the first evaluation of propranolol pharmacokinetics in term and preterm neonates. Propranolol concentrations were measured in four term and 32 preterm newborns treated with oral propranolol at the dose of 0.5 or 0.25 mg/kg every 6 h by serial dried blood spots. The levels of propranolol, although with high inter-individual variability, were proportional with the administered dose. Pharmacokinetic parameters evaluated at the steady state in newborns treated with 0.5 mg/kg/6 h showed values of maximal (71.7 ± 29.8 ng/mL), minimal (42.2 ± 20.8 ng/mL) and average concentration (60.8 ± 25.0 ng/mL), time of maximal concentration (2.6 ± 0.9 h) and area under the time-concentration curve (364.7 ± 150.2 ng/mL/h) similar to those observed in adults. In both dosing groups, elimination half-life was significantly longer (14.9 ± 4.3 and 15.9 ± 6.1 h), and apparent total body clearance (27.2 ± 13.9 and 31.3 ± 13.3 mL/kg/min) lower than those reported in adults, suggesting a slower metabolism in newborns. No differences were observed between newborns with different gestational age or different sex. Neonates treated with propranolol-exhibited drug concentrations proportional with the dose, with significant long half-life.",success
